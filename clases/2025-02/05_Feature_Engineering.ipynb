{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrAEIjr4m_0"
   },
   "source": [
    "# Clase 5 - Feature Engineering üìé\n",
    "\n",
    "- MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_SoNsDt-JEY"
   },
   "source": [
    "## Objetivos de la Clase üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFXUMGNe-IMq"
   },
   "source": [
    "- Entender qu√© es el feature engineering y c√≥mo puede impactar en el performance de los modelos\n",
    "- Aprender a implementar t√©cnicas de feature engineering a trav√©s de Scikit-learn usando `Pipeline` y `ColumnTransformer`\n",
    "- Entender y resolver la complejidad de datos faltantes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYa6XjsD9fDH"
   },
   "source": [
    "## ¬øQu√© es Feature Engineering? üßÆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xEZkmnQ89it"
   },
   "source": [
    "\n",
    "\n",
    "Feature engineering es el proceso de seleccionar, transformar y crear caracter√≠sticas relevantes de entrada para construir un modelo de aprendizaje autom√°tico preciso y eficiente.\n",
    "\n",
    "> **Pregunta ‚ùì:** ¬øSe necesita alg√∫n conocimiento previo para realizar Feature Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0_pBNdFxBNW"
   },
   "source": [
    "## Problema a visitar: House Pricing\n",
    "\n",
    "![House Pricing](https://www.kaggle.com/competitions/5407/images/header)\n",
    "\n",
    "Fuente: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "Al igual que en la clase anterior, el dataset **`house pricing`** consiste en 80 variables (79 variables explicativas m√°s una variable objetivo) que describen aspectos fundamentales de hogares residenciales en la ciudad de *Ames, Iowa*.\n",
    "\n",
    "La variable objetivo es el precio final de cada hogar (regresi√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1995,
     "status": "ok",
     "timestamp": 1713367908990,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "efbl5YD5xBNW"
   },
   "outputs": [],
   "source": [
    "# Importamos librer√≠as a utilizar en la clase\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1713367908991,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "QRgO-_DcxBNX"
   },
   "outputs": [],
   "source": [
    "# El conjunto a trabajar es el de entrenamiento\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/MDS7202/MDS7202/main/recursos/2023-01/13-EDA//train.csv\",\n",
    "    index_col=\"Id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K1fsLxTxBNY"
   },
   "source": [
    "> **Actividad üìé**: Imaginen que nos pasan estos datos y nos se√±alan que el conjunto de datos esta compuesto por:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzrbD1hdxBNY"
   },
   "source": [
    "- Terreno üèîÔ∏è\n",
    "        \"LotArea\",  # Area del terreno\n",
    "        \"LandSlope\",  # Pendiente del terreno\n",
    "        \"Neighborhood\",  # Barrio\n",
    "- Metadatos de la Vivienda üìÜ\n",
    "        \"BldgType\",  # Tipo de vivienda\n",
    "        \"YearBuilt\",  # A√±o de construcci√≥n\n",
    "        \"YearRemodAdd\",  # A√±o de remodelaci√≥n\n",
    "        \"Utilities\",  # Agua, luz, etc...\n",
    "- Materiales üß±\n",
    "        \"Foundation\",  # Fundaci√≥n de la vivienda\n",
    "        \"RoofMatl\",  # Material del techo\n",
    "        \"RoofStyle\",  # Estilo del techo\n",
    "        \"Exterior1st\",  # Material del Exterior\n",
    "        \"ExterCond\",  # Condici√≥n del material exterior\n",
    "- Interior de la casa üè°\n",
    "        \"GrLivArea\",  # Area habitable sobre el nivel del suelo.\n",
    "        \"1stFlrSF\",  # Area primer piso\n",
    "        \"2ndFlrSF\",  # Area segundo piso\n",
    "        \"FullBath\",  # Ba√±os completos\n",
    "        \"HalfBath\",  # Ba√±os de visita?\n",
    "        \"BedroomAbvGr\",  # Piezas\n",
    "        \"KitchenAbvGr\",  # Cocinas\n",
    "        \"KitchenQual\",  # Calidad de la cocina\n",
    "- S√≥tano ü™®\n",
    "        \"TotalBsmtSF\",  # Total s√≥tano\n",
    "        \"BsmtCond\",  # Condici√≥n del s√≥tano\n",
    "- Garaje üöó\n",
    "        \"GarageType\",  # Tipo de garaje\n",
    "        \"GarageCars\",  # Cantidad de autos por garaje\n",
    "- Piscina ü§Ω‚Äç‚ôÇÔ∏è\n",
    "        \"PoolArea\",  # Area de la piscina\n",
    "        \"PoolQC\",  # Calidad de la piscina\n",
    "- Calefacci√≥n y Aire üå¶Ô∏è\n",
    "        \"Heating\",  # Calefacci√≥n\n",
    "        \"HeatingQC\",  # Calidad de la Calefacci√≥n\n",
    "        \"CentralAir\",  # Aire Acondicionado Central\n",
    "- Calidad y Condici√≥n üåü\n",
    "        \"OverallQual\",  # Calidad general\n",
    "        \"OverallCond\",  # Condici√≥n general actual\n",
    "- Datos de la venta  üíµ\n",
    "        \"SaleType\",  # Tipo de venta\n",
    "        \"SaleCondition\",  # Condici√≥n de la vivienda en la venta\n",
    "        \"SalePrice\",  # Precio de la venta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1713367908991,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ddGKTFmtxBNY",
    "outputId": "c740591a-bb75-4572-a5bc-cb39ef0b422d"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu√© deber√≠amos hacer ahora?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yUaME55xBNY"
   },
   "source": [
    "### ¬øPor qu√© es importante el Feature Engineering? ü§®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K70OhJ0uxBNY"
   },
   "source": [
    "Muchos de los algoritmos de aprendizaje automatico **poseen un mejor desempe√±o cuando los valores de las features** (aka columnas) **se transforman a un valor facil de interpretar por los modelos**. Por otro lado, datos sucios pueden entorpecer las predicciones generadas por nuestros modelos, al igual que las escalas en que se presentan los datos. Por esto, **es relevante que las features que utilizemos se encuentren en escalas similares y con distribuciones relativamente similares a la distribuci√≥n normal**.\n",
    "\n",
    "> Importante ‚ùó: Gran parte de los modelos que se generan en la industria son centrados en datos, a diferencia de la academia el trabajo no se centra en el desarrollo de modelos ultra complejos, sino en modelos completamente estandarizados y por ello se necesita un mayor tiempo en la extracci√≥n de features desde los datos. Fuente interesante: [Data-Centric Approach vs Model-Centric Approach in Machine Learning](https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning).\n",
    "\n",
    "Consideren los siguientes ejemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABWqISGWxBNY"
   },
   "source": [
    "**Ejemplo 1**\n",
    "\n",
    "- El gr√°fico de la izquierda se grafica una variable con respecto a otra sin escalar\n",
    "- El gr√°fico de la derecha muestra ambas variables estandarizadas\n",
    "\n",
    "> **Pregunta ‚ùì:** ¬øEs entendible para un humano el gr√°fico de la derecha?¬øQu√© se puede interpretar de este?¬øQu√© efectos tendr√° usar las variables no escaladas (izquierda) vs las escaladas (derecha) usando alg√∫n modelo predictivo basado en distancias?\n",
    "\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/14-Feature-Engineering-Parte-I/escalamiento_2.png?raw=true' width=1000 />\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html'>Compare the effect of different scalers on data with outliers en el User Guide de Scikit-learn</a>.\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "    Los Datos son del dataset <a href='https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset'>California Housing dataset</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVsxeqJKxBNZ"
   },
   "source": [
    "**Ejemplo 2**\n",
    "\n",
    "- El gr√°fico de la derecha se grafica una variable un conjunto de variables en 2 dimensiones diferenciadas por la clase.\n",
    "- El gr√°fico de la izquierda se grafica el mismo grafico pero con el conjunto de variables transformado\n",
    "\n",
    "> **Pregunta ‚ùì:** ¬øCual de los dos conjuntos de datos es mas facil de separar?¬øQue efectos tendr√≠a utilizar las variables en el estado original contra el estado final para alg√∫n modelo lineal?.\n",
    "\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/14-Feature-Engineering-Parte-I/feature-engineering1.jpg?raw=true' width=1000 />\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://www.kdnuggets.com/2018/12/feature-engineering-explained.html'>Feature Engineering for Machine Learning: 10 Examples</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f77XXaFh9r-D",
    "tags": []
   },
   "source": [
    "### ¬øPero que hay del Deep Learning, no que solucionaba este problema? üò£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUafO1u69q6R"
   },
   "source": [
    "Si bien uno de los aspectos m√°s relevantes del Deep Learning es la extracci√≥n autom√°tica de las features desde los datos, sin embargo:\n",
    "\n",
    "- El desarrollo actual de deep learning no nos permite abstraer todas las features desde los datos.\n",
    "- Muchas empresas no utilizan deep learning para generar modelos, ya sea por capacidad o simplemente porque no lo necesitan. **Recalcar, el deep learning no es la soluci√≥n para todo!** [Ejemplo](https://arxiv.org/pdf/2106.03253.pdf)\n",
    "- Deep learning suele tener buenos resultados en datos no estructurados, sin embargo, en datos tabulares no siempre es el caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vdg2OynTEp7F"
   },
   "source": [
    "## Veamos nuestra librer√≠a core: Scikit-Learn ‚öíÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt5zcy-yFvrE"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MDS7202/MDS7202/a60ef458182c2f26af3aaf4f8e0446a8512d4f75/clases/2022-01/15_Preprocesamiento_Intro_a_Scikit-Learn/resources/scikit-learn.png\" width=400/>\n",
    "\n",
    "[`scikit-learn`](https://scikit-learn.org/stable/) es probablemente una de las librer√≠as de Aprendizaje Autom√°tico m√°s populares para Python. *Open-source* y construida sobre `numpy`, `scipy` y `matplotlib`, ofrece interfaces y flujos de trabajos (*frameworks*) simples y eficientes para construir aplicaciones enfocadas an√°lisis de datos y predicci√≥n.\n",
    "Sus *APIs* permite generar c√≥digo limpio y est√° provista de una extensa documentaci√≥n.\n",
    "\n",
    "> Parentesis: API: **Application Programming Interface / Interfaz de programaci√≥n de aplicaciones** - Son las interfaces comunes (funciones, objetos, m√©todos, etc..) que un software o librer√≠a ofrece para comunicarse con el resto. Esta define entre otras cosas: el tipo de llamadas o funciones que pueden ser hechas, como hacerlas, el tipo de datos de entrada y salida, etc...\n",
    "\n",
    "Una gran ventaja de `Scikit-learn` consiste en su estructura transversal de clases y herencia. La mayor√≠a de clase pertenece a alguna de estas dos categor√≠as:\n",
    "\n",
    "* **`transformers`**: Permite **transformar datos input antes de utilizar algoritmos de aprendizaje sobre ellos**. A partir de este m√≥dulo, se puede realizar imputaciones de valores faltantes, estandarizaci√≥n de variables, escalamientos y seleccion de caracter√≠siticas por medio de algoritmos especializados. Esto comunmente se logra a trav√©s de las interfaces\n",
    "    - `fit` que permite aprender los par√°metros de la transformaci√≥n, por ejemplo la media y varianza en la normalizaci√≥n.\n",
    "    - `transform` que aplica la transformaci√≥n a los datos.\n",
    "    - `fit_transform` permite ambas operaciones al mismo tiempo.\n",
    "\n",
    "* **`estimators`**: Proveen los **algoritmos** de aprendizaje autom√°tico a trav√©s de los m√©todos `fit` y `predict`.\n",
    "\n",
    "El m√©todo usual de importaci√≥n se basa en seleccionar un subm√≥dulo de la librer√≠a indicando (de manera opcional) el objeto que se utilizar√°. Por ejemplo, si se desea utilizar el escalador de datos Min-M√°x del subm√≥dulo `preprocessing`, se har√≠a de la manera usual, por medio de:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "> **Nota**: No se recomienda importar la librer√≠a completa `import sklearn as sk` pues su estructura de subm√≥dulos es suficientemente grande, como para considerar cada uno como una librer√≠a.\n",
    "\n",
    "\n",
    "A lo largo del curso se estudiar√°n distintos componentes de esta librer√≠a. Durante esta clase nos centraremos en los m√≥dulos `preprocessing`, `compose` y `pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62-sDVIg39ya"
   },
   "source": [
    "## Operaciones Comunes de Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XnmMHqNxBNa"
   },
   "source": [
    "Debido a la importancia que posee de la etapa de Feature Engineering en los proyectos de ML, se han desarrollado muchas t√©cnicas para agilizar el proceso.\n",
    "\n",
    "Algunas de las operaciones que podemos realizar con el m√≥dulo **`transformers`** incluyen:\n",
    "\n",
    "- **Creaci√≥n de nuevas Features** a partir de operaciones usando los datos disponibles.\n",
    "- **Transformaciones** como las vistas en la clase de preprocesamiento (escalamiento, normalizaci√≥n, one hot encoding para variables categ√≥ricas etc...).\n",
    "- **Reducci√≥n de Caracter√≠sticas** en la que se combinan/reducen caracter√≠sticas redundantes (usando por ejemplo, PCA).\n",
    "- **Selecci√≥n de Caracter√≠sticas** en la que a partir de diversos criterios se seleccionan las caracter√≠sticas que m√°s aportan al modelo.\n",
    "\n",
    "El proceso de generar y preprocesar las features requiere mucha creatividad y al mismo conocimiento del dominio del problema.\n",
    "\n",
    "Para efectos de esta clase revisaremos las siguientes:\n",
    "\n",
    "- **Escalamiento de variables**\n",
    "- **Codificaci√≥n de caracter√≠sticas categ√≥ricas**\n",
    "- **Missing Values**\n",
    "\n",
    "Sin embargo, a pesar de que no forma parte de un proceso de feature Engineering tradicional, comentaremos que es el **data-drift** y porque es importante que lo midas al momento de trabajar con tus variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3V965Cp_xBNa",
    "tags": []
   },
   "source": [
    "## Escalamiento\n",
    "\n",
    "Un paso importante antes de introducir features en los modelos, es el escalamiento de las variables. El objetivo de esto es que el dominio de las variables sea similar y de esta forma obtener mejores resultados. Este proceso es una de las cosas m√°s sencillas que se pueden hacer y que (por lo general) se traduce en un aumento del rendimiento del modelo. Ojo que no hacer esto puede hacer que su modelo no tenga sentido como es el caso de algoritmos cl√°sicos.\n",
    "\n",
    "Tomar en consideraci√≥n que muchos de los algoritmos modernos como **XGBoost** se√±alan que no necesitan escalamiento para el entrenamiento, sin embargo, escalar las variables de entrada puede impactar positivamente en el poder predictivo del modelo.\n",
    "\n",
    "Para realizar el escalamiento utilizaremos el m√≥dulo `sklearn.preprocessing`, este entrega diversas t√©cnicas de escalamiento, normalizaci√≥n y estandarizaci√≥n de datos a trav√©s de clases `Transformers` (no confundir con los transformers de Deep Learning).\n",
    "\n",
    "### Estandarizaci√≥n\n",
    "\n",
    "La est√°ndarizaci√≥n es una de las transformaciones mas relevantes a tener presente durante el modelamiento. Esto debido a que un gran cantidad de algoritmos de aprendizaje autom√°tico / estad√≠stico, asumen que los datos a operar se encuentran **distribuidos de manera normal**. **Si los datos no se distribuyen normalmente y contienen valores at√≠picos**, **es posible que la media y la desviaci√≥n t√≠pica no reflejen con exactitud la tendencia central y la variabilidad de los datos**.\n",
    "\n",
    "El objeto `StandarScaler` permite estandarizar datos para que tengan **media = 0 y desviaci√≥n st√°ndar = 1**. \n",
    "\n",
    "**Importante:** StandardScaler solo estandariza los datos, pero **no los normaliza!** Por lo tanto, s√≥lo deben ocuparlo cuando:\n",
    "- El algoritmo que ocupan requiere datos con distribuci√≥n gaussiana\n",
    "- Sus datos tienen distribuci√≥n gaussiana.\n",
    "\n",
    "**Ejemplo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Qed6IUY4NlL"
   },
   "source": [
    "Comencemos graficando el histograma de algunas variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1713367909501,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "sm33HTMPxBNf",
    "outputId": "19cb6c24-24fc-4a3a-95f6-ac3f7a4db676"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "cols_to_estandarize = [\n",
    "    'OverallQual',\n",
    "    'OverallCond',\n",
    "    'GarageCars',\n",
    "    'GarageArea',\n",
    "    'GrLivArea'\n",
    "]\n",
    "fig1 = px.histogram(df[cols_to_estandarize].melt(),\n",
    "             x='value',\n",
    "             color='variable',\n",
    "             barmode='group')\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKdUrD7yxBNf"
   },
   "source": [
    "La transformaci√≥n a aplicar es mover todos los datos a una distribuci√≥n normal con media 0 y varianza 1.\n",
    "\n",
    "Para esto, por cada dato:\n",
    "    \n",
    "$$ z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "En donde $\\mu$ es la media de la columna y $\\sigma$ es la desviaci√≥n est√°ndar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIx90ox0xBNg"
   },
   "source": [
    "Para esto, importamos escalador, lo inicializamos y luego ejecutamos `fit_transform` sobre los datos.\n",
    "Notese que esto retorna un arreglo numpy con los datos escalados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1713367909502,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "JfiYyrJr4rfy",
    "outputId": "ae434b5e-ea5d-4053-b70f-22381ee90152"
   },
   "outputs": [],
   "source": [
    "data_to_standarize = df.loc[:, cols_to_estandarize] # separar data\n",
    "data_to_standarize.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1713367909502,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "bY1rKZrvxBNg",
    "outputId": "6886b528-f7b9-4616-e544-1081c25f1897"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "standard_scaler = StandardScaler() # inicializar scaler\n",
    "standarized_df = standard_scaler.fit_transform(data_to_standarize) # escalar datos\n",
    "\n",
    "standarized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEqpPtA7xBNg"
   },
   "source": [
    "Por comodidad, convertimos los datos escalados a un Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1713367909502,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "7BR3xPORxBNg",
    "outputId": "39cd5d18-181e-4fac-dbca-17075759b440"
   },
   "outputs": [],
   "source": [
    "standarized_df = pd.DataFrame(standarized_df, columns=cols_to_estandarize)\n",
    "standarized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1713367910004,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "zvecCsEtxBNg",
    "outputId": "a5559bc7-4a58-43e4-a950-a88f4986563f"
   },
   "outputs": [],
   "source": [
    "standarized_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypt7zUtbxBNg"
   },
   "source": [
    "Veamos ahora como se muestran las distribuciones de los datos estandarizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd2mT7X_xBNh"
   },
   "source": [
    "\n",
    "Al aplicar `.fit_transform()` se obtienen los par√°metros de media `.mean_` y desviaci√≥n est√°ndar `.scale_` para cada columna del dataframe operado. Observe que tales atributos del objeto tipo `StandardScaler` son p√∫blicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1713367910004,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "31JX_zHyxBNh",
    "outputId": "56a4e387-4fbf-47ec-d005-6edbafdc0cdd"
   },
   "outputs": [],
   "source": [
    "standard_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1713367910004,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "qLzQjYFqxBNh",
    "outputId": "20e39df6-1674-4f95-f78d-21cef984a61b"
   },
   "outputs": [],
   "source": [
    "standard_scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1713367910004,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "qqrvWm0txBNi",
    "outputId": "5819d83d-64e8-4534-f13a-7d5b120424ed"
   },
   "outputs": [],
   "source": [
    "standard_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1713367910004,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "jdlHHPJkxBNi",
    "outputId": "f844c44d-04b2-4848-be8d-0a527e7692cd"
   },
   "outputs": [],
   "source": [
    "px.histogram(standarized_df[cols_to_estandarize].melt(),\n",
    "             x='value',\n",
    "             color='variable',\n",
    "             barmode='group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "executionInfo": {
     "elapsed": 1959,
     "status": "ok",
     "timestamp": 1713367911957,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "GZGp1Dp2xBNj",
    "outputId": "51c882ff-e35a-4c5c-a39a-39569314c4f2"
   },
   "outputs": [],
   "source": [
    "standarized_df.plot.kde(figsize=(16,9), bw_method=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhMGFNlZxBNk"
   },
   "source": [
    "> **Pregunta ‚ùì**: ¬øQu√© sucede con las variables que se alejan mucho de ser normales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1713367911957,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "jeKq7WDrxBNk",
    "outputId": "08dee746-901f-420e-d110-0e6e71230594"
   },
   "outputs": [],
   "source": [
    "px.histogram(df, x='BsmtUnfSF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0kLBzzSxBNl"
   },
   "source": [
    "### Escalamiento m√≠nimo-m√°ximo\n",
    "\n",
    "Una buena alternativa al m√©todo anterior, es el escalamiento por rango, este tiene la forma:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_{i} - \\min(x)}{\\max (x)-\\min (x)}\n",
    "\\end{equation}\n",
    "\n",
    "para $x$ columna a tratar, $x_i$ elemento a transformar. Esta transformaci√≥n permite hacer que los datos se muevan entre 0 y 1 y puede ser utilizado sobre una distribuci√≥n de datos no normales.\n",
    "\n",
    "> **Nota:** este transformador se ve afectado por la **presencia de outliers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1230,
     "status": "ok",
     "timestamp": 1713367913181,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "uPjj78MwjOrT"
   },
   "outputs": [],
   "source": [
    "cols_to_scale = ['BsmtUnfSF', 'Fireplaces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1713367913181,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "VpJUlFckxBNl",
    "outputId": "b7c470f6-fc38-4a50-c12d-554b23ff53b4"
   },
   "outputs": [],
   "source": [
    "px.histogram(df, 'Fireplaces') # notar que solo tiene 3 valores posibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1713367913181,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ZhbqbEkXxBNl",
    "outputId": "a82c1555-df5c-4e2c-d4ee-ef7eae000ed5"
   },
   "outputs": [],
   "source": [
    "# BsmtUnfSF = Unfinished square feet of basement area\n",
    "df.loc[:, 'BsmtUnfSF'].plot.hist(backend='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "Uug-kRGk_bLR",
    "outputId": "73d05a4d-a831-4112-c50a-8e0b47406c44"
   },
   "outputs": [],
   "source": [
    "data_to_scale = df.loc[:, cols_to_scale] # separar datos\n",
    "data_to_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "K-v6qaZTxBNm",
    "outputId": "3106e1d2-ed27-46c5-c7e1-0c55692ec2dd"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax_scaler = MinMaxScaler() # inicializar scaler\n",
    "\n",
    "scaled_data = minmax_scaler.fit_transform(data_to_scale) # escalar datos\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=cols_to_scale) #  transformar a dataframe\n",
    "\n",
    "scaled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "Vk_71nKRxBNm",
    "outputId": "4c4dd0c3-856c-4e8d-88f3-a20c26f56e46"
   },
   "outputs": [],
   "source": [
    "scaled_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "cb320FuVxBNm",
    "outputId": "0a683e6d-8e13-4e67-9b63-d1ac49844025"
   },
   "outputs": [],
   "source": [
    "scaled_data.plot.hist(backend='plotly', barmode='overlay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__7JsP5QxBNm"
   },
   "source": [
    "Comprobamos minimos y m√°ximos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "t3VtAIDPxBNm",
    "outputId": "181fcc79-1be8-4965-d2a6-cc32e772ab10"
   },
   "outputs": [],
   "source": [
    "scaled_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "zlo3YMkCxBNm",
    "outputId": "89203e18-7ec2-4a1e-ddf3-196e72df3eb4"
   },
   "outputs": [],
   "source": [
    "scaled_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfaCz-GgxBNn"
   },
   "source": [
    "**Nota**: Si se desea escalar por rango, la mejor pr√°ctica es comprender los m√≠nimos y m√°ximos *absolutos* para cada columna. Esto se refiere, a las cotas superiores e inferiores que posee la columna **por definici√≥n**. A modo de ejemplo, considere un dataframe con las notas de una asginatura donde se enze√±a an√°lisis de datos, se sabe que la nota m√°xima en cierto √≠tem se codifica en una columna y su m√°ximo en efecto es 7.0. Sin embargo el m√≠nimo en dicha columna es 1.5, que es distinto al m√≠nimo natural para dicho item que es 1.0. Esto puede acarrear problemas con datos nuevos, sobretodo si aparece una nota inferior a 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwTkjM2zxBNn"
   },
   "source": [
    "### Outliers\n",
    "\n",
    "Un outlier es un at√≠picos que est√°n fuera del rango comun del resto de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "W6_0qQwqxBNn",
    "outputId": "e47f1d29-7628-4d68-e619-40f996cc9742"
   },
   "outputs": [],
   "source": [
    "# histograma con los datos brutos\n",
    "px.histogram(df, x='LotArea', marginal='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "bQD0nOHDxBNn",
    "outputId": "d54dfaab-ce4d-4442-c1c3-ffc4913b2cfb"
   },
   "outputs": [],
   "source": [
    "# histograma con los datos escalados con MinMax\n",
    "minmax_scaler = MinMaxScaler()\n",
    "scaled_data = minmax_scaler.fit_transform(df.loc[:, ['LotArea']])\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=['LotArea'])\n",
    "scaled_data.plot.hist(backend='plotly', barmode='overlay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMoxybqJxBNn"
   },
   "source": [
    "> **Pregunta:** ¬øQu√© pasa cuando queremos estandarizar pero tenemos outliers?\n",
    "\n",
    "**Respuesta**: La gran mayor√≠a de los datos tienen a quedar en un rango muy acotado. En el caso anterior, hacia la izquierda.\n",
    "\n",
    "Hagamos el siguiente ejercicio: comparemos los estad√≠sticos de los datos con y sin filtro IQR (datos contenidos en el rango intercuart√≠lico):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "r1EosryDxBNn"
   },
   "outputs": [],
   "source": [
    "# media con los todos los datos de LotArea\n",
    "original = round(df['LotArea'].describe(), 3)\n",
    "original.name = 'LotArea Original'\n",
    "\n",
    "# datos presentes solo en el rango intercuant√≠lico\n",
    "q1 = df['LotArea'].quantile(.25)\n",
    "q3 = df['LotArea'].quantile(.75)\n",
    "mask = df['LotArea'].between(q1, q3, inclusive='both')\n",
    "iqr = df.loc[mask, 'LotArea']\n",
    "iqr.name ='LotArea Filtro IQR'\n",
    "iqr = round(iqr.describe(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KF4Z6u5xBNo"
   },
   "source": [
    "Observen las diferencias entre las medias y las desviaciones est√°ndar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "z5_bMwn9xBNo",
    "outputId": "fd7f1787-da6b-4014-92ae-a1f89668454b"
   },
   "outputs": [],
   "source": [
    "desc = pd.concat([original, iqr], axis=1)\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmy5kJGOxBNo"
   },
   "source": [
    "Si normalizamos usando estandarizaci√≥n original, cada dato va a ser dividido por 9981, **9 veces m√°s grande que la versi√≥n sin outliers usando IQR!**. Esto en t√©rminos pr√°cticos har√° que los datos tiendan a concentrarse mucho m√°s **cercanos a la media**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c6mxWSsxBNq"
   },
   "source": [
    "### Transformaci√≥n Robusta\n",
    "\n",
    "Cuando se trabaja con columnas que poseen valores fuera de rango (**outliers**) las transformaciones anteriores pueden fallar. En este caso, se recomienda utilizar una transformaci√≥n similar a la estandarizaci√≥n, pero que trabaje sobre la mediana como media y el rango intercuant√≠lico como desviaci√≥n est√°ndar:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_i - Q_2(x)}{Q_3(x) - Q_1(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Donde $IQR = Q_3(x) - Q_1(x)$ es el rango intercuart√≠lico de la columna $x$.\n",
    "\n",
    "En otras palabras, por cada ejemplo se sustrae la mediana y se divide por el rango intercuartil 75%- 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "Zv_yK0fUxBNq",
    "outputId": "3ce03eb9-d481-42ec-f302-a3dfd86e52d9"
   },
   "outputs": [],
   "source": [
    "# Ejemplo para obtener IQR\n",
    "data = np.array([1, 14, 19, 20, 22, 24, 26, 47])\n",
    "\n",
    "# Calculamos rango intercurtil\n",
    "q3, q1 = np.percentile(data, [75 ,25])\n",
    "q3 - q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQAXf1XCxBNq"
   },
   "source": [
    "**Ejemplo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-04lcwsxBNq"
   },
   "source": [
    "Se importa el objeto `RobustScaler` y se aplica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "k9FX_MWPxBNq",
    "outputId": "b68039d7-bc05-4118-d3af-93e1f595716e"
   },
   "outputs": [],
   "source": [
    "px.histogram(df['LotArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "f5JfxuXKxBNq"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "lot_area_standarized = standard_scaler.fit_transform(df[['LotArea']])\n",
    "robust_scaled_lot_area = robust_scaler.fit_transform(df[['LotArea']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "iLFs0laRxBNq"
   },
   "outputs": [],
   "source": [
    "comparacion = np.concatenate([lot_area_standarized, robust_scaled_lot_area], axis=1)\n",
    "comparacion = pd.DataFrame(comparacion, columns=['Estandarizacion Com√∫n', 'Estandarizaci√≥n Robusta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "zSiK4omNxBNr",
    "outputId": "fb619329-de62-43ad-9963-d368ede465a4"
   },
   "outputs": [],
   "source": [
    "px.histogram(comparacion, marginal='box', barmode='overlay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzXAUPJixBNr"
   },
   "source": [
    "Podemos observar como los **datos en la estandarizaci√≥n robusta se distribuyen de forma mas amplia** que en caso de la estandarizaci√≥n normal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XvKI-XyxBNu"
   },
   "source": [
    "### ¬øQu√© aspectos deber√≠amos considerar al momento de realizar escalamientos? üòÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txf3PenBxBNu"
   },
   "source": [
    "Al momento de realizar escalamiento de cualquier tipo deben considerar los siguientes puntos:\n",
    "\n",
    "- **El escalamiento es una fuente de data leakage** (Revisar m√°s abajo).\n",
    "- **Necesidad de entrenamiento ante data drift**. Muchos de los escalamientos necesitan el c√°lculo de un estad√≠stico para realizar la transformaci√≥n, esto implica que si los datos vistos durante el entrenamiento cambian en producci√≥n las transformaciones no ser√°n las mejores y necesitar√°n un reentrenamiento de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ech6DIbhxBNv"
   },
   "source": [
    "---\n",
    "\n",
    "## Codificaci√≥n de Variables Ordinales\n",
    "\n",
    "Para el manejo de adecuado de variables ordinales,  se recomienda expresar sus valores en funci√≥n de c√≥digos n√∫mericos. El transformer `OrdinalEncoder` permite transformar caracter√≠sticas categ√≥ricas en c√≥digos enteros (n√∫meros enteros) empezando desde 0 hasta N-1.\n",
    "\n",
    "Veamos algunas variables ordinales:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsQdAImYxBNv"
   },
   "source": [
    "BsmtQual: Evaluates the height of the basement\n",
    "\n",
    "       Ex\tExcellent (100+ inches)\n",
    "       Gd\tGood (90-99 inches)\n",
    "       TA\tTypical (80-89 inches)\n",
    "       Fa\tFair (70-79 inches)\n",
    "       Po\tPoor (<70 inches\n",
    "       NA\tNo Basement\n",
    "\n",
    "BsmtCond: Evaluates the general condition of the basement\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical - slight dampness allowed\n",
    "       Fa\tFair - dampness or some cracking or settling\n",
    "       Po\tPoor - Severe cracking, settling, or wetness\n",
    "       NA\tNo Basement\n",
    "\n",
    "       \n",
    "HeatingQC: Heating quality and condition\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "M-_tZFevxBNv",
    "outputId": "2e1b558b-d131-4e59-c76a-1c05a132d0cf"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "\n",
    "variables_ordinal = df[['BsmtCond', 'BsmtQual','HeatingQC']] # separamos las variables ordinales\n",
    "\n",
    "# Se entrena el codificador\n",
    "ordinales = enc.fit_transform(variables_ordinal.dropna()) # se transforma variables a codificacion ordinal\n",
    "ordinales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "N2IaLxWfnT0Y",
    "outputId": "92c3586e-5de0-46d0-da4a-0c531572ce02"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(ordinales, columns = ['BsmtCond', 'BsmtQual','HeatingQC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzVqD2SExBNv"
   },
   "source": [
    "Se obtienen las categor√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "U5ZrEAqmxBNw",
    "outputId": "3d098038-4b79-4741-b1f1-a1f33d48e447"
   },
   "outputs": [],
   "source": [
    "enc.categories_ # verificamos el orden impuesto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3qwedcBxBNw"
   },
   "source": [
    "> **Pregunta ‚ùì**: ¬øExiste alg√∫n problema en estas codificaciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "-CaKjwd7xBNw",
    "outputId": "f4541e96-4312-464e-a25d-2b1bd710dbf0"
   },
   "outputs": [],
   "source": [
    "categories_order = [\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n",
    "]\n",
    "\n",
    "enc_2 = OrdinalEncoder(categories=categories_order)\n",
    "\n",
    "ordinales = enc_2.fit_transform(variables_ordinal.dropna())\n",
    "\n",
    "pd.DataFrame(ordinales, columns = ['BsmtCond', 'BsmtQual','HeatingQC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ED6Khw72nf8f",
    "outputId": "821faf3f-05f2-4001-d534-c29654f85a53"
   },
   "outputs": [],
   "source": [
    "enc_2.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBMrxpHBxBNw"
   },
   "source": [
    "> **Pregunta ‚ùì**: Codificamos estas variables con `variables_ordinal.dropna()`. ¬øPor qu√© tuvimos que botar los valores faltantes y que se puede hacer en este caso ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZutoRYzxBNw"
   },
   "source": [
    "> **Pregunta ‚ùì**: ¬øC√≥mo codificamos las variables que no tienen orden?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4KFKr98xBNw"
   },
   "source": [
    "## Codificaci√≥n de Variables Categoricas\n",
    "\n",
    "Un problema com√∫n con la c√≥dificaci√≥n ordinal es que las variables pasan a ser consideradas continuas por algoritmos de machine learning (en especial por la API *estimators* de scikit-learn). Para evitar esto es posible convertir cada categor√≠a en una columna por si sola y asignar un 1 cuando est√© presente.\n",
    "\n",
    "![One Hot Encoding](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/14-Feature-Engineering-Parte-I/ohe.png?raw=true)\n",
    "<center>Fuente: https://morioh.com/p/811a5d22bbca </center>\n",
    "\n",
    "Esto se puede llevar a cabo por medio del transformador ` OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "l18wuwtOxBNw",
    "outputId": "cb4c391b-432f-4881-b039-4126b2f33c23"
   },
   "outputs": [],
   "source": [
    "df['Foundation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "3hLMBncmxBNw",
    "outputId": "d81f2249-b829-47bd-b073-3c1aaacd7952"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder() # inicializamos encoder\n",
    "\n",
    "cod = ohe.fit_transform(df.loc[:, ['Foundation']]) # encodeamos categorias\n",
    "cod # notar que es una matriz sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "3-wDlosYxBNx",
    "outputId": "27d1f5e3-6960-4f4d-89ac-476e62e54293"
   },
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4p7epaIL7MH"
   },
   "source": [
    "### C√≥mo puedo transformar la matriz sparse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_6TzLw5MKgY"
   },
   "source": [
    "#### Transformando a array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "snOvpYi_Mp6n",
    "outputId": "6a15510c-6f59-4308-fb49-2fafa98c3892"
   },
   "outputs": [],
   "source": [
    "cod.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "MRsJoCIlxBNx",
    "outputId": "e22affc7-7be4-4ac1-ad91-a9304283ff04"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cod.toarray(), columns=ohe.categories_) # generar dataframe con categorias onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQhuDKfZMZFB"
   },
   "source": [
    "#### O simplemente definiendo `sparse_output = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "FNssAwVeMgFP",
    "outputId": "ef57db27-cb18-44c9-8e13-14b5e54d6f15"
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output = False) # inicializamos encoder\n",
    "cod = ohe.fit_transform(df.loc[:, ['Foundation']]) # encodeamos categorias\n",
    "\n",
    "cod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WngLwjXzOpXd"
   },
   "source": [
    "Volvamos a hacer lo mismo con `Neighborhood`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "Xx_zLTcsxBNx",
    "outputId": "61c91d42-1099-4aa0-fe1b-7353299adf89"
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'Neighborhood'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "jQsPHoELxBNx",
    "outputId": "f22b8e2e-7a62-44ba-a85d-e41fbd642d95"
   },
   "outputs": [],
   "source": [
    "# Ahora, con los barrios\n",
    "ohe = OneHotEncoder(sparse_output = False) # inicializamos encoder con sparse_output = False\n",
    "categories = ohe.fit_transform(df.loc[:, ['Neighborhood']]) # noten como ahora no uso .toarray()\n",
    "\n",
    "pd.DataFrame(categories, columns = ohe.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A2I2BXzWrDA"
   },
   "source": [
    "## Pipelines\n",
    "\n",
    "La manera anterior es bastante clara de comprender, sin embargo, es redundante y repite muchos patrones de asignaci√≥n tediosos. Los `Pipelines` est√°n dise√±ados para resolver este problema.\n",
    "\n",
    "\n",
    "Las transformaciones en un dataset son combinadas entre si, hasta obtener una versi√≥n ordenada de los datos, posteriormente, estas se combinan con estimadores para formar un flujo de trabajo *input-output*. En Sckit-Learn el flujo antes nombrado de denomina *composite estimator* y se construye por medio de objetos tipo `Pipeline`.\n",
    "\n",
    "Pero mas importante aun, los Pipelines nos ayudan resolver el problema de **Data Leakage** de forma natural.\n",
    "\n",
    "Pero.. ¬øQu√© es **Data Leakage**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que queremos entrenar un modelo para predecir la variable `Accident`.\n",
    "\n",
    "Para eso, un colega nos facilita c√≥digo para pre procesar los datos y entrenar un modelo:\n",
    "\n",
    "```python\n",
    "def preprocess(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepara el dataframe para luego ser entrenado. En particular:\n",
    "    - Imputa valores nulos\n",
    "    - Genera features para aumentar la explicabilidad del modelo\n",
    "    \"\"\"\n",
    "\n",
    "    df_proc = df.copy()\n",
    "\n",
    "    # Imputar\n",
    "    ## Weather\n",
    "    weather_mode = df_proc[\"Weather\"].mode().iloc[0]\n",
    "    df_proc[\"Weather\"] = df_proc[\"Weather\"].fillna(weather_mode)\n",
    "\n",
    "    ## Driver_Alcohol\n",
    "    df_proc[\"Driver_Alcohol\"] = df_proc[\"Driver_Alcohol\"].fillna(0)\n",
    "\n",
    "    ## Driver_Age\n",
    "    age_mean = df_proc[\"Driver_Age\"].mean()\n",
    "    df_proc[\"Driver_Age\"] = df_proc[\"Driver_Age\"].fillna(age_mean)\n",
    "\n",
    "    # Feature Engineering\n",
    "    df_proc[\"Speed_Accident\"] = df_proc[\"Speed_Limit\"] * df_proc[\"Accident\"]\n",
    "    df_proc[\"Traffic_Norm\"] = df_proc[\"Traffic_Density\"] - df_proc[\"Traffic_Density\"].mean()\n",
    "\n",
    "    return df_proc\n",
    "\n",
    "# aplicar preprocessing\n",
    "df_proc = preprocess(df) \n",
    "\n",
    "# separar X,y\n",
    "target = \"Accident\"\n",
    "X = df.drop(columns = [target])\n",
    "y = df[target]\n",
    "\n",
    "# entrenar y evaluar performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, stratify=y)\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "> **Pregunta:** ¬øDetectan alg√∫n problema en el c√≥digo anterior? ¬øQu√© implicancias podr√≠a tener esto para el performance del modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definici√≥n \n",
    "\n",
    "**Data Leakage** (o fuga de datos) en Machine Learning ocurre cuando se **filtra informaci√≥n del conjunto de prueba** o de variables futuras hacia el modelo durante el entrenamiento. Esto provoca que el modelo aprenda patrones que no estar√°n disponibles en un entorno real, generando resultados demasiado optimistas en la validaci√≥n y un desempe√±o pobre en producci√≥n.\n",
    "\n",
    "#### ¬øPor qu√© es importante hablar de Data Leakage?\n",
    "\n",
    "El **Data Leakage** es uno de los errores m√°s comunes ‚Äîy a la vez m√°s cr√≠ticos‚Äî en los que puede incurrir un cient√≠fico de datos. Su impacto suele pasar desapercibido durante el desarrollo del modelo, pero puede tener consecuencias graves una vez que se despliega en producci√≥n.\n",
    "\n",
    "Ignorar o no detectar Data Leakage de forma oportuna puede acarrear los siguientes **riesgos**:\n",
    "\n",
    "- **Sobreajuste (overfitting):** El modelo aprende patrones que en realidad no estar√°n disponibles en datos nuevos, lo que lleva a un desempe√±o artificialmente alto durante el entrenamiento pero pobre en producci√≥n.\n",
    "\n",
    "- **Evaluaciones sesgadas:** Las m√©tricas obtenidas durante la validaci√≥n pueden ser optimistas o poco representativas, generando una falsa sensaci√≥n de calidad del modelo.\n",
    "\n",
    "- **Decisiones de negocio equivocadas:** Un modelo que parece funcionar bien en pruebas puede fallar en condiciones reales, provocando decisiones costosas para el negocio.\n",
    "\n",
    "- **P√©rdida de confianza en los modelos:** En industrias sensibles (salud, finanzas, etc), un modelo que falla por *data leakage* puede deteriorar la confianza de stakeholders y usuarios en la anal√≠tica predictiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y82cM-IUWu4c"
   },
   "source": [
    "### Como Definimos un Pipeline\n",
    "\n",
    "Un pipeline es una **lista de tuplas**.\n",
    "\n",
    "- La lista contiene todos los pasos que se efectuan desde la entrada hasta la salida del pipeline\n",
    "- Cata tupla de la lista representa un subproceso del pipeline. Este debe estar compuesto por un nombre u la clase que corresponda.\n",
    "\n",
    "![Pipeline](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/15-Feature-Engineering-Parte-II/pipeline.png?raw=true)\n",
    "\n",
    "Cada tupla debe seguir la siguiente estructura:\n",
    "\n",
    "```python\n",
    "step = ('alias', Transformer()) # Transformer = Procesamiento a realizar\n",
    "```\n",
    "\n",
    "Por ejemplo, para la im√°gen anterior, el pipeline ser√≠a:\n",
    "\n",
    "```python\n",
    "pipe = Pipeline([('scaling', Scaler()),\n",
    "                 ('dimensionality_reduction', DimReductor()),\n",
    "                 ('predictive_model', Model())])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-1JB5iiXUba"
   },
   "source": [
    "Los **`pipelines`** nos ayuda en los siguientes puntos:\n",
    "\n",
    "1. **Simplificaci√≥n del proceso de aprendizaje autom√°tico**: Los Pipelines permiten <u>combinar m√∫ltiples pasos</u> en el proceso de aprendizaje autom√°tico, como el preprocesamiento de datos, la selecci√≥n de caracter√≠sticas y el entrenamiento del modelo, en una √∫nica entidad. Esto simplifica el flujo de trabajo general y reduce las posibilidades de errores.\n",
    "\n",
    "2. **Procesamiento de datos consistente**: Los Pipelines aseguran que los mismos pasos de preprocesamiento de datos se apliquen de manera consistente tanto a los datos de entrenamiento como a los de prueba. Esto reduce el riesgo de sobreajuste (producto de un data-leakage) y facilita la comparaci√≥n del rendimiento de diferentes modelos.\n",
    "\n",
    "3. **Ajuste de hiperpar√°metros f√°cil**: Los Pipelines permiten ajustar los hiperpar√°metros de varios pasos en el proceso de aprendizaje autom√°tico de manera simult√°nea. Esto puede ayudar a encontrar la combinaci√≥n √≥ptima de hiperpar√°metros y mejorar el rendimiento del modelo.\n",
    "\n",
    "4. **Legibilidad y reutilizaci√≥n de c√≥digo**: Los Pipelines proporcionan una forma clara y concisa de organizar el c√≥digo, lo que facilita su lectura y mantenimiento. Tambi√©n permiten reutilizar la misma tuber√≠a para diferentes conjuntos de datos y modelos, ahorrando tiempo y esfuerzo.\n",
    "\n",
    "5. **Mejora del rendimiento**: Al reducir la cantidad de manipulaci√≥n de datos y c√°lculo requerido, los pipelines pueden llevar a tiempos de entrenamiento e inferencia de modelos m√°s r√°pidos. Esto puede ser especialmente √∫til al trabajar con conjuntos de datos grandes o modelos complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFMTbhioZ7AZ"
   },
   "source": [
    "Veamos un ejemplo para transformar `LotArea` con `MinMaxScaler` y `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "X7UA4B6zYnCT",
    "outputId": "d997451d-067e-4064-db93-b4ef80b3dee6"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# instanciamos pipeline\n",
    "LotArea_pipe = Pipeline([\n",
    "    ('scaler_1', MinMaxScaler()),\n",
    "    ('scaler_2', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# aplicamos pipeline sobre LotArea\n",
    "result = LotArea_pipe.fit_transform(df[['LotArea']])\n",
    "\n",
    "# generamos dataframe con resultado\n",
    "pd.DataFrame(result, columns = LotArea_pipe.feature_names_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hvu88zS2XyGT"
   },
   "source": [
    "### ColumnTransformer\n",
    "\n",
    "Es muy frecuente, es que los datos sea heterog√©neos por ejemplo, es normal encontrar datasets con variables **ordinales**, **categoricas**, y **num√©ricas**.\n",
    "\n",
    "Para utilizar *pipelines* en este contexto, se necesitaria definir una por cada variable, repitiendo varios componentes de c√≥digo entre variables que son del mismo tipo, esto resulta en una redundancia excesiva que se puede atacar por medio de objetos tipo `ColumnTransformer`. **Estos objetos permite separar flujos de preprocesamiento, permitiendo seleccionar por columna o grupos de columna dentro de un `pipeline`.**\n",
    "\n",
    "Nuevamente, el ColumnTransformer se construye sobre una **lista de tuplas**:\n",
    "\n",
    "- Cada tupla representa la transformaci√≥n a aplicar sobre un conjunto de columnas\n",
    "- Cada tupla debe contener el listado de columnas a transformar y la transformaci√≥n a aplicar\n",
    "\n",
    "Cada tupla debe seguir la siguiente estructura:\n",
    "\n",
    "```python\n",
    "transformation = ('alias', Transformer(), [col1, col2]) # Transformer = Procesamiento a realizar\n",
    "```\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "572fokfCWjRP"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessing_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('OneHotEncoder', OneHotEncoder(),  ['Neighborhood', 'Utilities', 'Foundation']),\n",
    "        ('StandardScaler', StandardScaler(),['OverallQual', 'OverallCond', 'GarageCars', 'GarageArea']),\n",
    "        ('PowerTransform', LotArea_pipe, ['LotArea'])]) # notar como se usa un pipeline dentro de ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "dBEy57ySaKyZ"
   },
   "outputs": [],
   "source": [
    "# podemos encapsular el ColumnTransformer como paso de un Pipeline\n",
    "housing_pipeline = Pipeline([\n",
    "    ('Preprocessing', preprocessing_transformer)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkRYL6cneMXT"
   },
   "source": [
    "Finalmente se aplican los procedimientos planificados en la variable `prep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "d_fn_eeHeLSY",
    "outputId": "5abc2b04-d227-4bb4-fb33-2f2ba220039c"
   },
   "outputs": [],
   "source": [
    "df_preprocesado = housing_pipeline.fit_transform(df)\n",
    "df_preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "5YumlMlLeN9I",
    "outputId": "418cc2f3-34b8-40cf-fba3-6600ddf842bd"
   },
   "outputs": [],
   "source": [
    "df_preprocesado = pd.DataFrame(df_preprocesado.toarray())\n",
    "df_preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjCMvVW9eWrF"
   },
   "source": [
    "> **Pregunta ‚ùì**: ¬øQu√© sucedi√≥ con el resto de las columnas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dV75j5VweaW4"
   },
   "source": [
    "Nota que en este momento el Pipeline solo cuenta con una etapa de preprocesado. Sin embargo, la idea es que a futuro tenga el resto de los pasos de nuestro proyecto.\n",
    "\n",
    "\n",
    "```python\n",
    "housing_pipeline = Pipeline([\n",
    "    ('Imputaci√≥n', Imputador()),\n",
    "    ('Preprocessing', Preprocesador()),\n",
    "    ('Selector de Variables', SelectorDeVariables()),\n",
    "    ('Reduccion de Dimensionalidad', ReduccionDeDimensionalidad()),\n",
    "    ('Modelo', Modelo())\n",
    "\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyLp0Pg4eyKV"
   },
   "source": [
    "### En resumen... un Pipeline... üß™\n",
    "\n",
    "\n",
    "Un pipeline es una lista de tuplas.\n",
    "\n",
    "- La lista contiene todos los pasos que se efectuan desde la entrada hasta la salida del pipeline\n",
    "- Cata tupla de la lista representa un subproceso del pipeline. Este debe estar compuesto por un nombre u la clase que corresponda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ZtWpHj7FeVTU"
   },
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes('float')\n",
    "cat_cols = df.select_dtypes('object')\n",
    "\n",
    "# Ejemplo de Preprocesador Compuesto\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"standard_scaler\", StandardScaler(), num_cols),\n",
    "        (\"category_one_hot\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy1OP5M7g6Uv"
   },
   "source": [
    "## Manejo de valores faltantes\n",
    "\n",
    "Por lo general, existen razones pr√°cticas y conceptuales a tener en cuenta cuando se trabaja con valores faltantes.\n",
    "\n",
    "### Conceptual\n",
    "\n",
    "**La falta de informaci√≥n introduce sesgos en los modelos de datos**, pues hace que las muestras obtenidas no sean representativas del fen√≥meno que se desea estudiar. Esto puede generar conclusiones sesgadas y puede llevar a tomar malas decisiones.\n",
    "\n",
    "### Pr√°ctica\n",
    "\n",
    "**Los valores faltantes son incompatibles con algunos modelos de aprendizaje autom√°tico**, debido a que estos modelos son parte de la raz√≥n fundamental de analizar un fen√≥meno por medio de datos,es que se necesita comprender bien los mecanismos de manejo de este tipo de valores.\n",
    "\n",
    "Veamos si nuestro dataset contiene valores nulos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1713368042464,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ErLzFKljsMsL",
    "outputId": "20d030c8-f05e-4f4a-8735-c909edb11d7a"
   },
   "outputs": [],
   "source": [
    "# Print de valores nulos\n",
    "df.isnull().sum().sort_values(ascending = False).iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOnepRZ8hZrI"
   },
   "source": [
    "## Tratamiento de Datos Faltantes: Deletion & Imputation ‚ò†Ô∏è\n",
    "\n",
    "### Eliminaci√≥n/Deletion ü™Ñ\n",
    "\n",
    "Es el m√©todo m√°s sencillo, se conoce tambien como **list-wise deletion** y consiste en **eliminar filas o columnas de un dataset que presenten datos faltantes**. Se puede acceder a este tipo de tratamiento por medio de `.dropna()` objetos de Pandas.\n",
    "\n",
    "Se recomiendan cuando el patr√≥n de perdida de informaci√≥n observada (por ejemplo por medio de `mssingno`) es claramente aleatorio, y si adem√°s las variables con informaci√≥n faltante son 'pocas' y con 'pocos' valores faltantes. La definici√≥n de 'poco' varia en funci√≥n del problema, pero una buena huer√≠stica puede ser inferior al 15% en variables de poca importancia. **Estos m√©todos generan una p√©rdida de datos y potencialmente aumento en el sesgo de los modelos** (üëÄ OJO: esto depende mucho del caso de estudio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "AXJ6OE8-o1aR",
    "outputId": "77e83003-80c6-4fb3-e0d9-283b85bcb225"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"LotFrontage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "GHZhFdlao4ni",
    "outputId": "a625c04b-91ea-48c8-b634-c1109a2bfc4d"
   },
   "outputs": [],
   "source": [
    "df.drop(columns='Alley')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H00jHiOzo-Fw"
   },
   "source": [
    "### Imputaci√≥n üß©\n",
    "\n",
    "Corresponde a las t√©cnicas que permiten rellenar de informaci√≥n faltante por medio de estimaciones.\n",
    "\n",
    "> **Pregunta ‚ùì:** ¬øSe les ocurren ejemplos de casos de mala imputaci√≥n?.\n",
    "\n",
    "La imputaci√≥n al igual que la eliminaci√≥n de columnas y/o filas conlleva problemas. Entre los problemas de este m√©todo podemos encontrar: a√±adir ruido a nuestros datos, a√±adir sesgo y data leakage (¬øPor qu√©?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2LjIvpTpDCZ"
   },
   "source": [
    "#### Imputaci√≥n Simple\n",
    "\n",
    "Por lo general este tipo de imputaci√≥n presenta un buen rendimiento emp√≠rico en tareas de ciencia de datos y es ampliamente recomendado.\n",
    "\n",
    "> **Nota:** aplicar este tipo de m√©todos puede afectar el calculo de varianzas y covarianzas.\n",
    "\n",
    "En `pandas` podemos aplicar este tipo de imputaci√≥n por medio del m√©todo `.fillna()` ya sea entregando un valor precalculado (media, mediana, moda, etc...) o utilizando los argumentos `ffill` (usar el valor anterior)  y `bfill` (usar el valor siguiente).\n",
    "En `scikit-learn` por otra parte, podemos utilizar [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer), el cual es una transformaci√≥n, lo que hace este m√©todo compatible con los `Pipelines`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EFjmvaOpGHX"
   },
   "source": [
    "Existen directrices a tener en cuenta al momento de tratar valores faltantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ViEOFk7pIe0"
   },
   "source": [
    "**Variables categ√≥ricas**:\n",
    "\n",
    "    * Transformar valores faltantes en una nueva categor√≠a.\n",
    "    * Utilizar c√≥dificaci√≥n Dummy en variables categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1225,
     "status": "ok",
     "timestamp": 1713367914377,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "vmcuEeLSo68_",
    "outputId": "072f76af-306b-4efb-cac1-3f517f5bab40"
   },
   "outputs": [],
   "source": [
    "df[\"GarageType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1713367914377,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "CVty4-evpOw0",
    "outputId": "1e4401d0-241a-4a51-df2d-7ce6928941da"
   },
   "outputs": [],
   "source": [
    "df[\"GarageType\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1713367914377,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "YXztkBqspRUz",
    "outputId": "671da6c9-2b02-4d2a-afe9-4cccd3d210c0"
   },
   "outputs": [],
   "source": [
    "df[df[\"GarageType\"].isna()][\"GarageType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "auJM3_BGpSY9",
    "outputId": "316df807-3494-4b44-902a-5b4bcc9b5e3f"
   },
   "outputs": [],
   "source": [
    "garage_type = df[\"GarageType\"].fillna(\"NoGarage\")\n",
    "garage_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7i22h_YpUVy"
   },
   "source": [
    "**Valores Ordinales**\n",
    "\n",
    "    * Agregar la categor√≠a de valor faltante como orden inicial o final en categorias ordinales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "afGto8gZpmSJ",
    "outputId": "b00f06db-02a8-4cfe-e570-a7f669737469"
   },
   "outputs": [],
   "source": [
    "# ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "df[\"GarageCond\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "H5E9zOjcpmuj",
    "outputId": "c6f3fce4-c67b-48cc-fc98-04e1c8f8f070"
   },
   "outputs": [],
   "source": [
    "df[\"GarageCond\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "sLxPJUSTpotq",
    "outputId": "98159f4b-7541-4223-a5bd-197534779893"
   },
   "outputs": [],
   "source": [
    "garage_cond = df[\"GarageCond\"].fillna(\"NoGarage\")\n",
    "# Agregar despu√©s al Encoder NoGarage ['NoGarage', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "garage_cond.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMjAtsOYprla"
   },
   "source": [
    "**Variables num√©ricas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "0Cmz0GcBptvP",
    "outputId": "608cdfa2-a449-4197-a4ba-83e3b71998fe"
   },
   "outputs": [],
   "source": [
    "df[df[\"LotFrontage\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "YcOfe4LWpuJA",
    "outputId": "d941863c-84be-453f-f08e-e897ec2d0305"
   },
   "outputs": [],
   "source": [
    "df[\"LotFrontage\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdLtKKIgpwrg"
   },
   "source": [
    "En este caso, usaremos `SimpleImputer`.\n",
    "\n",
    "La estrategia se define por el par√°metro:\n",
    "\n",
    "`strategy`, default=‚Äômean‚Äô\n",
    "\n",
    "    The imputation strategy.\n",
    "\n",
    "        If ‚Äúmean‚Äù, then replace missing values using the mean along each column. Can only be used with numeric data.\n",
    "\n",
    "        If ‚Äúmedian‚Äù, then replace missing values using the median along each column. Can only be used with numeric data.\n",
    "\n",
    "        If ‚Äúmost_frequent‚Äù, then replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n",
    "\n",
    "        If ‚Äúconstant‚Äù, then replace missing values with fill_value. Can be used with strings or numeric data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "EIyBKX0Ep1jL",
    "outputId": "90324872-678b-40a8-9e2b-7a60bcd7be75"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "imputed_lotfrontage = si.fit_transform(df.loc[:, [\"LotFrontage\"]])\n",
    "\n",
    "imputed_lotfrontage = pd.DataFrame(imputed_lotfrontage, columns=[\"ImputedLotFrontage\"])\n",
    "\n",
    "imputed_lotfrontage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "kgVgCOksp2zq",
    "outputId": "8019d8a5-54bc-4ccd-ff69-0c8d5a504aa4"
   },
   "outputs": [],
   "source": [
    "imputed_lotfrontage.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "R1j4UrC9p3ut",
    "outputId": "577b36ee-e923-49cd-878e-2afe0451ddb1"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "lf = pd.concat((df.loc[:, [\"LotFrontage\"]], imputed_lotfrontage))\n",
    "px.histogram(lf, barmode=\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccT5Di7pp67j"
   },
   "source": [
    "#### Imputaci√≥n Multivariada\n",
    "\n",
    "Un problema de la imputaci√≥n singular es que modela los datos como uno completo, sin considerar la incertidumbre inherente a todos los otros datos. Una soluci√≥n para esto son los m√©todos de imputaci√≥n m√∫ltiple.\n",
    "\n",
    "Un m√©todo de inmputaci√≥n multiple estima los valores faltantes a partir de otros. Puede ser tanto univariada (solo considerando la variable objetivo) como multivariada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc_E0Jy1qQwI"
   },
   "source": [
    "##### KNNImputer\n",
    "\n",
    "Imputa usando k-Nearest Neighbors.\n",
    "\n",
    "Los valores imputados se calculan seg√∫n el par√°metro `weights`:\n",
    "\n",
    "- Si `weights=uniform`, se promedian los valores de los vecinos cercanos.\n",
    "- Si `weights=distance`, se ponderan los valores de los vecinos cercanos seg√∫n la distancia al punto.\n",
    "\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/15-Feature-Engineering-Parte-II/knn.png?raw=true' width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "_wkWR-w8QauR",
    "outputId": "87cf82e4-cf3f-4305-f9f3-3be453a722d0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "only_numeric = df.select_dtypes(include=np.number).drop(\n",
    "    columns=[\n",
    "        \"YearBuilt\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"YrSold\",\n",
    "        \"GarageYrBlt\",\n",
    "        \"SalePrice\",\n",
    "        \"MoSold\",\n",
    "        \"MSSubClass\",\n",
    "    ]\n",
    ")\n",
    "only_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ktqtv9gkqTtP"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# en este caso, simplemente se promedian los valores cercanos.\n",
    "KNNimputer = KNNImputer(n_neighbors=2, weights=\"uniform\") # notar como puedo definir el n¬∞ de vecinos\n",
    "KNN_imputed_data = KNNimputer.fit_transform(only_numeric)\n",
    "KNN_imputed_data = pd.DataFrame(KNN_imputed_data, columns=only_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "dIN1bjtqqUia",
    "outputId": "640db072-a7da-4cfe-90ae-060fc76b409f"
   },
   "outputs": [],
   "source": [
    "lf_imputed = KNN_imputed_data.loc[:, [\"LotFrontage\"]]\n",
    "lf_imputed.columns = [\"LotFrontageImputed\"]\n",
    "\n",
    "lf = pd.concat((df.loc[:, [\"LotFrontage\"]], lf_imputed))\n",
    "px.histogram(lf, barmode=\"group\", nbins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFYh2dbKqjiZ"
   },
   "source": [
    "### Incluyendo la imputaci√≥n en nuestra Pipeline\n",
    "\n",
    "Pongamos en pr√°ctica todo lo que hemos aprendido en esta clase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APjDBzlfCkD-"
   },
   "source": [
    "> **Ejercicio üìù**\n",
    "\n",
    "0. Primero separemos la columna `SalePrice` del dataframe. Ejecute el siguiente c√≥digo:\n",
    "\n",
    "```python\n",
    "target = 'SalePrice'\n",
    "y = df[target].copy()\n",
    "X = df.drop(columns = target).copy()\n",
    "```\n",
    "\n",
    "1. Usando el dataframe `X`, cree una lista conteniendo variables num√©ricas y otra lista conteniendo variables categ√≥ricas.\n",
    "2. Para cada tipo de variable, genere un `pipeline`. En espec√≠fico:\n",
    "  - Num√©ricas:\n",
    "    - Imputaci√≥n multivariada con KNN\n",
    "    - Escalamiento Robusto\n",
    "  - Categ√≥ricas:\n",
    "    - Imputaci√≥n univariada (la que ustedes prefieran)\n",
    "    - Transformaci√≥n a One Hot\n",
    "      - *Qu√© opci√≥n hab√≠a que definir para evitar un \"sparse output\"?*\n",
    "3. Con los pipelines creados, use un `ColumnTransformer` para englobarlos en un mismo paso.\n",
    "4. Genere un `pipeline` que contenga como primer paso la transformaci√≥n definida en el `ColumnTransformer`. Pru√©belo sobre los datos e imprima la salida.\n",
    "5. (Opcional) Vuelva a definir el `pipeline` anterior, pero definiendo como segundo paso el modelo de regresi√≥n que ustedes prefieran. Genere una predicci√≥n de  `SalePrice` y cuantifique el performance de su modelo usando [Mean Absolute Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error).\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "sPET2w1_IxMH"
   },
   "outputs": [],
   "source": [
    "# 0.\n",
    "target = 'SalePrice'\n",
    "y = df[target].copy()\n",
    "X = df.drop(columns = target).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "gxfPtNCfCq4T"
   },
   "outputs": [],
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "rhW3Siw5CzXs"
   },
   "outputs": [],
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "q_traBRKC27F"
   },
   "outputs": [],
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "_QacZ-0WC4LE"
   },
   "outputs": [],
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "Or2dEiIvC43A"
   },
   "outputs": [],
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iV_UXauNqtRX"
   },
   "source": [
    "## Ya pero quiero aprender mas... ¬øalgo para leer? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3x2oPcCqwRh"
   },
   "source": [
    "- Para comenzar pueden visualizar con mayor profundidad la documentaci√≥n de Scikit-Learn enfocada en la extracci√≥n de features: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "- Complementar la clase leyendo el capitulo 5 del libro [Designing Machine Learning Systems](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969)\n",
    "\n",
    "- Leer capitulo 1 del libro [Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and MLOps](https://www.amazon.com/-/es/Valliappa-Lakshmanan/dp/1098115783/ref=pd_bxgy_img_sccl_1/142-9514380-0202369?pd_rd_w=JQSaa&content-id=amzn1.sym.26a5c67f-1a30-486b-bb90-b523ad38d5a0&pf_rd_p=26a5c67f-1a30-486b-bb90-b523ad38d5a0&pf_rd_r=BJE9WMJ9X1QQMYF4C3EJ&pd_rd_wg=y78Jr&pd_rd_r=6a1994c2-2734-428a-a270-ded23f892987&pd_rd_i=1098115783&psc=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
