{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrAEIjr4m_0"
   },
   "source": [
    "# Clase 5 - Feature Engineering 📎\n",
    "\n",
    "- MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_SoNsDt-JEY"
   },
   "source": [
    "## Objetivos de la Clase 🎯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFXUMGNe-IMq"
   },
   "source": [
    "- Entender qué es el feature engineering y cómo puede impactar en el performance de los modelos\n",
    "- Aprender a implementar técnicas de feature engineering a través de Scikit-learn usando `Pipeline` y `ColumnTransformer`\n",
    "- Entender y resolver la complejidad de datos faltantes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYa6XjsD9fDH"
   },
   "source": [
    "## ¿Qué es Feature Engineering? 🧮"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xEZkmnQ89it"
   },
   "source": [
    "\n",
    "\n",
    "Feature engineering es el proceso de seleccionar, transformar y crear características relevantes de entrada para construir un modelo de aprendizaje automático preciso y eficiente.\n",
    "\n",
    "> **Pregunta ❓:** ¿Se necesita algún conocimiento previo para realizar Feature Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0_pBNdFxBNW"
   },
   "source": [
    "## Problema a visitar: House Pricing\n",
    "\n",
    "![House Pricing](https://www.kaggle.com/competitions/5407/images/header)\n",
    "\n",
    "Fuente: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "Al igual que en la clase anterior, el dataset **`house pricing`** consiste en 80 variables (79 variables explicativas más una variable objetivo) que describen aspectos fundamentales de hogares residenciales en la ciudad de *Ames, Iowa*.\n",
    "\n",
    "La variable objetivo es el precio final de cada hogar (regresión)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1995,
     "status": "ok",
     "timestamp": 1713367908990,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "efbl5YD5xBNW"
   },
   "outputs": [],
   "source": [
    "# Importamos librerías a utilizar en la clase\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1713367908991,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "QRgO-_DcxBNX"
   },
   "outputs": [],
   "source": [
    "# El conjunto a trabajar es el de entrenamiento\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/MDS7202/MDS7202/main/recursos/2023-01/13-EDA//train.csv\",\n",
    "    index_col=\"Id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K1fsLxTxBNY"
   },
   "source": [
    "> **Actividad 📎**: Imaginen que nos pasan estos datos y nos señalan que el conjunto de datos esta compuesto por:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzrbD1hdxBNY"
   },
   "source": [
    "- Terreno 🏔️\n",
    "        \"LotArea\",  # Area del terreno\n",
    "        \"LandSlope\",  # Pendiente del terreno\n",
    "        \"Neighborhood\",  # Barrio\n",
    "- Metadatos de la Vivienda 📆\n",
    "        \"BldgType\",  # Tipo de vivienda\n",
    "        \"YearBuilt\",  # Año de construcción\n",
    "        \"YearRemodAdd\",  # Año de remodelación\n",
    "        \"Utilities\",  # Agua, luz, etc...\n",
    "- Materiales 🧱\n",
    "        \"Foundation\",  # Fundación de la vivienda\n",
    "        \"RoofMatl\",  # Material del techo\n",
    "        \"RoofStyle\",  # Estilo del techo\n",
    "        \"Exterior1st\",  # Material del Exterior\n",
    "        \"ExterCond\",  # Condición del material exterior\n",
    "- Interior de la casa 🏡\n",
    "        \"GrLivArea\",  # Area habitable sobre el nivel del suelo.\n",
    "        \"1stFlrSF\",  # Area primer piso\n",
    "        \"2ndFlrSF\",  # Area segundo piso\n",
    "        \"FullBath\",  # Baños completos\n",
    "        \"HalfBath\",  # Baños de visita?\n",
    "        \"BedroomAbvGr\",  # Piezas\n",
    "        \"KitchenAbvGr\",  # Cocinas\n",
    "        \"KitchenQual\",  # Calidad de la cocina\n",
    "- Sótano 🪨\n",
    "        \"TotalBsmtSF\",  # Total sótano\n",
    "        \"BsmtCond\",  # Condición del sótano\n",
    "- Garaje 🚗\n",
    "        \"GarageType\",  # Tipo de garaje\n",
    "        \"GarageCars\",  # Cantidad de autos por garaje\n",
    "- Piscina 🤽‍♂️\n",
    "        \"PoolArea\",  # Area de la piscina\n",
    "        \"PoolQC\",  # Calidad de la piscina\n",
    "- Calefacción y Aire 🌦️\n",
    "        \"Heating\",  # Calefacción\n",
    "        \"HeatingQC\",  # Calidad de la Calefacción\n",
    "        \"CentralAir\",  # Aire Acondicionado Central\n",
    "- Calidad y Condición 🌟\n",
    "        \"OverallQual\",  # Calidad general\n",
    "        \"OverallCond\",  # Condición general actual\n",
    "- Datos de la venta  💵\n",
    "        \"SaleType\",  # Tipo de venta\n",
    "        \"SaleCondition\",  # Condición de la vivienda en la venta\n",
    "        \"SalePrice\",  # Precio de la venta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1713367908991,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ddGKTFmtxBNY",
    "outputId": "c740591a-bb75-4572-a5bc-cb39ef0b422d"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué deberíamos hacer ahora?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yUaME55xBNY"
   },
   "source": [
    "### ¿Por qué es importante el Feature Engineering? 🤨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K70OhJ0uxBNY"
   },
   "source": [
    "Muchos de los algoritmos de aprendizaje automatico **poseen un mejor desempeño cuando los valores de las features** (aka columnas) **se transforman a un valor facil de interpretar por los modelos**. Por otro lado, datos sucios pueden entorpecer las predicciones generadas por nuestros modelos, al igual que las escalas en que se presentan los datos. Por esto, **es relevante que las features que utilizemos se encuentren en escalas similares y con distribuciones relativamente similares a la distribución normal**.\n",
    "\n",
    "> Importante ❗: Gran parte de los modelos que se generan en la industria son centrados en datos, a diferencia de la academia el trabajo no se centra en el desarrollo de modelos ultra complejos, sino en modelos completamente estandarizados y por ello se necesita un mayor tiempo en la extracción de features desde los datos. Fuente interesante: [Data-Centric Approach vs Model-Centric Approach in Machine Learning](https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning).\n",
    "\n",
    "Consideren los siguientes ejemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABWqISGWxBNY"
   },
   "source": [
    "**Ejemplo 1**\n",
    "\n",
    "- El gráfico de la izquierda se grafica una variable con respecto a otra sin escalar\n",
    "- El gráfico de la derecha muestra ambas variables estandarizadas\n",
    "\n",
    "> **Pregunta ❓:** ¿Es entendible para un humano el gráfico de la derecha?¿Qué se puede interpretar de este?¿Qué efectos tendrá usar las variables no escaladas (izquierda) vs las escaladas (derecha) usando algún modelo predictivo basado en distancias?\n",
    "\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/14-Feature-Engineering-Parte-I/escalamiento_2.png?raw=true' width=1000 />\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html'>Compare the effect of different scalers on data with outliers en el User Guide de Scikit-learn</a>.\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "    Los Datos son del dataset <a href='https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset'>California Housing dataset</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVsxeqJKxBNZ"
   },
   "source": [
    "**Ejemplo 2**\n",
    "\n",
    "- El gráfico de la derecha se grafica una variable un conjunto de variables en 2 dimensiones diferenciadas por la clase.\n",
    "- El gráfico de la izquierda se grafica el mismo grafico pero con el conjunto de variables transformado\n",
    "\n",
    "> **Pregunta ❓:** ¿Cual de los dos conjuntos de datos es mas facil de separar?¿Que efectos tendría utilizar las variables en el estado original contra el estado final para algún modelo lineal?.\n",
    "\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/14-Feature-Engineering-Parte-I/feature-engineering1.jpg?raw=true' width=1000 />\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://www.kdnuggets.com/2018/12/feature-engineering-explained.html'>Feature Engineering for Machine Learning: 10 Examples</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f77XXaFh9r-D",
    "tags": []
   },
   "source": [
    "### ¿Pero que hay del Deep Learning, no que solucionaba este problema? 😣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUafO1u69q6R"
   },
   "source": [
    "Si bien uno de los aspectos más relevantes del Deep Learning es la extracción automática de las features desde los datos, sin embargo:\n",
    "\n",
    "- El desarrollo actual de deep learning no nos permite abstraer todas las features desde los datos.\n",
    "- Muchas empresas no utilizan deep learning para generar modelos, ya sea por capacidad o simplemente porque no lo necesitan. **Recalcar, el deep learning no es la solución para todo!** [Ejemplo](https://arxiv.org/pdf/2106.03253.pdf)\n",
    "- Deep learning suele tener buenos resultados en datos no estructurados, sin embargo, en datos tabulares no siempre es el caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vdg2OynTEp7F"
   },
   "source": [
    "## Veamos nuestra librería core: Scikit-Learn ⚒️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt5zcy-yFvrE"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MDS7202/MDS7202/a60ef458182c2f26af3aaf4f8e0446a8512d4f75/clases/2022-01/15_Preprocesamiento_Intro_a_Scikit-Learn/resources/scikit-learn.png\" width=400/>\n",
    "\n",
    "[`scikit-learn`](https://scikit-learn.org/stable/) es probablemente una de las librerías de Aprendizaje Automático más populares para Python. *Open-source* y construida sobre `numpy`, `scipy` y `matplotlib`, ofrece interfaces y flujos de trabajos (*frameworks*) simples y eficientes para construir aplicaciones enfocadas análisis de datos y predicción.\n",
    "Sus *APIs* permite generar código limpio y está provista de una extensa documentación.\n",
    "\n",
    "> Parentesis: API: **Application Programming Interface / Interfaz de programación de aplicaciones** - Son las interfaces comunes (funciones, objetos, métodos, etc..) que un software o librería ofrece para comunicarse con el resto. Esta define entre otras cosas: el tipo de llamadas o funciones que pueden ser hechas, como hacerlas, el tipo de datos de entrada y salida, etc...\n",
    "\n",
    "Una gran ventaja de `Scikit-learn` consiste en su estructura transversal de clases y herencia. La mayoría de clase pertenece a alguna de estas dos categorías:\n",
    "\n",
    "* **`transformers`**: Permite **transformar datos input antes de utilizar algoritmos de aprendizaje sobre ellos**. A partir de este módulo, se puede realizar imputaciones de valores faltantes, estandarización de variables, escalamientos y seleccion de caracterísiticas por medio de algoritmos especializados. Esto comunmente se logra a través de las interfaces\n",
    "    - `fit` que permite aprender los parámetros de la transformación, por ejemplo la media y varianza en la normalización.\n",
    "    - `transform` que aplica la transformación a los datos.\n",
    "    - `fit_transform` permite ambas operaciones al mismo tiempo.\n",
    "\n",
    "* **`estimators`**: Proveen los **algoritmos** de aprendizaje automático a través de los métodos `fit` y `predict`.\n",
    "\n",
    "El método usual de importación se basa en seleccionar un submódulo de la librería indicando (de manera opcional) el objeto que se utilizará. Por ejemplo, si se desea utilizar el escalador de datos Min-Máx del submódulo `preprocessing`, se haría de la manera usual, por medio de:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "> **Nota**: No se recomienda importar la librería completa `import sklearn as sk` pues su estructura de submódulos es suficientemente grande, como para considerar cada uno como una librería.\n",
    "\n",
    "\n",
    "A lo largo del curso se estudiarán distintos componentes de esta librería. Durante esta clase nos centraremos en los módulos `preprocessing`, `compose` y `pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62-sDVIg39ya"
   },
   "source": [
    "## Operaciones Comunes de Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XnmMHqNxBNa"
   },
   "source": [
    "Debido a la importancia que posee de la etapa de Feature Engineering en los proyectos de ML, se han desarrollado muchas técnicas para agilizar el proceso.\n",
    "\n",
    "Algunas de las operaciones que podemos realizar con el módulo **`transformers`** incluyen:\n",
    "\n",
    "- **Creación de nuevas Features** a partir de operaciones usando los datos disponibles.\n",
    "- **Transformaciones** como las vistas en la clase de preprocesamiento (escalamiento, normalización, one hot encoding para variables categóricas etc...).\n",
    "- **Reducción de Características** en la que se combinan/reducen características redundantes (usando por ejemplo, PCA).\n",
    "- **Selección de Características** en la que a partir de diversos criterios se seleccionan las características que más aportan al modelo.\n",
    "\n",
    "El proceso de generar y preprocesar las features requiere mucha creatividad y al mismo conocimiento del dominio del problema.\n",
    "\n",
    "Para efectos de esta clase revisaremos las siguientes:\n",
    "\n",
    "- **Escalamiento de variables**\n",
    "- **Codificación de características categóricas**\n",
    "- **Missing Values**\n",
    "\n",
    "Sin embargo, a pesar de que no forma parte de un proceso de feature Engineering tradicional, comentaremos que es el **data-drift** y porque es importante que lo midas al momento de trabajar con tus variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3V965Cp_xBNa",
    "tags": []
   },
   "source": [
    "## Escalamiento\n",
    "\n",
    "Un paso importante antes de introducir features en los modelos, es el escalamiento de las variables. El objetivo de esto es que el dominio de las variables sea similar y de esta forma obtener mejores resultados. Este proceso es una de las cosas más sencillas que se pueden hacer y que (por lo general) se traduce en un aumento del rendimiento del modelo. Ojo que no hacer esto puede hacer que su modelo no tenga sentido como es el caso de algoritmos clásicos.\n",
    "\n",
    "Tomar en consideración que muchos de los algoritmos modernos como **XGBoost** señalan que no necesitan escalamiento para el entrenamiento, sin embargo, escalar las variables de entrada puede impactar positivamente en el poder predictivo del modelo.\n",
    "\n",
    "Para realizar el escalamiento utilizaremos el módulo `sklearn.preprocessing`, este entrega diversas técnicas de escalamiento, normalización y estandarización de datos a través de clases `Transformers` (no confundir con los transformers de Deep Learning).\n",
    "\n",
    "### Estandarización\n",
    "\n",
    "La estándarización es una de las transformaciones mas relevantes a tener presente durante el modelamiento. Esto debido a que un gran cantidad de algoritmos de aprendizaje automático / estadístico, asumen que los datos a operar se encuentran **distribuidos de manera normal**. **Si los datos no se distribuyen normalmente y contienen valores atípicos**, **es posible que la media y la desviación típica no reflejen con exactitud la tendencia central y la variabilidad de los datos**.\n",
    "\n",
    "El objeto `StandarScaler` permite estandarizar datos para que tengan **media = 0 y desviación stándar = 1**. \n",
    "\n",
    "**Importante:** StandardScaler solo estandariza los datos, pero **no los normaliza!** Por lo tanto, sólo deben ocuparlo cuando:\n",
    "- El algoritmo que ocupan requiere datos con distribución gaussiana\n",
    "- Sus datos tienen distribución gaussiana.\n",
    "\n",
    "**Ejemplo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Qed6IUY4NlL"
   },
   "source": [
    "Comencemos graficando el histograma de algunas variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1713367909501,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "sm33HTMPxBNf",
    "outputId": "19cb6c24-24fc-4a3a-95f6-ac3f7a4db676"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "cols_to_estandarize = [\n",
    "    'OverallQual',\n",
    "    'OverallCond',\n",
    "    'GarageCars',\n",
    "    'GarageArea',\n",
    "    'GrLivArea'\n",
    "]\n",
    "fig1 = px.histogram(df[cols_to_estandarize].melt(),\n",
    "             x='value',\n",
    "             color='variable',\n",
    "             barmode='group')\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKdUrD7yxBNf"
   },
   "source": [
    "La transformación a aplicar es mover todos los datos a una distribución normal con media 0 y varianza 1.\n",
    "\n",
    "Para esto, por cada dato:\n",
    "    \n",
    "$$ z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "En donde $\\mu$ es la media de la columna y $\\sigma$ es la desviación estándar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIx90ox0xBNg"
   },
   "source": [
    "Para esto, importamos escalador, lo inicializamos y luego ejecutamos `fit_transform` sobre los datos.\n",
    "Notese que esto retorna un arreglo numpy con los datos escalados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1713367909502,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "JfiYyrJr4rfy",
    "outputId": "ae434b5e-ea5d-4053-b70f-22381ee90152"
   },
   "outputs": [],
   "source": [
    "data_to_standarize = df.loc[:, cols_to_estandarize] # separar data\n",
    "data_to_standarize.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1713367909502,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "bY1rKZrvxBNg",
    "outputId": "6886b528-f7b9-4616-e544-1081c25f1897"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "standard_scaler = StandardScaler() # inicializar scaler\n",
    "standarized_df = standard_scaler.fit_transform(data_to_standarize) # escalar datos\n",
    "\n",
    "standarized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEqpPtA7xBNg"
   },
   "source": [
    "Por comodidad, convertimos los datos escalados a un Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1713367909502,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "7BR3xPORxBNg",
    "outputId": "39cd5d18-181e-4fac-dbca-17075759b440"
   },
   "outputs": [],
   "source": [
    "standarized_df = pd.DataFrame(standarized_df, columns=cols_to_estandarize)\n",
    "standarized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1713367910004,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "zvecCsEtxBNg",
    "outputId": "a5559bc7-4a58-43e4-a950-a88f4986563f"
   },
   "outputs": [],
   "source": [
    "standarized_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypt7zUtbxBNg"
   },
   "source": [
    "Veamos ahora como se muestran las distribuciones de los datos estandarizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd2mT7X_xBNh"
   },
   "source": [
    "\n",
    "Al aplicar `.fit_transform()` se obtienen los parámetros de media `.mean_` y desviación estándar `.scale_` para cada columna del dataframe operado. Observe que tales atributos del objeto tipo `StandardScaler` son públicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1713367910004,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "31JX_zHyxBNh",
    "outputId": "56a4e387-4fbf-47ec-d005-6edbafdc0cdd"
   },
   "outputs": [],
   "source": [
    "standard_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1713367910004,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "qLzQjYFqxBNh",
    "outputId": "20e39df6-1674-4f95-f78d-21cef984a61b"
   },
   "outputs": [],
   "source": [
    "standard_scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1713367910004,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "qqrvWm0txBNi",
    "outputId": "5819d83d-64e8-4534-f13a-7d5b120424ed"
   },
   "outputs": [],
   "source": [
    "standard_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1713367910004,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "jdlHHPJkxBNi",
    "outputId": "f844c44d-04b2-4848-be8d-0a527e7692cd"
   },
   "outputs": [],
   "source": [
    "px.histogram(standarized_df[cols_to_estandarize].melt(),\n",
    "             x='value',\n",
    "             color='variable',\n",
    "             barmode='group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "executionInfo": {
     "elapsed": 1959,
     "status": "ok",
     "timestamp": 1713367911957,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "GZGp1Dp2xBNj",
    "outputId": "51c882ff-e35a-4c5c-a39a-39569314c4f2"
   },
   "outputs": [],
   "source": [
    "standarized_df.plot.kde(figsize=(16,9), bw_method=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhMGFNlZxBNk"
   },
   "source": [
    "> **Pregunta ❓**: ¿Qué sucede con las variables que se alejan mucho de ser normales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1713367911957,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "jeKq7WDrxBNk",
    "outputId": "08dee746-901f-420e-d110-0e6e71230594"
   },
   "outputs": [],
   "source": [
    "px.histogram(df, x='BsmtUnfSF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0kLBzzSxBNl"
   },
   "source": [
    "### Escalamiento mínimo-máximo\n",
    "\n",
    "Una buena alternativa al método anterior, es el escalamiento por rango, este tiene la forma:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_{i} - \\min(x)}{\\max (x)-\\min (x)}\n",
    "\\end{equation}\n",
    "\n",
    "para $x$ columna a tratar, $x_i$ elemento a transformar. Esta transformación permite hacer que los datos se muevan entre 0 y 1 y puede ser utilizado sobre una distribución de datos no normales.\n",
    "\n",
    "> **Nota:** este transformador se ve afectado por la **presencia de outliers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1230,
     "status": "ok",
     "timestamp": 1713367913181,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "uPjj78MwjOrT"
   },
   "outputs": [],
   "source": [
    "cols_to_scale = ['BsmtUnfSF', 'Fireplaces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1713367913181,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "VpJUlFckxBNl",
    "outputId": "b7c470f6-fc38-4a50-c12d-554b23ff53b4"
   },
   "outputs": [],
   "source": [
    "px.histogram(df, 'Fireplaces') # notar que solo tiene 3 valores posibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1713367913181,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ZhbqbEkXxBNl",
    "outputId": "a82c1555-df5c-4e2c-d4ee-ef7eae000ed5"
   },
   "outputs": [],
   "source": [
    "# BsmtUnfSF = Unfinished square feet of basement area\n",
    "df.loc[:, 'BsmtUnfSF'].plot.hist(backend='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "Uug-kRGk_bLR",
    "outputId": "73d05a4d-a831-4112-c50a-8e0b47406c44"
   },
   "outputs": [],
   "source": [
    "data_to_scale = df.loc[:, cols_to_scale] # separar datos\n",
    "data_to_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "K-v6qaZTxBNm",
    "outputId": "3106e1d2-ed27-46c5-c7e1-0c55692ec2dd"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax_scaler = MinMaxScaler() # inicializar scaler\n",
    "\n",
    "scaled_data = minmax_scaler.fit_transform(data_to_scale) # escalar datos\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=cols_to_scale) #  transformar a dataframe\n",
    "\n",
    "scaled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "Vk_71nKRxBNm",
    "outputId": "4c4dd0c3-856c-4e8d-88f3-a20c26f56e46"
   },
   "outputs": [],
   "source": [
    "scaled_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "cb320FuVxBNm",
    "outputId": "0a683e6d-8e13-4e67-9b63-d1ac49844025"
   },
   "outputs": [],
   "source": [
    "scaled_data.plot.hist(backend='plotly', barmode='overlay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__7JsP5QxBNm"
   },
   "source": [
    "Comprobamos minimos y máximos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "t3VtAIDPxBNm",
    "outputId": "181fcc79-1be8-4965-d2a6-cc32e772ab10"
   },
   "outputs": [],
   "source": [
    "scaled_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "zlo3YMkCxBNm",
    "outputId": "89203e18-7ec2-4a1e-ddf3-196e72df3eb4"
   },
   "outputs": [],
   "source": [
    "scaled_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfaCz-GgxBNn"
   },
   "source": [
    "**Nota**: Si se desea escalar por rango, la mejor práctica es comprender los mínimos y máximos *absolutos* para cada columna. Esto se refiere, a las cotas superiores e inferiores que posee la columna **por definición**. A modo de ejemplo, considere un dataframe con las notas de una asginatura donde se enzeña análisis de datos, se sabe que la nota máxima en cierto ítem se codifica en una columna y su máximo en efecto es 7.0. Sin embargo el mínimo en dicha columna es 1.5, que es distinto al mínimo natural para dicho item que es 1.0. Esto puede acarrear problemas con datos nuevos, sobretodo si aparece una nota inferior a 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwTkjM2zxBNn"
   },
   "source": [
    "### Outliers\n",
    "\n",
    "Un outlier es un atípicos que están fuera del rango comun del resto de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "W6_0qQwqxBNn",
    "outputId": "e47f1d29-7628-4d68-e619-40f996cc9742"
   },
   "outputs": [],
   "source": [
    "# histograma con los datos brutos\n",
    "px.histogram(df, x='LotArea', marginal='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1713367913182,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "bQD0nOHDxBNn",
    "outputId": "d54dfaab-ce4d-4442-c1c3-ffc4913b2cfb"
   },
   "outputs": [],
   "source": [
    "# histograma con los datos escalados con MinMax\n",
    "minmax_scaler = MinMaxScaler()\n",
    "scaled_data = minmax_scaler.fit_transform(df.loc[:, ['LotArea']])\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=['LotArea'])\n",
    "scaled_data.plot.hist(backend='plotly', barmode='overlay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMoxybqJxBNn"
   },
   "source": [
    "> **Pregunta:** ¿Qué pasa cuando queremos estandarizar pero tenemos outliers?\n",
    "\n",
    "**Respuesta**: La gran mayoría de los datos tienen a quedar en un rango muy acotado. En el caso anterior, hacia la izquierda.\n",
    "\n",
    "Hagamos el siguiente ejercicio: comparemos los estadísticos de los datos con y sin filtro IQR (datos contenidos en el rango intercuartílico):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "r1EosryDxBNn"
   },
   "outputs": [],
   "source": [
    "# media con los todos los datos de LotArea\n",
    "original = round(df['LotArea'].describe(), 3)\n",
    "original.name = 'LotArea Original'\n",
    "\n",
    "# datos presentes solo en el rango intercuantílico\n",
    "q1 = df['LotArea'].quantile(.25)\n",
    "q3 = df['LotArea'].quantile(.75)\n",
    "mask = df['LotArea'].between(q1, q3, inclusive='both')\n",
    "iqr = df.loc[mask, 'LotArea']\n",
    "iqr.name ='LotArea Filtro IQR'\n",
    "iqr = round(iqr.describe(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KF4Z6u5xBNo"
   },
   "source": [
    "Observen las diferencias entre las medias y las desviaciones estándar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "z5_bMwn9xBNo",
    "outputId": "fd7f1787-da6b-4014-92ae-a1f89668454b"
   },
   "outputs": [],
   "source": [
    "desc = pd.concat([original, iqr], axis=1)\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmy5kJGOxBNo"
   },
   "source": [
    "Si normalizamos usando estandarización original, cada dato va a ser dividido por 9981, **9 veces más grande que la versión sin outliers usando IQR!**. Esto en términos prácticos hará que los datos tiendan a concentrarse mucho más **cercanos a la media**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c6mxWSsxBNq"
   },
   "source": [
    "### Transformación Robusta\n",
    "\n",
    "Cuando se trabaja con columnas que poseen valores fuera de rango (**outliers**) las transformaciones anteriores pueden fallar. En este caso, se recomienda utilizar una transformación similar a la estandarización, pero que trabaje sobre la mediana como media y el rango intercuantílico como desviación estándar:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_i - Q_2(x)}{Q_3(x) - Q_1(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Donde $IQR = Q_3(x) - Q_1(x)$ es el rango intercuartílico de la columna $x$.\n",
    "\n",
    "En otras palabras, por cada ejemplo se sustrae la mediana y se divide por el rango intercuartil 75%- 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "Zv_yK0fUxBNq",
    "outputId": "3ce03eb9-d481-42ec-f302-a3dfd86e52d9"
   },
   "outputs": [],
   "source": [
    "# Ejemplo para obtener IQR\n",
    "data = np.array([1, 14, 19, 20, 22, 24, 26, 47])\n",
    "\n",
    "# Calculamos rango intercurtil\n",
    "q3, q1 = np.percentile(data, [75 ,25])\n",
    "q3 - q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQAXf1XCxBNq"
   },
   "source": [
    "**Ejemplo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-04lcwsxBNq"
   },
   "source": [
    "Se importa el objeto `RobustScaler` y se aplica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "k9FX_MWPxBNq",
    "outputId": "b68039d7-bc05-4118-d3af-93e1f595716e"
   },
   "outputs": [],
   "source": [
    "px.histogram(df['LotArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "f5JfxuXKxBNq"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "lot_area_standarized = standard_scaler.fit_transform(df[['LotArea']])\n",
    "robust_scaled_lot_area = robust_scaler.fit_transform(df[['LotArea']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "iLFs0laRxBNq"
   },
   "outputs": [],
   "source": [
    "comparacion = np.concatenate([lot_area_standarized, robust_scaled_lot_area], axis=1)\n",
    "comparacion = pd.DataFrame(comparacion, columns=['Estandarizacion Común', 'Estandarización Robusta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "zSiK4omNxBNr",
    "outputId": "fb619329-de62-43ad-9963-d368ede465a4"
   },
   "outputs": [],
   "source": [
    "px.histogram(comparacion, marginal='box', barmode='overlay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzXAUPJixBNr"
   },
   "source": [
    "Podemos observar como los **datos en la estandarización robusta se distribuyen de forma mas amplia** que en caso de la estandarización normal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XvKI-XyxBNu"
   },
   "source": [
    "### ¿Qué aspectos deberíamos considerar al momento de realizar escalamientos? 😅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txf3PenBxBNu"
   },
   "source": [
    "Al momento de realizar escalamiento de cualquier tipo deben considerar los siguientes puntos:\n",
    "\n",
    "- **El escalamiento es una fuente de data leakage** (Revisar más abajo).\n",
    "- **Necesidad de entrenamiento ante data drift**. Muchos de los escalamientos necesitan el cálculo de un estadístico para realizar la transformación, esto implica que si los datos vistos durante el entrenamiento cambian en producción las transformaciones no serán las mejores y necesitarán un reentrenamiento de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ech6DIbhxBNv"
   },
   "source": [
    "---\n",
    "\n",
    "## Codificación de Variables Ordinales\n",
    "\n",
    "Para el manejo de adecuado de variables ordinales,  se recomienda expresar sus valores en función de códigos númericos. El transformer `OrdinalEncoder` permite transformar características categóricas en códigos enteros (números enteros) empezando desde 0 hasta N-1.\n",
    "\n",
    "Veamos algunas variables ordinales:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsQdAImYxBNv"
   },
   "source": [
    "BsmtQual: Evaluates the height of the basement\n",
    "\n",
    "       Ex\tExcellent (100+ inches)\n",
    "       Gd\tGood (90-99 inches)\n",
    "       TA\tTypical (80-89 inches)\n",
    "       Fa\tFair (70-79 inches)\n",
    "       Po\tPoor (<70 inches\n",
    "       NA\tNo Basement\n",
    "\n",
    "BsmtCond: Evaluates the general condition of the basement\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical - slight dampness allowed\n",
    "       Fa\tFair - dampness or some cracking or settling\n",
    "       Po\tPoor - Severe cracking, settling, or wetness\n",
    "       NA\tNo Basement\n",
    "\n",
    "       \n",
    "HeatingQC: Heating quality and condition\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "M-_tZFevxBNv",
    "outputId": "2e1b558b-d131-4e59-c76a-1c05a132d0cf"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "\n",
    "variables_ordinal = df[['BsmtCond', 'BsmtQual','HeatingQC']] # separamos las variables ordinales\n",
    "\n",
    "# Se entrena el codificador\n",
    "ordinales = enc.fit_transform(variables_ordinal.dropna()) # se transforma variables a codificacion ordinal\n",
    "ordinales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1713367913183,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "N2IaLxWfnT0Y",
    "outputId": "92c3586e-5de0-46d0-da4a-0c531572ce02"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(ordinales, columns = ['BsmtCond', 'BsmtQual','HeatingQC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzVqD2SExBNv"
   },
   "source": [
    "Se obtienen las categorías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "U5ZrEAqmxBNw",
    "outputId": "3d098038-4b79-4741-b1f1-a1f33d48e447"
   },
   "outputs": [],
   "source": [
    "enc.categories_ # verificamos el orden impuesto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3qwedcBxBNw"
   },
   "source": [
    "> **Pregunta ❓**: ¿Existe algún problema en estas codificaciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "-CaKjwd7xBNw",
    "outputId": "f4541e96-4312-464e-a25d-2b1bd710dbf0"
   },
   "outputs": [],
   "source": [
    "categories_order = [\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n",
    "]\n",
    "\n",
    "enc_2 = OrdinalEncoder(categories=categories_order)\n",
    "\n",
    "ordinales = enc_2.fit_transform(variables_ordinal.dropna())\n",
    "\n",
    "pd.DataFrame(ordinales, columns = ['BsmtCond', 'BsmtQual','HeatingQC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ED6Khw72nf8f",
    "outputId": "821faf3f-05f2-4001-d534-c29654f85a53"
   },
   "outputs": [],
   "source": [
    "enc_2.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBMrxpHBxBNw"
   },
   "source": [
    "> **Pregunta ❓**: Codificamos estas variables con `variables_ordinal.dropna()`. ¿Por qué tuvimos que botar los valores faltantes y que se puede hacer en este caso ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZutoRYzxBNw"
   },
   "source": [
    "> **Pregunta ❓**: ¿Cómo codificamos las variables que no tienen orden?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4KFKr98xBNw"
   },
   "source": [
    "## Codificación de Variables Categoricas\n",
    "\n",
    "Un problema común con la códificación ordinal es que las variables pasan a ser consideradas continuas por algoritmos de machine learning (en especial por la API *estimators* de scikit-learn). Para evitar esto es posible convertir cada categoría en una columna por si sola y asignar un 1 cuando esté presente.\n",
    "\n",
    "![One Hot Encoding](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/14-Feature-Engineering-Parte-I/ohe.png?raw=true)\n",
    "<center>Fuente: https://morioh.com/p/811a5d22bbca </center>\n",
    "\n",
    "Esto se puede llevar a cabo por medio del transformador ` OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "l18wuwtOxBNw",
    "outputId": "cb4c391b-432f-4881-b039-4126b2f33c23"
   },
   "outputs": [],
   "source": [
    "df['Foundation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "3hLMBncmxBNw",
    "outputId": "d81f2249-b829-47bd-b073-3c1aaacd7952"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder() # inicializamos encoder\n",
    "\n",
    "cod = ohe.fit_transform(df.loc[:, ['Foundation']]) # encodeamos categorias\n",
    "cod # notar que es una matriz sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "3-wDlosYxBNx",
    "outputId": "27d1f5e3-6960-4f4d-89ac-476e62e54293"
   },
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4p7epaIL7MH"
   },
   "source": [
    "### Cómo puedo transformar la matriz sparse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_6TzLw5MKgY"
   },
   "source": [
    "#### Transformando a array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "snOvpYi_Mp6n",
    "outputId": "6a15510c-6f59-4308-fb49-2fafa98c3892"
   },
   "outputs": [],
   "source": [
    "cod.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "MRsJoCIlxBNx",
    "outputId": "e22affc7-7be4-4ac1-ad91-a9304283ff04"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cod.toarray(), columns=ohe.categories_) # generar dataframe con categorias onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQhuDKfZMZFB"
   },
   "source": [
    "#### O simplemente definiendo `sparse_output = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "FNssAwVeMgFP",
    "outputId": "ef57db27-cb18-44c9-8e13-14b5e54d6f15"
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output = False) # inicializamos encoder\n",
    "cod = ohe.fit_transform(df.loc[:, ['Foundation']]) # encodeamos categorias\n",
    "\n",
    "cod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WngLwjXzOpXd"
   },
   "source": [
    "Volvamos a hacer lo mismo con `Neighborhood`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "Xx_zLTcsxBNx",
    "outputId": "61c91d42-1099-4aa0-fe1b-7353299adf89"
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'Neighborhood'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1713367913184,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "jQsPHoELxBNx",
    "outputId": "f22b8e2e-7a62-44ba-a85d-e41fbd642d95"
   },
   "outputs": [],
   "source": [
    "# Ahora, con los barrios\n",
    "ohe = OneHotEncoder(sparse_output = False) # inicializamos encoder con sparse_output = False\n",
    "categories = ohe.fit_transform(df.loc[:, ['Neighborhood']]) # noten como ahora no uso .toarray()\n",
    "\n",
    "pd.DataFrame(categories, columns = ohe.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A2I2BXzWrDA"
   },
   "source": [
    "## Pipelines\n",
    "\n",
    "La manera anterior es bastante clara de comprender, sin embargo, es redundante y repite muchos patrones de asignación tediosos. Los `Pipelines` están diseñados para resolver este problema.\n",
    "\n",
    "\n",
    "Las transformaciones en un dataset son combinadas entre si, hasta obtener una versión ordenada de los datos, posteriormente, estas se combinan con estimadores para formar un flujo de trabajo *input-output*. En Sckit-Learn el flujo antes nombrado de denomina *composite estimator* y se construye por medio de objetos tipo `Pipeline`.\n",
    "\n",
    "Pero mas importante aun, los Pipelines nos ayudan resolver el problema de **Data Leakage** de forma natural.\n",
    "\n",
    "Pero.. ¿Qué es **Data Leakage**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que queremos entrenar un modelo para predecir la variable `Accident`.\n",
    "\n",
    "Para eso, un colega nos facilita código para pre procesar los datos y entrenar un modelo:\n",
    "\n",
    "```python\n",
    "def preprocess(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Prepara el dataframe para luego ser entrenado. En particular:\n",
    "    - Imputa valores nulos\n",
    "    - Genera features para aumentar la explicabilidad del modelo\n",
    "    \"\"\"\n",
    "\n",
    "    df_proc = df.copy()\n",
    "\n",
    "    # Imputar\n",
    "    ## Weather\n",
    "    weather_mode = df_proc[\"Weather\"].mode().iloc[0]\n",
    "    df_proc[\"Weather\"] = df_proc[\"Weather\"].fillna(weather_mode)\n",
    "\n",
    "    ## Driver_Alcohol\n",
    "    df_proc[\"Driver_Alcohol\"] = df_proc[\"Driver_Alcohol\"].fillna(0)\n",
    "\n",
    "    ## Driver_Age\n",
    "    age_mean = df_proc[\"Driver_Age\"].mean()\n",
    "    df_proc[\"Driver_Age\"] = df_proc[\"Driver_Age\"].fillna(age_mean)\n",
    "\n",
    "    # Feature Engineering\n",
    "    df_proc[\"Speed_Accident\"] = df_proc[\"Speed_Limit\"] * df_proc[\"Accident\"]\n",
    "    df_proc[\"Traffic_Norm\"] = df_proc[\"Traffic_Density\"] - df_proc[\"Traffic_Density\"].mean()\n",
    "\n",
    "    return df_proc\n",
    "\n",
    "# aplicar preprocessing\n",
    "df_proc = preprocess(df) \n",
    "\n",
    "# separar X,y\n",
    "target = \"Accident\"\n",
    "X = df.drop(columns = [target])\n",
    "y = df[target]\n",
    "\n",
    "# entrenar y evaluar performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, stratify=y)\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "> **Pregunta:** ¿Detectan algún problema en el código anterior? ¿Qué implicancias podría tener esto para el performance del modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición \n",
    "\n",
    "**Data Leakage** (o fuga de datos) en Machine Learning ocurre cuando se **filtra información del conjunto de prueba** o de variables futuras hacia el modelo durante el entrenamiento. Esto provoca que el modelo aprenda patrones que no estarán disponibles en un entorno real, generando resultados demasiado optimistas en la validación y un desempeño pobre en producción.\n",
    "\n",
    "#### ¿Por qué es importante hablar de Data Leakage?\n",
    "\n",
    "El **Data Leakage** es uno de los errores más comunes —y a la vez más críticos— en los que puede incurrir un científico de datos. Su impacto suele pasar desapercibido durante el desarrollo del modelo, pero puede tener consecuencias graves una vez que se despliega en producción.\n",
    "\n",
    "Ignorar o no detectar Data Leakage de forma oportuna puede acarrear los siguientes **riesgos**:\n",
    "\n",
    "- **Sobreajuste (overfitting):** El modelo aprende patrones que en realidad no estarán disponibles en datos nuevos, lo que lleva a un desempeño artificialmente alto durante el entrenamiento pero pobre en producción.\n",
    "\n",
    "- **Evaluaciones sesgadas:** Las métricas obtenidas durante la validación pueden ser optimistas o poco representativas, generando una falsa sensación de calidad del modelo.\n",
    "\n",
    "- **Decisiones de negocio equivocadas:** Un modelo que parece funcionar bien en pruebas puede fallar en condiciones reales, provocando decisiones costosas para el negocio.\n",
    "\n",
    "- **Pérdida de confianza en los modelos:** En industrias sensibles (salud, finanzas, etc), un modelo que falla por *data leakage* puede deteriorar la confianza de stakeholders y usuarios en la analítica predictiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y82cM-IUWu4c"
   },
   "source": [
    "### Como Definimos un Pipeline\n",
    "\n",
    "Un pipeline es una **lista de tuplas**.\n",
    "\n",
    "- La lista contiene todos los pasos que se efectuan desde la entrada hasta la salida del pipeline\n",
    "- Cata tupla de la lista representa un subproceso del pipeline. Este debe estar compuesto por un nombre u la clase que corresponda.\n",
    "\n",
    "![Pipeline](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/15-Feature-Engineering-Parte-II/pipeline.png?raw=true)\n",
    "\n",
    "Cada tupla debe seguir la siguiente estructura:\n",
    "\n",
    "```python\n",
    "step = ('alias', Transformer()) # Transformer = Procesamiento a realizar\n",
    "```\n",
    "\n",
    "Por ejemplo, para la imágen anterior, el pipeline sería:\n",
    "\n",
    "```python\n",
    "pipe = Pipeline([('scaling', Scaler()),\n",
    "                 ('dimensionality_reduction', DimReductor()),\n",
    "                 ('predictive_model', Model())])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-1JB5iiXUba"
   },
   "source": [
    "Los **`pipelines`** nos ayuda en los siguientes puntos:\n",
    "\n",
    "1. **Simplificación del proceso de aprendizaje automático**: Los Pipelines permiten <u>combinar múltiples pasos</u> en el proceso de aprendizaje automático, como el preprocesamiento de datos, la selección de características y el entrenamiento del modelo, en una única entidad. Esto simplifica el flujo de trabajo general y reduce las posibilidades de errores.\n",
    "\n",
    "2. **Procesamiento de datos consistente**: Los Pipelines aseguran que los mismos pasos de preprocesamiento de datos se apliquen de manera consistente tanto a los datos de entrenamiento como a los de prueba. Esto reduce el riesgo de sobreajuste (producto de un data-leakage) y facilita la comparación del rendimiento de diferentes modelos.\n",
    "\n",
    "3. **Ajuste de hiperparámetros fácil**: Los Pipelines permiten ajustar los hiperparámetros de varios pasos en el proceso de aprendizaje automático de manera simultánea. Esto puede ayudar a encontrar la combinación óptima de hiperparámetros y mejorar el rendimiento del modelo.\n",
    "\n",
    "4. **Legibilidad y reutilización de código**: Los Pipelines proporcionan una forma clara y concisa de organizar el código, lo que facilita su lectura y mantenimiento. También permiten reutilizar la misma tubería para diferentes conjuntos de datos y modelos, ahorrando tiempo y esfuerzo.\n",
    "\n",
    "5. **Mejora del rendimiento**: Al reducir la cantidad de manipulación de datos y cálculo requerido, los pipelines pueden llevar a tiempos de entrenamiento e inferencia de modelos más rápidos. Esto puede ser especialmente útil al trabajar con conjuntos de datos grandes o modelos complejos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFMTbhioZ7AZ"
   },
   "source": [
    "Veamos un ejemplo para transformar `LotArea` con `MinMaxScaler` y `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "X7UA4B6zYnCT",
    "outputId": "d997451d-067e-4064-db93-b4ef80b3dee6"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# instanciamos pipeline\n",
    "LotArea_pipe = Pipeline([\n",
    "    ('scaler_1', MinMaxScaler()),\n",
    "    ('scaler_2', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# aplicamos pipeline sobre LotArea\n",
    "result = LotArea_pipe.fit_transform(df[['LotArea']])\n",
    "\n",
    "# generamos dataframe con resultado\n",
    "pd.DataFrame(result, columns = LotArea_pipe.feature_names_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hvu88zS2XyGT"
   },
   "source": [
    "### ColumnTransformer\n",
    "\n",
    "Es muy frecuente, es que los datos sea heterogéneos por ejemplo, es normal encontrar datasets con variables **ordinales**, **categoricas**, y **numéricas**.\n",
    "\n",
    "Para utilizar *pipelines* en este contexto, se necesitaria definir una por cada variable, repitiendo varios componentes de código entre variables que son del mismo tipo, esto resulta en una redundancia excesiva que se puede atacar por medio de objetos tipo `ColumnTransformer`. **Estos objetos permite separar flujos de preprocesamiento, permitiendo seleccionar por columna o grupos de columna dentro de un `pipeline`.**\n",
    "\n",
    "Nuevamente, el ColumnTransformer se construye sobre una **lista de tuplas**:\n",
    "\n",
    "- Cada tupla representa la transformación a aplicar sobre un conjunto de columnas\n",
    "- Cada tupla debe contener el listado de columnas a transformar y la transformación a aplicar\n",
    "\n",
    "Cada tupla debe seguir la siguiente estructura:\n",
    "\n",
    "```python\n",
    "transformation = ('alias', Transformer(), [col1, col2]) # Transformer = Procesamiento a realizar\n",
    "```\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "572fokfCWjRP"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessing_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('OneHotEncoder', OneHotEncoder(),  ['Neighborhood', 'Utilities', 'Foundation']),\n",
    "        ('StandardScaler', StandardScaler(),['OverallQual', 'OverallCond', 'GarageCars', 'GarageArea']),\n",
    "        ('PowerTransform', LotArea_pipe, ['LotArea'])]) # notar como se usa un pipeline dentro de ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "dBEy57ySaKyZ"
   },
   "outputs": [],
   "source": [
    "# podemos encapsular el ColumnTransformer como paso de un Pipeline\n",
    "housing_pipeline = Pipeline([\n",
    "    ('Preprocessing', preprocessing_transformer)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkRYL6cneMXT"
   },
   "source": [
    "Finalmente se aplican los procedimientos planificados en la variable `prep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "d_fn_eeHeLSY",
    "outputId": "5abc2b04-d227-4bb4-fb33-2f2ba220039c"
   },
   "outputs": [],
   "source": [
    "df_preprocesado = housing_pipeline.fit_transform(df)\n",
    "df_preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "5YumlMlLeN9I",
    "outputId": "418cc2f3-34b8-40cf-fba3-6600ddf842bd"
   },
   "outputs": [],
   "source": [
    "df_preprocesado = pd.DataFrame(df_preprocesado.toarray())\n",
    "df_preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjCMvVW9eWrF"
   },
   "source": [
    "> **Pregunta ❓**: ¿Qué sucedió con el resto de las columnas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dV75j5VweaW4"
   },
   "source": [
    "Nota que en este momento el Pipeline solo cuenta con una etapa de preprocesado. Sin embargo, la idea es que a futuro tenga el resto de los pasos de nuestro proyecto.\n",
    "\n",
    "\n",
    "```python\n",
    "housing_pipeline = Pipeline([\n",
    "    ('Imputación', Imputador()),\n",
    "    ('Preprocessing', Preprocesador()),\n",
    "    ('Selector de Variables', SelectorDeVariables()),\n",
    "    ('Reduccion de Dimensionalidad', ReduccionDeDimensionalidad()),\n",
    "    ('Modelo', Modelo())\n",
    "\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyLp0Pg4eyKV"
   },
   "source": [
    "### En resumen... un Pipeline... 🧪\n",
    "\n",
    "\n",
    "Un pipeline es una lista de tuplas.\n",
    "\n",
    "- La lista contiene todos los pasos que se efectuan desde la entrada hasta la salida del pipeline\n",
    "- Cata tupla de la lista representa un subproceso del pipeline. Este debe estar compuesto por un nombre u la clase que corresponda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ZtWpHj7FeVTU"
   },
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes('float')\n",
    "cat_cols = df.select_dtypes('object')\n",
    "\n",
    "# Ejemplo de Preprocesador Compuesto\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"standard_scaler\", StandardScaler(), num_cols),\n",
    "        (\"category_one_hot\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qy1OP5M7g6Uv"
   },
   "source": [
    "## Manejo de valores faltantes\n",
    "\n",
    "Por lo general, existen razones prácticas y conceptuales a tener en cuenta cuando se trabaja con valores faltantes.\n",
    "\n",
    "### Conceptual\n",
    "\n",
    "**La falta de información introduce sesgos en los modelos de datos**, pues hace que las muestras obtenidas no sean representativas del fenómeno que se desea estudiar. Esto puede generar conclusiones sesgadas y puede llevar a tomar malas decisiones.\n",
    "\n",
    "### Práctica\n",
    "\n",
    "**Los valores faltantes son incompatibles con algunos modelos de aprendizaje automático**, debido a que estos modelos son parte de la razón fundamental de analizar un fenómeno por medio de datos,es que se necesita comprender bien los mecanismos de manejo de este tipo de valores.\n",
    "\n",
    "Veamos si nuestro dataset contiene valores nulos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1713368042464,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ErLzFKljsMsL",
    "outputId": "20d030c8-f05e-4f4a-8735-c909edb11d7a"
   },
   "outputs": [],
   "source": [
    "# Print de valores nulos\n",
    "df.isnull().sum().sort_values(ascending = False).iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOnepRZ8hZrI"
   },
   "source": [
    "## Tratamiento de Datos Faltantes: Deletion & Imputation ☠️\n",
    "\n",
    "### Eliminación/Deletion 🪄\n",
    "\n",
    "Es el método más sencillo, se conoce tambien como **list-wise deletion** y consiste en **eliminar filas o columnas de un dataset que presenten datos faltantes**. Se puede acceder a este tipo de tratamiento por medio de `.dropna()` objetos de Pandas.\n",
    "\n",
    "Se recomiendan cuando el patrón de perdida de información observada (por ejemplo por medio de `mssingno`) es claramente aleatorio, y si además las variables con información faltante son 'pocas' y con 'pocos' valores faltantes. La definición de 'poco' varia en función del problema, pero una buena huerística puede ser inferior al 15% en variables de poca importancia. **Estos métodos generan una pérdida de datos y potencialmente aumento en el sesgo de los modelos** (👀 OJO: esto depende mucho del caso de estudio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "AXJ6OE8-o1aR",
    "outputId": "77e83003-80c6-4fb3-e0d9-283b85bcb225"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"LotFrontage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1713367913185,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "GHZhFdlao4ni",
    "outputId": "a625c04b-91ea-48c8-b634-c1109a2bfc4d"
   },
   "outputs": [],
   "source": [
    "df.drop(columns='Alley')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H00jHiOzo-Fw"
   },
   "source": [
    "### Imputación 🧩\n",
    "\n",
    "Corresponde a las técnicas que permiten rellenar de información faltante por medio de estimaciones.\n",
    "\n",
    "> **Pregunta ❓:** ¿Se les ocurren ejemplos de casos de mala imputación?.\n",
    "\n",
    "La imputación al igual que la eliminación de columnas y/o filas conlleva problemas. Entre los problemas de este método podemos encontrar: añadir ruido a nuestros datos, añadir sesgo y data leakage (¿Por qué?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2LjIvpTpDCZ"
   },
   "source": [
    "#### Imputación Simple\n",
    "\n",
    "Por lo general este tipo de imputación presenta un buen rendimiento empírico en tareas de ciencia de datos y es ampliamente recomendado.\n",
    "\n",
    "> **Nota:** aplicar este tipo de métodos puede afectar el calculo de varianzas y covarianzas.\n",
    "\n",
    "En `pandas` podemos aplicar este tipo de imputación por medio del método `.fillna()` ya sea entregando un valor precalculado (media, mediana, moda, etc...) o utilizando los argumentos `ffill` (usar el valor anterior)  y `bfill` (usar el valor siguiente).\n",
    "En `scikit-learn` por otra parte, podemos utilizar [`SimpleImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer), el cual es una transformación, lo que hace este método compatible con los `Pipelines`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EFjmvaOpGHX"
   },
   "source": [
    "Existen directrices a tener en cuenta al momento de tratar valores faltantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ViEOFk7pIe0"
   },
   "source": [
    "**Variables categóricas**:\n",
    "\n",
    "    * Transformar valores faltantes en una nueva categoría.\n",
    "    * Utilizar códificación Dummy en variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1225,
     "status": "ok",
     "timestamp": 1713367914377,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "vmcuEeLSo68_",
    "outputId": "072f76af-306b-4efb-cac1-3f517f5bab40"
   },
   "outputs": [],
   "source": [
    "df[\"GarageType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1713367914377,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "CVty4-evpOw0",
    "outputId": "1e4401d0-241a-4a51-df2d-7ce6928941da"
   },
   "outputs": [],
   "source": [
    "df[\"GarageType\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1713367914377,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "YXztkBqspRUz",
    "outputId": "671da6c9-2b02-4d2a-afe9-4cccd3d210c0"
   },
   "outputs": [],
   "source": [
    "df[df[\"GarageType\"].isna()][\"GarageType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "auJM3_BGpSY9",
    "outputId": "316df807-3494-4b44-902a-5b4bcc9b5e3f"
   },
   "outputs": [],
   "source": [
    "garage_type = df[\"GarageType\"].fillna(\"NoGarage\")\n",
    "garage_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7i22h_YpUVy"
   },
   "source": [
    "**Valores Ordinales**\n",
    "\n",
    "    * Agregar la categoría de valor faltante como orden inicial o final en categorias ordinales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "afGto8gZpmSJ",
    "outputId": "b00f06db-02a8-4cfe-e570-a7f669737469"
   },
   "outputs": [],
   "source": [
    "# ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "df[\"GarageCond\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "H5E9zOjcpmuj",
    "outputId": "c6f3fce4-c67b-48cc-fc98-04e1c8f8f070"
   },
   "outputs": [],
   "source": [
    "df[\"GarageCond\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "sLxPJUSTpotq",
    "outputId": "98159f4b-7541-4223-a5bd-197534779893"
   },
   "outputs": [],
   "source": [
    "garage_cond = df[\"GarageCond\"].fillna(\"NoGarage\")\n",
    "# Agregar después al Encoder NoGarage ['NoGarage', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "garage_cond.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMjAtsOYprla"
   },
   "source": [
    "**Variables numéricas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "0Cmz0GcBptvP",
    "outputId": "608cdfa2-a449-4197-a4ba-83e3b71998fe"
   },
   "outputs": [],
   "source": [
    "df[df[\"LotFrontage\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "YcOfe4LWpuJA",
    "outputId": "d941863c-84be-453f-f08e-e897ec2d0305"
   },
   "outputs": [],
   "source": [
    "df[\"LotFrontage\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdLtKKIgpwrg"
   },
   "source": [
    "En este caso, usaremos `SimpleImputer`.\n",
    "\n",
    "La estrategia se define por el parámetro:\n",
    "\n",
    "`strategy`, default=’mean’\n",
    "\n",
    "    The imputation strategy.\n",
    "\n",
    "        If “mean”, then replace missing values using the mean along each column. Can only be used with numeric data.\n",
    "\n",
    "        If “median”, then replace missing values using the median along each column. Can only be used with numeric data.\n",
    "\n",
    "        If “most_frequent”, then replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n",
    "\n",
    "        If “constant”, then replace missing values with fill_value. Can be used with strings or numeric data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "EIyBKX0Ep1jL",
    "outputId": "90324872-678b-40a8-9e2b-7a60bcd7be75"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "imputed_lotfrontage = si.fit_transform(df.loc[:, [\"LotFrontage\"]])\n",
    "\n",
    "imputed_lotfrontage = pd.DataFrame(imputed_lotfrontage, columns=[\"ImputedLotFrontage\"])\n",
    "\n",
    "imputed_lotfrontage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "kgVgCOksp2zq",
    "outputId": "8019d8a5-54bc-4ccd-ff69-0c8d5a504aa4"
   },
   "outputs": [],
   "source": [
    "imputed_lotfrontage.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "R1j4UrC9p3ut",
    "outputId": "577b36ee-e923-49cd-878e-2afe0451ddb1"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "lf = pd.concat((df.loc[:, [\"LotFrontage\"]], imputed_lotfrontage))\n",
    "px.histogram(lf, barmode=\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccT5Di7pp67j"
   },
   "source": [
    "#### Imputación Multivariada\n",
    "\n",
    "Un problema de la imputación singular es que modela los datos como uno completo, sin considerar la incertidumbre inherente a todos los otros datos. Una solución para esto son los métodos de imputación múltiple.\n",
    "\n",
    "Un método de inmputación multiple estima los valores faltantes a partir de otros. Puede ser tanto univariada (solo considerando la variable objetivo) como multivariada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc_E0Jy1qQwI"
   },
   "source": [
    "##### KNNImputer\n",
    "\n",
    "Imputa usando k-Nearest Neighbors.\n",
    "\n",
    "Los valores imputados se calculan según el parámetro `weights`:\n",
    "\n",
    "- Si `weights=uniform`, se promedian los valores de los vecinos cercanos.\n",
    "- Si `weights=distance`, se ponderan los valores de los vecinos cercanos según la distancia al punto.\n",
    "\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/15-Feature-Engineering-Parte-II/knn.png?raw=true' width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1713367914378,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "_wkWR-w8QauR",
    "outputId": "87cf82e4-cf3f-4305-f9f3-3be453a722d0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "only_numeric = df.select_dtypes(include=np.number).drop(\n",
    "    columns=[\n",
    "        \"YearBuilt\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"YrSold\",\n",
    "        \"GarageYrBlt\",\n",
    "        \"SalePrice\",\n",
    "        \"MoSold\",\n",
    "        \"MSSubClass\",\n",
    "    ]\n",
    ")\n",
    "only_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "ktqtv9gkqTtP"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# en este caso, simplemente se promedian los valores cercanos.\n",
    "KNNimputer = KNNImputer(n_neighbors=2, weights=\"uniform\") # notar como puedo definir el n° de vecinos\n",
    "KNN_imputed_data = KNNimputer.fit_transform(only_numeric)\n",
    "KNN_imputed_data = pd.DataFrame(KNN_imputed_data, columns=only_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "dIN1bjtqqUia",
    "outputId": "640db072-a7da-4cfe-90ae-060fc76b409f"
   },
   "outputs": [],
   "source": [
    "lf_imputed = KNN_imputed_data.loc[:, [\"LotFrontage\"]]\n",
    "lf_imputed.columns = [\"LotFrontageImputed\"]\n",
    "\n",
    "lf = pd.concat((df.loc[:, [\"LotFrontage\"]], lf_imputed))\n",
    "px.histogram(lf, barmode=\"group\", nbins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFYh2dbKqjiZ"
   },
   "source": [
    "### Incluyendo la imputación en nuestra Pipeline\n",
    "\n",
    "Pongamos en práctica todo lo que hemos aprendido en esta clase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APjDBzlfCkD-"
   },
   "source": [
    "> **Ejercicio 📝**\n",
    "\n",
    "0. Primero separemos la columna `SalePrice` del dataframe. Ejecute el siguiente código:\n",
    "\n",
    "```python\n",
    "target = 'SalePrice'\n",
    "y = df[target].copy()\n",
    "X = df.drop(columns = target).copy()\n",
    "```\n",
    "\n",
    "1. Usando el dataframe `X`, cree una lista conteniendo variables numéricas y otra lista conteniendo variables categóricas.\n",
    "2. Para cada tipo de variable, genere un `pipeline`. En específico:\n",
    "  - Numéricas:\n",
    "    - Imputación multivariada con KNN\n",
    "    - Escalamiento Robusto\n",
    "  - Categóricas:\n",
    "    - Imputación univariada (la que ustedes prefieran)\n",
    "    - Transformación a One Hot\n",
    "      - *Qué opción había que definir para evitar un \"sparse output\"?*\n",
    "3. Con los pipelines creados, use un `ColumnTransformer` para englobarlos en un mismo paso.\n",
    "4. Genere un `pipeline` que contenga como primer paso la transformación definida en el `ColumnTransformer`. Pruébelo sobre los datos e imprima la salida.\n",
    "5. (Opcional) Vuelva a definir el `pipeline` anterior, pero definiendo como segundo paso el modelo de regresión que ustedes prefieran. Genere una predicción de  `SalePrice` y cuantifique el performance de su modelo usando [Mean Absolute Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error).\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "sPET2w1_IxMH"
   },
   "outputs": [],
   "source": [
    "# 0.\n",
    "target = 'SalePrice'\n",
    "y = df[target].copy()\n",
    "X = df.drop(columns = target).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "gxfPtNCfCq4T"
   },
   "outputs": [],
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "rhW3Siw5CzXs"
   },
   "outputs": [],
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "q_traBRKC27F"
   },
   "outputs": [],
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "_QacZ-0WC4LE"
   },
   "outputs": [],
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1713367914379,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "Or2dEiIvC43A"
   },
   "outputs": [],
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iV_UXauNqtRX"
   },
   "source": [
    "## Ya pero quiero aprender mas... ¿algo para leer? 🤔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3x2oPcCqwRh"
   },
   "source": [
    "- Para comenzar pueden visualizar con mayor profundidad la documentación de Scikit-Learn enfocada en la extracción de features: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "- Complementar la clase leyendo el capitulo 5 del libro [Designing Machine Learning Systems](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969)\n",
    "\n",
    "- Leer capitulo 1 del libro [Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and MLOps](https://www.amazon.com/-/es/Valliappa-Lakshmanan/dp/1098115783/ref=pd_bxgy_img_sccl_1/142-9514380-0202369?pd_rd_w=JQSaa&content-id=amzn1.sym.26a5c67f-1a30-486b-bb90-b523ad38d5a0&pf_rd_p=26a5c67f-1a30-486b-bb90-b523ad38d5a0&pf_rd_r=BJE9WMJ9X1QQMYF4C3EJ&pd_rd_wg=y78Jr&pd_rd_r=6a1994c2-2734-428a-a270-ded23f892987&pd_rd_i=1098115783&psc=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
