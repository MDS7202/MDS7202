{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 19: Reinforcement Learning\n",
    "\n",
    "**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "- Introducir al estudiante a la disciplina de Reinforcement Learning\n",
    "- Conocer sobre MDP y sus principales componentes\n",
    "- Aprender a resolver problemas de RL mediante Q-learning\n",
    "- Aprender a implementar algoritmos basados en DRL usando Stable-Baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistemas Autónomos\n",
    "\n",
    "Los **Sistemas Autónomos** son modelos o sistemas diseñados para operar sin la necesidad de supervisión humana. Al igual que otros sistemas, reciben una entrada $X$ y generan una salida $f(X)$. Sin embargo, su característica distintiva es la capacidad de **operar de manera independiente**. Esta autonomía se basa en la capacidad del sistema para ejecutar acciones en momentos específicos, conocidos como **triggers**. Por ejemplo, un sistema autónomo podría activarse automáticamente una vez al día a las 12:00 PM, o en respuesta a acciones específicas del usuario, como enviar un mensaje a un chatbot.\n",
    "\n",
    "El siguiente diagrama ilustra la idea del párrafo anterior:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../../recursos/2024-01/rl/autonomous.png\" style=\"width: 35%;\">\n",
    "</div>\n",
    "\n",
    "Veamos un ejemplo de un sistema autónomo simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, -1, -1, -1, -1, -1, 1, -1, 1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# definición de modelo\n",
    "f = lambda x: 1 if x >= 0 else -1 # modelo\n",
    "\n",
    "# modelo en \"producción\"\n",
    "outputs = []\n",
    "for _ in range(1000): # período en el que se ejecuta el sistema\n",
    "  x = np.random.randn() # entrada del sistema\n",
    "  f_x = f(x) # salida del sistema\n",
    "  outputs.append(f_x) # guardamos salida\n",
    "\n",
    "outputs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo de esta y la próxima clase trabajaremos con dos grandes paradigmas para generar sistemas autónomos: **Reinforcement Learning** y **LLM**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://datasciencedojo.com/wp-content/uploads/ml-ds-algos.jpg\" style=\"width: 35%;\">\n",
    "</div>\n",
    "\n",
    "### ¿Qué es?\n",
    "\n",
    "**Reinforcement Learning** (también conocido como aprendizaje reforzado) es un marco para resolver **tareas dinámicas** de control de forma **autónoma** y a través de **agentes**. Estos agentes son entrenados a partir de **interacciones prueba-error con un ambiente**, *reforzando* las interacciones \"buenas\" y *castigando* las interacciones \"malas\".\n",
    "\n",
    "### ¿Cómo se distingue de lo que ya hemos aprendido?\n",
    "\n",
    "El aprendizaje reforzado posee varios elementos distintivos que lo diferencian del Apredizaje Supervisado Tradicional, en particular:\n",
    "\n",
    "- **Resuelve problemas de control dinámico**: Esto quiere decir que no buscaremos la mejor acción, sino la **mejor secuencia de acciones** que nos llevan a alcanzar nuestro objetivo de forma autónoma. Este aspecto del problema no es trivial, pues tiene por supuesto que nuestro problema contiene **dependencia entre los estados temporales** (por ejemplo, para jugar ajedrez no es lo mismo mover el caballo y la reina, que la reina y el caballo). \n",
    "\n",
    "> **Pregunta:** ¿Qué pasa si mi problema no posee esta dependencia temporal?\n",
    "\n",
    "- **No necesita de datos etiquetados, pero necesita de un ambiente**: A diferencia del aprendizaje supervisado donde queremos aproximar lo mejor posible a una variable objetivo, en apredizaje reforzado se busca generar una propia distribución que resuelva el problema a través de interacciones prueba y error. ¿Qué significa esto? Pensemos por un momento que queremos entrenar un modelo que juegue Mario Bros:\n",
    "    - Si quisiéramos modelar esto con aprendizaje supervisado, necesitaríamos de un dataset con las imágenes del juego y la *mejor jugada* dada estas imágenes (típicamente obtenida de un experto). Por lo tanto, **nuestro modelo podría ser tan bueno jugando Mario Bros como lo sea el experto.**\n",
    "    - Si quisiéramos mdoelar esto con aprendizaje reforzado, necesitaríamos de un ambiente con las *reglas* de Mario Bros (¿qué pasa si me muevo hacia la derecha o a la izquierda?). Luego, nuestro modelo aprendería a interacciones prueba-error cual es el mejor set de acciones para resolver nuestro problema. Por lo tanto, **nuestro modelo podría ser el <u>mejor</u> jugador de Mario Bros**.\n",
    "\n",
    "> **Pregunta:** ¿Qué implicancias tiene esto para la humanidad?\n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "Para poder entrenar nuestro agente debemos contar con el siguiente esquema de entrenamiento:\n",
    "1. El **agente observa** la información disponible entregada por el ambiente\n",
    "2. Considerando la observación obtenida, el **agente ejecuta una acción**\n",
    "3. La acción \"impacta\" al ambiente, devolviendo una **recompensa** (feedback de qué tan buena fue la acción ejecutada) y una **nueva observación**\n",
    "4. Volver al paso 1\n",
    "\n",
    "De esta forma, la idea es repetir muchas (**muchas!**) veces estos pasos para así maximizar las recompensas obtenidas. Como salida, obtendremos una política $\\pi$ con la distribución de probabilidades de qué accion ejecutar en un momento determinado.\n",
    "\n",
    "### Ejemplos\n",
    "\n",
    "Entre algunos ejemplos notables de aplicaciones de RL podemos encontrar:\n",
    "- [Robótica](https://www.youtube.com/watch?v=k7K7JG_RQhQ)\n",
    "- [Vehículos Autónomos ](https://www.youtube.com/watch?v=fKBes088KV4)\n",
    "- [AlphaGo](https://deepmind.google/technologies/alphago/)\n",
    "\n",
    "> **Pregunta:** ¿Qué otros ejemplos de se les ocurre que puedan ser modelados con RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema de hoy\n",
    "\n",
    "Para ejemplificar los conceptos y consolidar lo aprendido, el dia de hoy trabajaremos en resolver el videojuego **Frozen Lake**. Este videojuego ilustra la tarea de un elfo navideño, el cual debe atravesar por un lago congelado evitando caer al agua y así recuperar el regalo de navidad. Pueden encontrar la definición formal del problema en este [link](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "\n",
    "Conociendo el contexto, **¿como podemos usar RL para resolver este problema?**\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://www.gymlibrary.dev/_images/frozen_lake.gif\" style=\"width: 20%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "El primer paso para usar RL es formular nuestro problema como un **Proceso de Decisión de Markov** (MDP). Un MDP es un proceso de control estocástico en tiempo discreto, el cual proporciona un marco matemático para **modelar la toma de decisiones** en problemas donde los resultados son controlados por un tomador de decisiones. Los procesos de decisión de Markov son una extensión de las cadenas de Markov y son utilizados en muchas disciplinas, incluyendo la robótica, el control automático, la economía y la manufactura.\n",
    "\n",
    "Un Proceso de Decisión de Markov se constituye de los siguientes elementos:\n",
    "\n",
    "- **Estados**: También se le llama *observaciones*, es la información que obtiene el agente del ambiente.\n",
    "- **Acciones**: Set de acciones posibles a realizar por el agente.\n",
    "- **Recompensas**: Feedback otorgado al agente por ejecutar una acción $a$ en un estado $s$.\n",
    "\n",
    "donde la colección $(S_0, A_0, R_1, S_1)$ se le conoce como una **experiencia**.\n",
    "\n",
    "> **Pregunta:** ¿Quién define los Estados, Acciones y Recompensas? ¿Existe una única definición de estos elementos?\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg\" style=\"width: 25%;\">\n",
    "</div>\n",
    "\n",
    "De esta manera, en el tiempo $t$ el agente recibe el estado $S_t$, a lo que responde ejecutando una acción $A_t$. Esta acción altera el ambiente, otorgando una recompensa $R_t$ y retornando un nuevo estado $S_{t+1}$. Dado que $S_{t+1}$ depende únicamente del conjunto $(S_t, A_t)$ se concluye que la transición de entre estados cumple la <u>**Propiedad de Markov**</u>: Agentes solo deben tener en cuenta el **estado actual** $S_t$ para decidir qué acción tomar y **<u>NO</u> la historia de todos los estados y acciones anteriores**.\n",
    "\n",
    "El siguiente diagrama ilustra la representación típica de un MDP:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywOrdJAHgSL5RP-AuxsfJQ.png\" style=\"width: 50%;\">\n",
    "</div>\n",
    "\n",
    "Detallemos un poco mejor cada uno de los componentes antes mencionados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estados\n",
    "\n",
    "Las **Observaciones o Estados** son la información que nuestro agente obtiene del entorno. En el caso de un videojuego, esto podría ser una captura de pantalla. Para un agente de trading, podría ser el valor de una acción específica, entre otros ejemplos.\n",
    "\n",
    "Sin embargo, hay una diferencia que hacer entre observación y estado:\n",
    "\n",
    "- **Estado**: Entrega una **descripción completa** del estado del ambiente. Es un entorno completamente observado. \n",
    "- **Observación**: Es una **descripción parcial** del estado del ambiente.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/obs_space_recap.jpg\" style=\"width: 35%;\">\n",
    "</div>\n",
    "\n",
    "Un aspecto importante a considerar es que **no existe una única representación de las observaciones.** Por ejemplo, el MDP puede recoger observaciones de solo algunas piezas del ajedrez y aún así llegar a un buen resultado (aunque esto probablemente sea una mala idea). Es responsabilidad del programador preocuparse para que las observaciones recibidas por el agente sean de utilidad para obtener buenos resultados, teniendo especial cuidado en respetar la Propiedad de Markov.\n",
    "\n",
    "Volviendo al problema de **Frozen Lake**, los estados son definidos a partir de la posición en la cuadrídula del agente (elfo). Como el mapa tiene un tamaño de 4x4, existen 16 estados posibles, los cuales pueden ser enumerados de la siguiente manera:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../../recursos/2024-01/rl/states.png\" style=\"width: 20%;\">\n",
    "</div>\n",
    "\n",
    "Donde se observa que el agente comenzará siempre en el estado 0 y su meta es llegar al estado 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones\n",
    "\n",
    "Las acciones hacen referencia a todo el set posible de acciones a ejecutar por el agente. \n",
    "\n",
    "El espacio de acciones puede ser de dos tipos:\n",
    "\n",
    "- **Discreto**: El número de acciones posibles es **finito**\n",
    "- **Continuo**: El número de acciones posibles es **infinito**\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/action_space.jpg\" style=\"width: 35%;\">\n",
    "</div>\n",
    "\n",
    "**Ojo! Tener presente el tipo de acciones (discretas o continuas) de nuestro ambiente es un aspecto <u>crucial</u> para elegir el algoritmo de RL.**\n",
    "\n",
    "Retornando a nuestro problema, **Frozen Lake** puede ser caracterizado como un ambiente de **acciones discretas** con 4 acciones posibles:\n",
    "\n",
    "- **0**: Izquierda ⬅️\n",
    "- **1**: Abajo ⬇️\n",
    "- **2**: Derecha ➡️\n",
    "- **3**: Arriba ⬆️\n",
    "\n",
    "donde cada elemento representa la dirección de movimiento del elfo navideño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompensas\n",
    "\n",
    "Las recompensas son fundamentas en RL pues son el **único feedback del agente**. A través del agente, el **agente aprende si la acción ejecutada fue buena o no**. \n",
    "\n",
    "En el ejemplo del ajedrez, el agente puede obtener una recompensa positiva si captura una pieza del oponente, y una recompensa negativa si el oponente captura una pieza del agente.\n",
    "\n",
    "La recompensa acumulada en cada timestep $t$ puede ser escrita como:\n",
    "\n",
    "$$R(\\tau) = r_{t+1} + r_{t+2} + r_{t+3} + r_{t+4} + ...$$\n",
    "$$R(\\tau) = \\sum_{k=0}^{\\infty} r_{t+k+1}$$\n",
    "\n",
    "Dado que las recompensas futuras son más lejanas de $t$ y por lo tanto menos deseables, un procedimiento común es descontar las recompensas futuras por un factor $\\gamma$:\n",
    "\n",
    "$$R(\\tau) = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\gamma^3 r_{t+4} + ...$$\n",
    "$$R(\\tau) = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$$\n",
    "\n",
    "donde $\\gamma$ es un factor definido por el programador y usualmente varia entre los valores 0.95 y 0.99.\n",
    "\n",
    "Al igual que los estados, el **esquema de recompensas no es único y puede ser configurable por el usuario**. Es responsabilidad del usuario definir un buen esquema de recompensas acorde al problema a resolver para lograr una solución efectiva.\n",
    "\n",
    "Para el problema de **Frozen Lake**, las recompensas son definidas en función de la posición del elfo:\n",
    "\n",
    "- **Regalo**: +1\n",
    "- **Agua**: +0\n",
    "- **Hielo**: +0\n",
    "\n",
    "> **Pregunta**: ¿Qué problemas podría tener este esquema de recompensas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalización del problema\n",
    "\n",
    "El objetivo de Aprendizaje Reforzado es **encontrar una política $\\pi$ que maximice la recompensa del agente** a lo largo del experimento. La política $\\pi$ puede ser interpretada como una función que, dado un estado $s$, retorna la **probabilidad** de tomar cada acción disponible a ejecutar por el agente.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/policy.jpg\" style=\"width: 35%;\">\n",
    "</div>\n",
    "\n",
    "En otras palabras, el problema de optimización a resolver puede ser formalizado por:\n",
    "\n",
    "\n",
    "$$\\max_\\pi J(\\pi) = \\mathbb{E}_{\\tau \\sim p_\\pi(\\tau)}\\left[\\sum_{t=1}^T \\gamma^{t-1} r\\left(s_t, a_t \\sim \\pi (a|s)\\right)\\right]$$\n",
    "\n",
    "\n",
    "Existen 2 grandes familias de métodos para encontrar $\\pi$: **Value-Based** y **Policy-Based**. Revisemos en detalle cada uno de estos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value-Based\n",
    "\n",
    "En los métodos **Value-Based**, el objetivo es **aprender y representar la función de valor de un estado y acción**, denotada comúnmente como $Q(s,a)$. La función de valor refleja la **utilidad esperada asociada de tomar una acción $a$ en un estado $s$** en términos de la recompensa acumulada. La [Ecuación de Bellman](https://huggingface.co/learn/deep-rl-course/unit2/bellman-equation) es una expresión clave en estos métodos:\n",
    "\n",
    "$$Q(s, a) = R(s, a) + \\gamma \\max_a Q(s', a)$$\n",
    "\n",
    "Donde $s$ y $a$ representan el estado actual y la acción tomada, respectivamente, $s'$ es el siguiente estado, $\\gamma$ es el factor de descuento que pondera las recompensas futuras, y $\\max_a Q(s', a)$ denota la estimación del valor máximo del próximo estado. Si bien existen algunas excepciones, los algoritmos Value-Based son **generalmente de acciones discretas**. Algunos algoritmos notables de este enfoque son Q-learning y Deep Q-Network.\n",
    "\n",
    "Finalmente con la representación del valor de cada par $(s, a)$, la política $\\pi$ puede ser formulada como:\n",
    "\n",
    "$$\\pi(s) = \\arg\\max_{a} Q(s, a)$$\n",
    "\n",
    "### Policy-Based\n",
    "\n",
    "En contraste, los métodos **Policy-Based** buscan aprender directamente la política de decisión del agente, es decir, la distribución de probabilidad sobre las acciones condicionadas al estado. La política es denotada como $\\pi(a|s, \\theta)$, que representa la probabilidad de elegir la acción $a$ dado el estado $s$ y parámetros $\\theta$. La actualización de la política se realiza mediante gradientes bajo la siguiente regla:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\sum_s \\mu(s) \\sum_a \\nabla_\\theta \\pi(a|s, \\theta)Q^\\pi(s, a)$$\n",
    "\n",
    "Donde $J(\\theta)$ representa el rendimiento esperado de la política, $\\mu(s)$ es la distribución estacionaria de estados bajo la política actual, $\\pi(a|s, \\theta)$ es la política parametrizada por $\\theta$ y $Q^{\\pi}(s,a)$ es la función de valor bajo la política $\\pi$. Si bien existen algunas excepciones, los algoritmos Policy-Based son **generalmente de acciones continuas**. Ejemplos notables de este enfoque son los algoritmos REINFORCE y TRPO.\n",
    "\n",
    "### Paréntesis: Actor-Critic\n",
    "\n",
    "Al igual que en algunos tópicos pasados, existe también un tercer enfoque que combina los conceptos de los métodos Value-Based y Policy-Based. Esta familia de métodos recibe el nombre de **Actor-Critic** y son la clase de algoritmos de RL más avanzados a la fecha. Dada la alta complejidad de estos algoritmos, su estudio se escapa de los límites de este curso.\n",
    "\n",
    "### Resumen de Métodos\n",
    "\n",
    "En resumen, existen 3 familias de métodos:\n",
    "\n",
    "- **Policy-Based**: Entrenar la política directamente para aprender qué accion tomar dado un estado $s$\n",
    "- **Value-Based**: Entrenar una función de valor para aprender qué estado tiene mayor valor. Usar esta función para tomar la acción que nos lleve al mejor estado. \n",
    "- **Actor-Critic**: Combina los dos enfoques anteriores. Es la familia de algoritmos más avanzados a la fecha.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches.jpg\" style=\"width: 35%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Ya que conocemos lo básico de RL, introduzcamos ahora un algoritmo. **Q-learning** es un algoritmo de reinforcement learning del tipo **Value-Based** (es decir, tiene por objetivos aprender el valor de una acción $a$ en un determinado estado $s$). La principal diferencia y virtud de este algoritmo es su simplicidad, pues intenta modelar la representación $Q(s, a)$ de forma **tabular**. De esta forma, este algoritmo permite a un agente tomar decisiones óptimas mediante la actualización iterativa de los valores tabuleras basados en las recompensas obtenidas al interactuar con el entorno.\n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "El algoritmo puede explicar en los siguientes pasos:\n",
    "\n",
    "1. **Inicializar la tabla Q**: Crear e inicializar una matriz de dimensión para cada par $(s,a)$, por ejemplo:\n",
    "\n",
    "<style>\n",
    "  table {\n",
    "    margin: auto;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "|          | Izquierda | Abajo | Derecha | Arriba |\n",
    "|----------|-----------|-------|---------|--------|\n",
    "| Estado 1   | 0         | 0     | 0       | 0      |\n",
    "| Estado 2   | 0         | 0     | 0       | 0      |\n",
    "| Estado 3   | 0         | 0     | 0       | 0      |\n",
    "| Estado 4   | 0         | 0     | 0       | 0      |\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Pregunta**: ¿Qué desventajas podría tener este enfoque?\n",
    "\n",
    "2. **Recolectar experiencias**: A partir de la interacción con el ambiente, recolectar experiencias:\n",
    "\n",
    "$$(S_t, A_t, R_{t+1}, S_{t+1})$$\n",
    "\n",
    "3. **Actualizar la tabla Q**: Utilizando la **Ecuación de Bellman**, actualizar los valores de $Q(s,a)$ usando la siguiente expresión:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg\" style=\"width: 40%;\">\n",
    "</div>\n",
    "\n",
    "### Epsilon-greedy\n",
    "\n",
    "En las primeras secciones de esta clase hablamos del **trade-off entre exploración y explotación**, es decir, el balance que debe realizar nuestro agente para (i) obtener nuevo conocimiento y (ii) hacer uso de su conocimiento actual para obtener el máximo de recompensa. Para resolver este dilema, Q-learning utiliza la estrategia **epsilon-greedy**:\n",
    "\n",
    "\\begin{equation}\n",
    "a_t = \n",
    "\\begin{cases} \n",
    "\\text{random action}, & \\text{with probability } \\epsilon_t \\\\\n",
    "\\underset{a}{\\mathrm{argmax}} \\, Q(S_t, a), & \\text{with probability } 1 - \\epsilon_t\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "De esta manera, con probabilidad $\\epsilon_t$ el agente toma una acción aleatoria (es decir, *explora* nuevas acciones y estados), mientras que con probabilidad $1 - \\epsilon_t$ el agente elige el par $(s,a)$ con la mayor función de valor (el agente \"rentabiliza\" su conocimiento).\n",
    "\n",
    "Noten que como la política epsilon-greedy es una política de acciones diferente a la política óptima $\\pi$, se dice que el algoritmo Q-learning es del tipo **off-policy** (es decir, utiliza una política distinta para tomar acciones y actualizar su aprendizaje). \n",
    "\n",
    "Por último, algo común es imponer que la fase de exploración sea intensiva en la primera parte del entrenamiento, para luego dedicarse a explotar el conocimiento adquirido. En otras palabras, es deseable que la probabilidad de exploración $\\epsilon_t$ disminuya mientras más se entrene el agente. Esto se puede lograr agregando un factor **decay** a epsilon, el cual puede ser lineal, exponencial, etc. Veremos una implementación de este último un poco mas adelante.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg\" style=\"width: 30%;\">\n",
    "</div>\n",
    "\n",
    "### Pseudocódigo\n",
    "\n",
    "En función de todo lo aprendido hasta ahora, el algoritmo Q-learning puede escribirse usando el siguiente pseudocódigo:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" style=\"width: 50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación: Resolviendo FrozenLake con Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar librerias necesarias\n",
    "\n",
    "Como siempre, antes de trabajar debemos instalar las librerias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swig\n",
      "  Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
      "Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: swig\n",
      "Successfully installed swig-4.4.0\n",
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.2.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from gymnasium) (4.14.1)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.2.2-py3-none-any.whl (952 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.1/952.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.2.2\n",
      "Requirement already satisfied: gymnasium[box2d] in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from gymnasium[box2d]) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pygame>=2.1.3 (from gymnasium[box2d])\n",
      "  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: swig==4.* in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium[box2d]) (4.4.0)\n",
      "Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=495893 sha256=1034939f5cafaa260c4e711202563f6456017ceee0eac60855b34a0729b96b2e\n",
      "  Stored in directory: /nfs/br1_student/giturra/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
      "Successfully built box2d-py\n",
      "Installing collected packages: box2d-py, pygame\n",
      "Successfully installed box2d-py-2.3.5 pygame-2.6.1\n",
      "Collecting stable_baselines3\n",
      "  Downloading stable_baselines3-2.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from stable_baselines3) (1.2.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from stable_baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from stable_baselines3) (2.6.0)\n",
      "Requirement already satisfied: cloudpickle in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from stable_baselines3) (3.1.1)\n",
      "Requirement already satisfied: pandas in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from stable_baselines3) (2.3.1)\n",
      "Requirement already satisfied: matplotlib in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from stable_baselines3) (3.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from gymnasium<1.3.0,>=0.29.1->stable_baselines3) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium<1.3.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (3.18.0)\n",
      "Requirement already satisfied: networkx in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from torch<3.0,>=2.3->stable_baselines3) (3.2.0)\n",
      "Collecting sympy==1.13.1 (from torch<3.0,>=2.3->stable_baselines3)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from matplotlib->stable_baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from pandas->stable_baselines3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from pandas->stable_baselines3) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
      "Downloading stable_baselines3-2.7.0-py3-none-any.whl (187 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Installing collected packages: sympy, stable_baselines3\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "onnxruntime 1.22.1 requires protobuf, which is not installed.\n",
      "pyserini 1.2.0 requires tiktoken>=0.4.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed stable_baselines3-2.7.0 sympy-1.13.1\n",
      "Requirement already satisfied: gymnasium[toy-text] in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium[toy-text]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium[toy-text]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /mnt/beegfs/home/giturra/.local/lib/python3.10/site-packages (from gymnasium[toy-text]) (4.14.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium[toy-text]) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /nfs/br1_student/giturra/miniconda3/envs/models/lib/python3.10/site-packages (from gymnasium[toy-text]) (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "# descomentar estas lineas para instalar librerias\n",
    "!pip install swig\n",
    "!pip install gymnasium\n",
    "!pip install gymnasium[box2d]\n",
    "!pip install stable_baselines3\n",
    "!pip install \"gymnasium[toy-text]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializar el ambiente FrozenLake\n",
    "\n",
    "Para trabajar con el ambiente **FrozenLake** haremos uso de la libreria [gymnasium](https://gymnasium.farama.org). Esta libreria brilla por tener múltiples implementaciones de problemáticas de RL en versión MDP, los que podemos utilizar con unas pocas lineas de código.\n",
    "\n",
    "Veamos como podemos hacer uso de esta librería para nuestro problema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada ambiente de `gym` tiene 3 métodos por default:\n",
    "- **reset**: Reinicia el ambiente a su estado inicial y devuelve el estado inicial junto a información adicional. Para el caso particular de FrozenLake, devuelve el elfo a la posición 0.\n",
    "- **step**: Recibe una acción del agente, devuelve:\n",
    "    - **new_state**: El nuevo estado del ambiente ($S_{t+1}$)\n",
    "    - **reward**: La recompensa generada por el ambiente ($R_{t+1}$)\n",
    "    - **terminated**: Booleano si el ambiente llegó a su estado terminal\n",
    "    - **truncated**: Similar a terminated, Booleano si el ambiente llegó a un estado truncado\n",
    "    - **info**: Información adicional para debuggear\n",
    "- `close`: Cierra el ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0, False, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada ambiente tiene además dos atributos muy importantes: `action_space` y `observation_space`. Estos atributos definen el espacio posible de acciones y observaciones, respectivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el ambiente permite 4 acciones discretas y 16 estados discretos. \n",
    "\n",
    "Finalmente, algo útil para nuestro algoritmo es guardar la dimensionalidad de espacios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_actions: 4\n",
      "dim_states: 16\n"
     ]
    }
   ],
   "source": [
    "dim_actions = env.action_space.n # dimensión de las acciones\n",
    "dim_states = env.observation_space.n # dimensión de los estados (observaciones)\n",
    "\n",
    "print('dim_actions:', dim_actions)\n",
    "print('dim_states:', dim_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación Q-learning\n",
    "\n",
    "Con lo apredido en la sección anterior, podemos implementar Q-learning de forma nativa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class QAgent():\n",
    "  def __init__(self, dim_states, dim_actions, gamma = 0.95, lr = 2e-1, epsilon = 0.99, decay = 1e-4):\n",
    "    '''\n",
    "    initialize QAgent parameters\n",
    "    '''\n",
    "    self.dim_states = dim_states\n",
    "    self.dim_actions = dim_actions\n",
    "    self.lr = lr\n",
    "    self.epsilon = epsilon\n",
    "    self.gamma = gamma\n",
    "    self.decay = decay\n",
    "\n",
    "    # initialize Q-table\n",
    "    self.Qtable = np.zeros((dim_states, dim_actions))\n",
    "\n",
    "  def select_action(self, state, greedy=False):\n",
    "    '''\n",
    "    select action using epsilon-greedy\n",
    "    '''\n",
    "    # randomly generate a number between 0 and 1\n",
    "    random_int = random.uniform(0, 1)\n",
    "    if random_int > self.epsilon or greedy:  # exploitation\n",
    "        action = np.argmax(self.Qtable[state])  # take the action with the highest value given a state\n",
    "    else:  # exploration\n",
    "        action = np.random.randint(0, self.dim_actions)  # take random action\n",
    "        self.epsilon *= (1 - self.decay)\n",
    "\n",
    "    return action\n",
    "\n",
    "  def update(self, state, action, reward, new_state, done):\n",
    "    '''\n",
    "    update Q-table\n",
    "    '''\n",
    "    self.Qtable[state][action] += self.lr * (reward + self.gamma * np.max(self.Qtable[new_state, :]) * (1 - done) - self.Qtable[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos ahora el funcionamiento de nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.QAgent at 0x7fd53b64fa30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = QAgent(dim_states, dim_actions) # noten como el agente debe recibir (dim_states, dim_actions) para inicializar la tabla\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Qtable # tabla con el valor para cada (estado, acción)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.select_action(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento\n",
    "\n",
    "Con el ambiente y el agente, podemos pasar a entrenar el agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "episodes = 1000\n",
    "timesteps = 100\n",
    "\n",
    "# training loop\n",
    "for episode in range(episodes):\n",
    "  # reset env\n",
    "  state, info = env.reset() \n",
    "  for timestep in range(timesteps):\n",
    "    # select action\n",
    "    action = agent.select_action(state)\n",
    "    # obtain env response\n",
    "    new_state, reward, done, truncated, info = env.step(action)\n",
    "    # gather experience\n",
    "    experience = (state, action, reward, new_state, done)\n",
    "    # update agent parameters\n",
    "    agent.update(*experience)\n",
    "    # stop if done\n",
    "    if done or truncated:\n",
    "      break\n",
    "    # update state\n",
    "    state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro código funcionó correctamente, pero como sabemos si nuestro agente efectivamente pudo resolver FrozenLake?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación\n",
    "\n",
    "Para evaluar que nuestro agente converge a una política óptima, haremos uso de 2 métodos de evaluación:\n",
    "\n",
    "1. **Cuantificar recompensas en entrenamiento**: La idea es medir el nivel de recompensas promedio cada X steps de entrenamiento\n",
    "2. **Evaluación visual**: Observar al agente jugando al juego FrozenLake\n",
    "\n",
    "Primero definiremos una función para testear nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent, test_episodes = 20, test_timesteps = 100):\n",
    "  '''\n",
    "  returns avg and std of rewards\n",
    "  '''\n",
    "  test_rewards = np.zeros(test_episodes)\n",
    "  for episode in range(test_episodes):\n",
    "    episode_reward = 0\n",
    "    state, info = env.reset()\n",
    "    for timestep in range(test_timesteps):\n",
    "      action = agent.select_action(state, greedy = True) # notar como solo seleccionamos acciones \"greedy\"\n",
    "      new_state, reward, done, truncated, info = env.step(action)\n",
    "      episode_reward += reward\n",
    "      if done:\n",
    "        break\n",
    "      state = new_state\n",
    "    test_rewards[episode] = episode_reward\n",
    "\n",
    "  return np.mean(test_rewards), np.std(test_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la función, la adjuntamos a nuestro loop de entrenamiento ejecutandola cada 20 episodios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# init env\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "# gather (dim_actions, dim_states)\n",
    "dim_actions = env.action_space.n\n",
    "dim_states = env.observation_space.n\n",
    "\n",
    "# init agent\n",
    "agent = QAgent(dim_states, dim_actions)\n",
    "\n",
    "# set hyperparams\n",
    "episodes = 1000\n",
    "timesteps = 100\n",
    "test_each = 20 # parámetro nuevo: testear cada 20 episodios!\n",
    "\n",
    "# init arrays for metrics\n",
    "test_episodes, test_avg_rewards, test_std_rewards = [], [], []\n",
    "\n",
    "# training loop\n",
    "for episode in range(episodes):\n",
    "  # reset env\n",
    "  state, info = env.reset() \n",
    "  for timestep in range(timesteps):\n",
    "    # select action\n",
    "    action = agent.select_action(state)\n",
    "    # obtain env response\n",
    "    new_state, reward, done, truncated, info = env.step(action)\n",
    "    # gather experience\n",
    "    experience = (state, action, reward, new_state, done)\n",
    "    # update agent parameters\n",
    "    agent.update(*experience)\n",
    "    # stop if done\n",
    "    if done or truncated:\n",
    "      break\n",
    "    # update state\n",
    "    state = new_state\n",
    "\n",
    "  # test each X episodes\n",
    "  if episode % test_each == 0:\n",
    "    avg_rewards, std_rewards = test_agent(env, agent)\n",
    "    test_episodes.append(episode)\n",
    "    test_avg_rewards.append(avg_rewards)\n",
    "    test_std_rewards.append(std_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora las recompensas obtenidas por el agente a lo largo del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (12, 4))\n",
    "plt.errorbar(test_episodes, test_avg_rewards, test_std_rewards, marker = '.')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Test Rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar el comportamiento del agente de forma visual, podemos inicializar el ambiente con el argumento `render_mode=\"human\"`. Esta opción no funciona bien en Google Colab, por lo que les recomiendo correr este código desde sus equipos personales.\n",
    "\n",
    "Veamos primero como se ve un agente que toma **acciones aleatorias**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\", map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "for episode in range(10):\n",
    "    done = truncated = False\n",
    "    state, info = env.reset()\n",
    "    for timestep in range(100):\n",
    "        action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, veamos como se ve el agente que entrenamos con Q-learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\", map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "for episode in range(10):\n",
    "    done = truncated = False\n",
    "    state, info = env.reset()\n",
    "    for timestep in range(100):\n",
    "        action = agent.select_action(state, greedy = True)\n",
    "        action = action.item()\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, veamos como quedó la tabla con los valores Q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.6983373 , 0.73509189],\n",
       "       [0.73509189, 0.        , 0.66292137, 0.69831519],\n",
       "       [0.6982489 , 0.54968721, 0.46990025, 0.60172276],\n",
       "       [0.60614198, 0.        , 0.26277443, 0.05283569],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.86776324, 0.        , 0.3218492 ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378092],\n",
       "       [0.81449495, 0.9025    , 0.90241327, 0.        ],\n",
       "       [0.85111412, 0.94999025, 0.        , 0.75674134],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.90248713, 0.95      , 0.85736916],\n",
       "       [0.90144915, 0.94983654, 1.        , 0.90178732],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Qtable # noten como los valores son distintos a los iniciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable-Baselines3\n",
    "\n",
    "En la sección anterior implementamos de forma nativa el algoritmo Q-learning para resolver FrozenLake. Si bien tuvimos resultados satisfactorios, implementar nativamente un algoritmo no es lo más deseable para trabajar de forma profesional, especialmente en el campo de RL donde los algoritmos actuales son **diametralmente de mayor complejidad**.\n",
    "\n",
    "[`Stable-Baselines3`](https://stable-baselines3.readthedocs.io/en/master/) es una librería que nos facilita la implementación de algoritmos de RL en pocas líneas de código. Gracias a su amplia aceptación y a su equipo de mantenimiento, esta librería se ha posicionado como una de las principales alternativas para resolver problemas de RL, pudiendo hacer uso de sus modelos en pocas líneas de código. \n",
    "\n",
    "A diferencia de librerías como scikit-learn, SB3 provee la mayor parte de su soporte a la implementación de modelos *deep*, es decir, modelos que usan **redes neuronales** por detrás. De esta forma, no sólo podemos implementar modelos de RL en pocas líneas de código, sino que estos modelos además son bastantes cercanos al **estado del arte**.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://stable-baselines3.readthedocs.io/en/master/_static/logo.png\" style=\"width: 30%;\">\n",
    "</div>\n",
    "\n",
    "Veamos como hacer uso de `SB3` para resolver el problema FrozenLake usando DQN (Q-learning pero con redes neuronales):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/stud/giturra/.local/lib/python3.10/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/stud/giturra/.local/lib/python3.10/site-packages/rich/live.py:256: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fd4ae95de70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\", map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "# init agent\n",
    "model = DQN(\"MlpPolicy\", env, verbose=0)\n",
    "# train the agent and display a progress bar\n",
    "model.learn(total_timesteps=int(2e5), progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noten como para entrenar el agente no tuvimos que escribir el loop de entrenamiento. \n",
    "\n",
    "**Ojo! Como estos modelos son de mayor complejidad, en algunos casos el entrenamiento puede tardar bastante. Pueden acelerar exponencialmente el entrenamiento de su modelo ejecutando el código en Google Colab y habilitando la opción de GPU.**\n",
    "\n",
    "Luego, pueden exportar el modelo entrenado a un archivo `.zip` en pocas lineas de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"dqn_frozen\") # export model (i.e from Google Colab)\n",
    "del model  # delete trained model to demonstrate loading\n",
    "model = DQN.load(\"dqn_frozen\", env=env) # load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasemos ahora a evaluar nuestro agente usando la función `evaluate_policy`. Al igual que en la sección anterior, esta función nos devuelve la recompensa promedio y su desviación obtenida por el agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos evaluar de manera visual el comportamiento de nuestro agente usando el mismo esquema de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:855:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\n",
      "ALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\", map_name=\"4x4\", is_slippery=False)\n",
    "\n",
    "for episode in range(10):\n",
    "    done = truncated = False\n",
    "    state, info = env.reset()\n",
    "    for timestep in range(100):\n",
    "        action, _states = model.predict(state, deterministic=True)\n",
    "        action = action.item() # important step: gather action of array\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        if done or truncated:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Ambientes Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ya tienen un conocimiento básico de RL, el siguiente paso natural es **usar RL para resolver nuestros propios problemas**. \n",
    "\n",
    "Para eso, deben seguir los siguientes pasos:\n",
    "\n",
    "1. Formular problema como un MDP (es decir, definir estados, acciones y recompensas)\n",
    "2. Desarrollar el Ambiente de entrenamiento\n",
    "3. Entrenar y desplegar el modelo\n",
    "\n",
    "Para hacerlo compatible con el notebook que vimos en esta clase, el ambiente debe seguir la siguiente estructura:\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.observation_space = ... # dimensión de la observación\n",
    "        self.action_space = ... # dimensión de las acciones\n",
    "\n",
    "    def reset(self):\n",
    "        return observation # devolver observación inicial\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # acá debe estar programada la dinámica del ambiente\n",
    "        # es decir, S_t --> A_t --> S_t+1\n",
    "\n",
    "        return observation, reward, done, info # devolver nueva observacion y recompensa\n",
    "```\n",
    "\n",
    "Finalmente, entrenan el agente para resolver su problema:\n",
    "\n",
    "```python\n",
    "env = CustomEnv(...)\n",
    "model = DQN(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=int(2e5), progress_bar=True)\n",
    "```\n",
    "\n",
    "Link de referencia:\n",
    "\n",
    "- [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.\n",
    "- [RL Course by David Silver](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-)\n",
    "- [Deep RL Course - HuggingFace](https://huggingface.co/learn/deep-rl-course/unit2/introduction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
