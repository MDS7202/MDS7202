{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6684655-91ae-488a-a543-03d28bbd81b9",
   "metadata": {
    "id": "c6684655-91ae-488a-a543-03d28bbd81b9"
   },
   "source": [
    "# Clase 09: Gradient boosting Machines (GBM)\n",
    "\n",
    "\n",
    "**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**\n",
    "\n",
    "**Objetivos**:\n",
    "- Entender el funcionamiento de **Gradient Boosting Machines** y sus ventajas/desventajas\n",
    "- Aprender a incorporar restricciones de modelo, como monoticidad y restricciones de interacción\n",
    "- Introducir el concepto de features importances\n",
    "- Reconocer y solucionar overfitting por medio de *Early Stopping*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GPum-gNFvRyN",
   "metadata": {
    "id": "GPum-gNFvRyN"
   },
   "source": [
    "## Contexto: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XQYP55LKv0V-",
   "metadata": {
    "id": "XQYP55LKv0V-"
   },
   "source": [
    "Para esta clase, necesitaremos tener un mínimo entendimiento del funcionamiento de **Árboles de Decisión** pues servirán como la unidad básica sobre la cual se construye el algoritmo **Gradient Boosting**.\n",
    "\n",
    "Pueden encontrar una buena explicación a este algoritmo en el siguiente enlace:\n",
    "[Medium](https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e/).\n",
    "\n",
    "<center>\n",
    "<img src='https://towardsdatascience.com/wp-content/uploads/2024/08/1-kVbTztm62HThXb6oy8arg-768x470.png' width=500 />\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6715d-90a9-4edc-a64e-3554fd759868",
   "metadata": {
    "id": "0ab6715d-90a9-4edc-a64e-3554fd759868"
   },
   "source": [
    "## Gradient boosting Machines (GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e3ccc-bc09-45f0-bfe8-1c9934f802a0",
   "metadata": {
    "id": "f97e3ccc-bc09-45f0-bfe8-1c9934f802a0",
    "tags": []
   },
   "source": [
    "Las GBM son algoritmos que se volvieron muy populares con la aparición de XGBoost en el 2016 para las tareas de clasificación y regresión. En términos de funcionamiento, estos algoritmos utilizan múltiples **weak-learners** con los que a través de múltiples iteraciones corrigen los errores que presentan sus regresores/clasificadores en etapas anteriores.\n",
    "\n",
    "\t\t❓ Pregunta: ¿Qué características tienen los \"weak-learners\"?\n",
    "\n",
    "1. El objetivo no es crear clasificadores potentes con ellos, si no como su nombre lo dice queremos generar \"aprendices débiles\" que se potencian con otros.\n",
    "2. En general se utilizan como weak learners árboles de decisión por su simpleza y versatilidad que nos ofrecen para regresión y clasificación.\n",
    "3. Pueden ser entrenados con una muestra de la totalidad para obtener entrenamientos rápidos sobre ellos.\n",
    "4. Su uso en un algoritmo implica la comprensión de estos elementos, ya que se heredan sus problemas en una estructura más compleja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef55d0-679c-40f4-93fc-d861ad65f08d",
   "metadata": {
    "id": "74ef55d0-679c-40f4-93fc-d861ad65f08d"
   },
   "source": [
    "❓ Pregunta: ¿Qué problemas tenían los árboles de decisión?\n",
    "\n",
    "<center>\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1702/1*KbRVJC5B0EgO8YAyeCrLkA.png' width=500 />\n",
    "</center>\n",
    "\n",
    "Recordar que los árboles de decisión son elementos que tienen una alta facilidad de sobre-ajustarse si aumentamos la profundidad o número de hijos por nodo. Esto puede causar problemas en un algoritmo de GBM y por ello, este debe ser un valor que debemos controlar de forma directa o indirectamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a42f5ff-80b5-490a-924e-5fef5dfc8102",
   "metadata": {
    "id": "6a42f5ff-80b5-490a-924e-5fef5dfc8102"
   },
   "source": [
    "Adentrándonos más en detalles en el algoritmo de gradient boosting, estos forman parte de los algoritmos de **ensemble**, es decir, algoritmos que **combinan múltiples modelos para el proceso de predicción**. Una forma de esto es a través de **modelos aditivos**, donde entrenando múltiples algoritmos simples de forma independiente, utilizamos la suma de sus salidas para generar un modelo más potente para la regresión/clasificación:\n",
    "$$F_M(x) = f_1(x)+...+f_M(x)=\\sum_{m=1}^M f_m(x)$$\n",
    "\n",
    "<center>\n",
    "<img src='https://machinelearningmastery.com/wp-content/uploads/2020/07/Example-of-Combining-Decision-Boundaries-Using-an-Ensemble.png' width=500 />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e28f15-95f2-4125-87f6-bdfa13312dc2",
   "metadata": {
    "id": "a6e28f15-95f2-4125-87f6-bdfa13312dc2"
   },
   "source": [
    "### GBM: Regresión\n",
    "\n",
    "La idea detrás de los GBM es realizar **descenso por gradiente en el espacio de funciones** para encontrar una función $\\hat{f}(x)$ que minimice lo más posible una función de pérdida $L$. Formalmente, buscamos:\n",
    "\n",
    "$$\n",
    "\\hat{f} = \\arg \\min_f \\sum_{i=1}^N L(y_i, f(x_i))\n",
    "$$\n",
    "\n",
    "donde en el caso de la regresión, es usual fijar $\\mathcal{L} = \\frac{1}{2} (y - f(x_i))^2$.\n",
    "\n",
    "> **❓ Pregunta:** ¿Qué función de pérdida es esta? ¿De donde viene el 1/2?\n",
    "\n",
    "El algoritmo se inicializa con una predicción **base y constante** $f_0(x)$ que minimiza la función de pérdida con respecto a los datos:\n",
    "\n",
    "$$\\arg \\min_F \\sum_{i=1}^N L(y_i, F(x_i))$$\n",
    "\n",
    "donde en la práctica se puede demostrar que este valor es el **promedio** de los valores $y$.\n",
    "\n",
    "Luego, agregaremos $m$ árboles de decisión o weak learners (donde $m$ es un parámetro definido por el usuario) repitiendo los siguientes pasos:\n",
    "\n",
    "- Primero, calcularemos los pseudo residuos $r_{im}$ con respecto al modelo actual:\n",
    "\n",
    "$$\n",
    "r_{im} = -\\left[\\dfrac{\\partial \\mathcal{L}(y_i, f(x_i))}{\\partial f(x_i)}\\right]_{f = f_{m-1}}\n",
    "$$\n",
    "\n",
    "Remplazando el error cuadrático medio en la expresión anterior, obtenemos\n",
    "\n",
    "$$\n",
    "r_{im} = y - f(x_i)\n",
    "$$\n",
    "\n",
    "- Luego, se entrena el weak learner para predecir los residuos $r_{im}$:\n",
    "\n",
    "$$\\arg \\min_F \\sum_{i=1}^N L(r_{im}, F(x_i))$$\n",
    "$$F_m = \\arg \\min_F \\sum_{i=1}^N \\frac{1}{2}(r_{im}-F(x_i))^2$$\n",
    "\n",
    "Notar que $F(x_i)$ es un weak learner que se entrena en cada una de las iteraciones de nuestro algoritmo de GBM.\n",
    "\n",
    "- Por último, se agrega el weak learner $F_m$ al modelo:\n",
    "\n",
    "$$f_m = f_{m-1} + \\nu F_{m}$$\n",
    "Donde $\\nu$ se define como el largo del paso en la corrección. Visto de otra forma, podemos interpretar este valor como una **tasa de aprendizaje** que podremos fijar o optimizar en la función, señalando cuánta corrección aplicaremos del proceso anterior en el proceso actual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106ab54-77ce-4806-994c-7a4dc0020537",
   "metadata": {
    "id": "0106ab54-77ce-4806-994c-7a4dc0020537"
   },
   "source": [
    "Finalmente el algoritmo se vería como:\n",
    "\n",
    "![](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/21_Ensamblaje/Pasted%20image%2020230527153450.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceffe09-45a9-4009-9131-c5f0c4a260b6",
   "metadata": {
    "id": "7ceffe09-45a9-4009-9131-c5f0c4a260b6"
   },
   "source": [
    "### GBM: Clasificación\n",
    "\n",
    "\t\t❓ Pregunta: ¿Debería ser diferente la clasificación a la regresión?\n",
    "\n",
    "La respuesta directa a esto es \"sí\". Los cambios que presenta la clasificación respecto a la regresión se dan netamente en la **elección de la función de pérdida** ($\\mathcal{L}$) y algunas consideraciones que se toman para las salidas de los nodos.\n",
    "\n",
    "Respecto a las funciones de pérdida, esto se debe a cómo se define el problema de clasificación supervisada, donde nuestro interés no será la reducción del error que tenemos del valor estimado respecto a un valor continuo. En su reemplazo son utilizadas **funciones de pérdida para clasificación**, quienes tienen como principal objetivo disminuir el error al predecir una etiqueta. Una de las funciones de pérdidas más conocidas/utilizadas para el problema de clasificación se encuentra la **logistic-loss**/**cross-entropy**:\n",
    "$$\\mathcal{L}_i = -(y_i log(p_i) + (1-y_i)log(1-p_i))$$\n",
    "\t❓ Pregunta:  ¿Cómo podemos interpretar esta función de pérdida?\n",
    "\n",
    "Donde su derivada es:\n",
    "$$\\dfrac{\\partial \\mathcal{L}_i}{\\partial \\hat{y}}=p_i-y_i$$\n",
    "El segundo punto a considerar es que la estimación de $y$ será igual a:\n",
    "$$\\hat{y}=log(odds)=log(\\dfrac{p}{1-p})$$\n",
    "\t\t❓ Pregunta:  ¿Por qué usamos odds?\n",
    "\n",
    "Básicamente debido a que los modelos de clasificación con GBM son modelos de regresión, por lo que sus salidas no pueden ser consideradas como probabilidades, sino como scores que debemos transformar y normalizar.\n",
    "\n",
    "Finalmente para calcular las probabilidades de las salidas de nuestro GBM tendremos que calcular la función softmax de $\\hat{y}$, la que viene dada por:\n",
    "$$\\text{softmax}(\\hat{y})=\\dfrac{1}{1+e^{-\\hat{y}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a3d69-63da-47bc-8468-c5517cec4274",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1745247279861,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "6a9a3d69-63da-47bc-8468-c5517cec4274",
    "outputId": "50774028-dcf8-4212-cfaf-e6d550ba60e3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Ejemplo de función de pérdida: Caso con score alejado de la etiqueta\n",
    "p = 0.99 # salida del modelo\n",
    "y = 0 # etiqueta real\n",
    "-(y*math.log(p)+(1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d66a4-9905-40d6-9501-4b58f59d469c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745247281680,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "d63d66a4-9905-40d6-9501-4b58f59d469c",
    "outputId": "f5d2ebcb-2519-41e1-b110-8c9d720d085c"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de función de pérdida: Caso con score cercano a la etiqueta\n",
    "p = 0.9 # salida del modelo\n",
    "y = 1 # etiqueta real\n",
    "-(y* math.log(p) + (1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36f6af-cde4-4944-b2c8-07b2df783e68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745247283408,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "2f36f6af-cde4-4944-b2c8-07b2df783e68",
    "outputId": "4d301ad8-e064-4e3a-ee21-2496bcac6f8f"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de función de pérdida: Caso con score cercano a la etiqueta (etiqueta 0)\n",
    "p = 0.3 # salida del modelo\n",
    "y = 0 # etiqueta real\n",
    "-(y* math.log(p) + (1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef24a7",
   "metadata": {
    "id": "00ef24a7"
   },
   "source": [
    "Podemos graficar el comportamiento de la loss function para diferentes valores de $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35087c5c-1382-4698-bfc5-b28334ea2811",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1745247297858,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "35087c5c-1382-4698-bfc5-b28334ea2811",
    "outputId": "64e8ab04-459c-4272-cd30-3cdcd8b75a99"
   },
   "outputs": [],
   "source": [
    "lis1, prob = [], []\n",
    "for p in range(1, 10):\n",
    "    y = 1\n",
    "    lis1.append(-(y* math.log(p/10) + (1-y)*math.log(1-p/10)))\n",
    "    prob.append(p/10)\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "px.line(x=prob, y=lis1, title=f'Loss with different predictions: y={y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753bb7e-4280-47c2-bae5-c3990a0ad0b9",
   "metadata": {
    "id": "4753bb7e-4280-47c2-bae5-c3990a0ad0b9"
   },
   "source": [
    "### Retrospectiva\n",
    "\n",
    "\t\t❓ Pregunta: ¿Pero cómo podríamos simplificar estas definiciones que acabamos de dar?\n",
    "\n",
    "Si visualizamos GBM en un juego de golf, tendríamos un jugador que comienza con un tiro $f_0$, el cual a medida que va realizando cada tiro va aplicando una **corrección** $\\Delta_m$ en cada una de sus jugadas. Con esto, si el jugador corrige sus errores a través de una función MSE, tendremos que el jugador irá optimizando esta función hasta meter la pelota en el agujero (mínimo de la función de pérdida).\n",
    "\n",
    "$\\text{MSE LOSS} = \\mathcal{L}(y, F_M(X))=\\dfrac{1}{N} \\sum_{i=1}^N (y_i - F_M(x_i))^2$\n",
    "\n",
    "<center>\n",
    "<img src='https://explained.ai/gradient-boosting/images/golf-MSE.png' width=500 />\n",
    "</center>\n",
    "\n",
    "\t\t❓ ¿Cómo podría el golfista corregir de mejor forma sus tiros? ¿Existe alguna forma?.\n",
    "\n",
    "<center>\n",
    "<img src='https://explained.ai/gradient-boosting/images/1d-vectors.png' width=500 />\n",
    "</center>\n",
    "\n",
    "Como vimos durante el entrenamiento de GBM, este proceso considera un $\\nu$ conocido como shrinkage rate o **learning rate**. Este valor nos permitirá realizar correcciones en las actualizaciones de nuestras funciones $f_m$, ya que podría darse el caso que el gradiente obtenido durante la optimización sea muy alto y nos permita obtener un mínimo adecuado para el problema. De esta forma:\n",
    "$$f_m(x) = f_{m-1}(x) + \\nu F_m(x)$$\n",
    "Otra forma podría ser **cambiando la función de pérdida** que definimos en el problema. Dependiendo si es un problema de clasificación o regresión existen múltiples funciones de pérdida con diferentes interpretaciones que pueden mejorar significativamente el problema que deseamos resolver. Algunas de estas son:\n",
    "\n",
    "| Nombre         | Loss                         | Derivada           |  \n",
    "| -------------- | ---------------------------- | ------------------ |\n",
    "| Squared Error | $\\dfrac{1}{2}(y_i-f(x_i))^2$ | $y_i -f(x_i)$      |\n",
    "| Absolute Error | $y_i - f(x_i)$               | $sgn(y_i -f(x_i))$ |\n",
    "| Binary Logloss                | $log(1+e^{-y_if_i})$  | $y-\\sigma(2f(x_i))$    |\n",
    "\n",
    "En tercer lugar tenemos la adición de un **regularizador** a nuestra función de pérdida, de esta forma podremos disminuir el sobreajuste de nuestro modelo, generando pequeñas modificaciones en los árboles que vamos generando.\n",
    "$$\\mathcal{L}(f) = \\sum_{i=1}^Nl(y_i,f(x))+\\Omega(f))$$\n",
    "donde $\\Omega$:\n",
    "$$\\Omega(f)= \\gamma J+\\dfrac{1}{2}\\lambda \\sum_{j=1}^J w^2_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b7e14-270b-4467-924a-10189151b12d",
   "metadata": {
    "id": "d41b7e14-270b-4467-924a-10189151b12d"
   },
   "source": [
    "### Ventajas & Desventajas de GBM\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Capacidad para manejar tanto problemas de regresión como de clasificación.\n",
    "- Puede capturar relaciones no lineales y características complejas en los datos.\n",
    "- Adaptabilidad a diferentes tipos de predictores, incluyendo variables numéricas y categóricas.\n",
    "- Robustez ante valores atípicos y datos ruidosos.\n",
    "- Capacidad para manejar grandes conjuntos de datos y escalabilidad.\n",
    "- Flexibilidad en la elección de funciones de pérdida y métricas de evaluación.\n",
    "- Capacidad para manejar características faltantes y utilizar todas las observaciones disponibles.\n",
    "- Puede generar importancia de variables para ayudar en la interpretación y selección de características.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Mayor complejidad y tiempo de entrenamiento en comparación con algoritmos más simples.\n",
    "- Puede ser propenso a sobreajuste si no se controlan los hiperparámetros adecuadamente.\n",
    "- Requiere una configuración cuidadosa de los hiperparámetros para obtener el mejor rendimiento.\n",
    "- Interpretación más difícil debido a la naturaleza de combinación de múltiples árboles.\n",
    "- Sensible a datos desequilibrados, donde las clases minoritarias pueden no recibir suficiente atención.\n",
    "- La importancia de las variables puede verse sesgada hacia las variables numéricas o con mayor cardinalidad.\n",
    "- No es adecuado para problemas con alta dimensionalidad o con muchos predictores.\n",
    "- Mayor consumo de recursos computacionales en comparación con algoritmos más simples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac118e-ce1c-438a-891d-ea51090f5278",
   "metadata": {
    "id": "1bac118e-ce1c-438a-891d-ea51090f5278"
   },
   "source": [
    "## Ejemplo a Mano 🤨\n",
    "\n",
    "Como ya sabemos como funciona por detrás un algoritmo simple de Gradient Boosting, veamos un ejemplo para ejecutar un pequeño GBM a mano. Para esto consideremos los siguientes datos:\n",
    "\n",
    "| index | $feature_1$ | $feature_2$ | y     |  \n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1     | 1.12  | 1.4   | 1     |   \n",
    "| 2     | 5.45  | 3.1   | 0     |   \n",
    "| 3     | 3.54  | 1.2   | 1     |   \n",
    "\n",
    "Considerando los datos construir un modelo de GBM con 2 weak learners y un $\\nu$ igual a $0.1$.\n",
    "\n",
    "**Desarrollo:**\n",
    "Con la tabla anterior, comenzamos realizando nuestra primera iteración (m=1), para ello se considera que todos los datos se encuentran en el mismo nodo. De esta forma, calcularemos nuestra predicción base a través de odds y los log(odds)::\n",
    "\n",
    "$$odds = \\dfrac{\\text{cantidad de casos exitosos (y=1)}}{\\text{cantidad de casos fallidos (y=0)}}$$\n",
    "\n",
    "$$odds = 2/1 = 2$$\n",
    "Luego para los $log_n(odds)$:\n",
    "$$log_n(odds)=log_n(2)=0.693$$\n",
    "Notar que debido a que todos los valores están en el mismo nodo, todos los valores poseerán el mismo $log_n(odds)$, y por lo tanto, la misma predicción inicial:\n",
    "$$p=\\dfrac{1}{1+e^{-\\hat{y}}}=\\dfrac{1}{1+e^{-0.693}}=0.67$$\n",
    "Con este valor, calculamos los pseudo residuos para cada observación:\n",
    "$$r_{10}=y-p_1=1-0.67=0.33$$\n",
    "$$r_{20}=y-p_2=0-0.67=-0.67$$\n",
    "$$r_{30}=y-p_3=1-0.67=0.33$$\n",
    "Tenemos nuestro primer estimador calculado, ahora el siguiente paso es generar un nuevo árbol. Para esto vamos a generar un árbol donde el nodo padre tiene la regla $feature_2>2$.\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/21_Ensamblaje/Flowchart%20Template.jpg?raw=true' width=400 />\n",
    "</center>\n",
    "    \n",
    "Considerando los resultados obtenidos en la iteración anterior, podemos calcular las predicciones del árbol $\\gamma_i$:\n",
    "\n",
    "$$\\gamma_{i}=\\dfrac{\\sum_i y_i-p_{i}}{\\sum_i p_{i}(1-p_{i})}$$\n",
    "\n",
    "donde el índice $i \\in (1, 2)$ representa la hoja del árbol.\n",
    "\n",
    "Reemplazando cada valor:\n",
    "$$\\gamma_{1}=\\dfrac{0.33+0.33}{2*(0.67(1-0.67))}=1.49$$\n",
    "\n",
    "$$\\gamma_{2}=\\dfrac{-0.67}{0.67(1-0.67)}=-3.03$$\n",
    "\n",
    "Finalmente, con los valores obtenidos calculamos el $f_m$ resultante de las 2 iteraciones para cada una de las hojas:\n",
    "\n",
    "$$f_1(x)=f_{0}(x)+\\nu \\cdot \\gamma_{1}$$\n",
    "\n",
    "Reemplazamos para cada uno de los valores que tenemos por fila:\n",
    "\n",
    "$$\\hat{y_1}=f_1(x_1)=0.69+0.1*1.49=0.839$$\n",
    "$$\\hat{y_2}=f_1(x_2)=0.69+0.1*(-3.03)=0.387$$\n",
    "$$\\hat{y_3}=f_1(x_3)=0.69+0.1*1.49=0.839$$\n",
    "\n",
    "Como podemos ver, estos valores solamente representan un score que no nos indica la probabilidad, para esto aplicamos una softmax para normalizar las salidas:\n",
    "\n",
    "$$p_1=\\dfrac{1}{1+e^{-\\hat{y}}}=0.698$$\n",
    "$$p_2=0.595$$\n",
    "$$p_3=0.698$$\n",
    "Finalmente los residuales de cada una de las salidas es:\n",
    "\n",
    "$$r_{12} = r_{32} = 0.302$$\n",
    "$$r_{22} = -0.595$$\n",
    "\n",
    "Noten como el error de cada observación disminuye :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03c8ee-e596-49dc-991d-491c3f944c32",
   "metadata": {
    "id": "ca03c8ee-e596-49dc-991d-491c3f944c32"
   },
   "source": [
    "## Ejemplo con Código 🧐\n",
    "\n",
    "Como ya comprendimos que es un algoritmo de GBM de forma teórica y como este se calcula a mano, el último paso es visualizar como funciona el entrenamiento a través de código. Para realizar este ejercicio modificaremos desde 0 el algoritmo de GBM utilizando los `DecisionTreeClassifier` de scikit-learn.\n",
    "\n",
    "**Objetivo:** Codificar cada una de las partes que contiene un algoritmo de GBM en una clase de python.\n",
    "\n",
    "<center>\n",
    "<img src='https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Gradient-boosting-LightGBM-vs-XGBoost.png?resize=591%2C431&ssl=1' width=500 />\n",
    "</center>\n",
    "    \n",
    "Comenzamos definiendo el inicializador de nuestra clase `GBMClassifier`, para esto definimos los siguientes valores:\n",
    "- `n_estimators` = Número de árboles que vamos a entrenar en el entrenamiento secuencial e iterativo.\n",
    "- `max_depth` = Máxima profundidad de los arboles que vamos a entrenar.\n",
    "- `lr` = Tasa de aprendizaje de nuestro GBM.\n",
    "- `loss` = Definimos la función de pérdida que vamos a utilizar para el problema.\n",
    "\n",
    "```python\n",
    "class GBMClassifier:\n",
    "    def __init__(self, n_trees, learning_rate=0.1, max_depth=1, loss_function=None):\n",
    "        self.n_trees=n_trees\n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_depth=max_depth\n",
    "        self.loss_function = loss_function\n",
    "```\n",
    "\n",
    "En segundo lugar debemos definir el método que entrenará a nuestro modelo GBM. Para la construcción consideramos los siguientes pasos:  \n",
    "1. Generamos una **lista donde almacenaremos nuestros árboles** en cada una de las iteraciones, para esto generamos un atributo donde será almacenado.\n",
    "2. En segundo lugar es necesario **obtener la predicción base** de la primera etapa de nuestro modelo utilizando la función que calcula los gradientes de la función de pérdida.\n",
    "3. Generamos un **loop en el que entrenaremos los $n$ árbole**s que definimos al inicializar la GBM y realizamos los siguientes pasos.\n",
    "\t1. **Obtener los gradientes ($\\gamma$)** de cada una de las etapas.\n",
    "\t2. **Entrenamos los árboles** con los gradientes obtenidos en 1.\n",
    "\t3. **Actualizamos las predicciones** realizadas por nuestros árboles.\n",
    "\t4. **Obtenemos $F_M$ utilizando las predicciones** y el learning rate.\n",
    "\t5. **Guardamos el árbol entrenado** en esta etapa en nuestra lista de árboles.\n",
    "\n",
    "```python\n",
    "def fit(self):\n",
    "    self.trees = []\n",
    "    self.base_prediction = self._argmin_fun(y=y)\n",
    "    current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
    "    for _ in range(self.n_trees):\n",
    "        pseudo_residuals = self.loss_function.negative_gradient(y, current_predictions)\n",
    "        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "        tree.fit(X, pseudo_residuals)\n",
    "        self._update_terminal_nodes(tree, X, y, current_predictions)\n",
    "        current_predictions += self.learning_rate * tree.predict(X)\n",
    "        self.trees.append(tree)\n",
    "```\n",
    "\n",
    "Para calcular los gradientes de nuestra función utilizaremos una función de minimización, de esta forma no nos preocupamos de las derivadas. En la definición de este método tendremos dos casos: el caso base, cuando no se tiene un $\\hat{y}$ y un segundo cuando se tiene las predicciones de un estimador.\n",
    "\n",
    "```python\n",
    "def _argmin_fun(self, y, y_hat=None):\n",
    "    if np.array(y_hat).all() == None:\n",
    "        fun = lambda c: self.loss_function.loss(y, c)\n",
    "    else:\n",
    "        fun = lambda c: self.loss_function.loss(y, y_hat+c)\n",
    "    c0 = y.mean()\n",
    "    return minimize(fun=fun, x0=c0).x[0]\n",
    "```\n",
    "\n",
    "Finalmente tenemos la actualización de los nodos del árbol entrenado, para esto comenzamos obtenido las `ids` de los nodos que contienen las predicciones y los recorremos en un loop:\n",
    "\n",
    "1. Dentro del loop encontramos todos los valores que estén la misma hoja y seleccionamos los `y` e $\\hat{y}$ que poseen estos valores.\n",
    "2. Calculamos el $\\gamma$ de estos valores y los reemplazamos en los árboles.\n",
    "\n",
    "```python\n",
    "def _update_terminal_nodes(self, tree, X, y, current_predictions):\n",
    "    leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
    "    leaf_node_for_each_sample = tree.apply(X)\n",
    "    for leaf in leaf_nodes:\n",
    "        samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
    "        y_in_leaf = y.take(samples_in_this_leaf, axis=0)\n",
    "        preds_in_leaf = current_predictions.take(samples_in_this_leaf, axis=0)\n",
    "        val = self._argmin_fun(y=y_in_leaf, y_hat=preds_in_leaf)\n",
    "        tree.tree_.value[leaf, 0, 0] = val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bc227-ecef-40fa-a0c4-e481915fe7b6",
   "metadata": {
    "id": "620bc227-ecef-40fa-a0c4-e481915fe7b6"
   },
   "source": [
    "### Definamos la clase que creamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917a2ec-2322-4c76-9e06-1d92a6606339",
   "metadata": {
    "executionInfo": {
     "elapsed": 1675,
     "status": "ok",
     "timestamp": 1745246740804,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "4917a2ec-2322-4c76-9e06-1d92a6606339"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class GradientBoostingMachine():\n",
    "    def __init__(self, n_trees, learning_rate=0.1, max_depth=1, loss_function=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Función para entrenar GBM'''\n",
    "        self.trees = []\n",
    "        self.base_prediction = self._argmin_fun(y=y)  # obtener predicción base\n",
    "        current_predictions = self.base_prediction * np.ones_like(y)\n",
    "\n",
    "        for _ in range(self.n_trees):  # iterar para cada árbol a agregar\n",
    "            pseudo_residuals = self.loss_function.negative_gradient(y, current_predictions)\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, pseudo_residuals)\n",
    "            self._update_terminal_nodes(tree, X, y, current_predictions)\n",
    "            current_predictions += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _argmin_fun(self, y, y_hat=None):\n",
    "        '''Encuentra el mejor valor constante que minimiza la pérdida'''\n",
    "        if y_hat is None:\n",
    "            fun = lambda c: self.loss_function.loss(y, c * np.ones_like(y))\n",
    "        else:\n",
    "            fun = lambda c: self.loss_function.loss(y, y_hat + c)\n",
    "        c0 = np.array([y.mean()])\n",
    "        return minimize(fun=fun, x0=c0).x[0]\n",
    "\n",
    "    def _update_terminal_nodes(self, tree, X, y, current_predictions):\n",
    "        '''Actualiza los nodos terminales del árbol'''\n",
    "        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
    "        leaf_node_for_each_sample = tree.apply(X)\n",
    "        for leaf in leaf_nodes:\n",
    "            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
    "            if len(samples_in_this_leaf) == 0:\n",
    "                continue\n",
    "            y_in_leaf = y[samples_in_this_leaf]\n",
    "            preds_in_leaf = current_predictions[samples_in_this_leaf]\n",
    "            val = self._argmin_fun(y=y_in_leaf, y_hat=preds_in_leaf)\n",
    "            tree.tree_.value[leaf, 0, 0] = val\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''Genera predicción para un nuevo conjunto de datos X'''\n",
    "        return (self.base_prediction +\n",
    "                self.learning_rate *\n",
    "                np.sum([tree.predict(X) for tree in self.trees], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4732efd-d1e8-4fc1-8d20-71810335d08a",
   "metadata": {
    "id": "d4732efd-d1e8-4fc1-8d20-71810335d08a"
   },
   "source": [
    "#### Probemos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed80534-1480-4ff3-9bdf-e87931701bba",
   "metadata": {
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1745246740895,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "fed80534-1480-4ff3-9bdf-e87931701bba"
   },
   "outputs": [],
   "source": [
    "import numpy.random as rng\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# test data\n",
    "def make_test_data(n, noise_scale):\n",
    "    x = np.linspace(0, 10, 500).reshape(-1,1)\n",
    "    y = (np.where(x < 5, x, 5) + rng.normal(0, noise_scale, size=x.shape)).ravel()\n",
    "    return x, y\n",
    "\n",
    "# print model loss scores\n",
    "def print_model_loss_scores(obj, y, preds, sk_preds):\n",
    "    print(f'From Scratch Loss = {obj.loss(y, pred):0.4}')\n",
    "    print(f'Scikit-Learn Loss = {obj.loss(y, sk_pred):0.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4578ab0-647c-4feb-ba15-5551f209c185",
   "metadata": {
    "id": "c4578ab0-647c-4feb-ba15-5551f209c185"
   },
   "source": [
    "Comenzamos generando unos datos con cierto grado de relación para generar una regresión simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be632ae1-a6f8-414e-8a8a-0dad43952404",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1745246741023,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "be632ae1-a6f8-414e-8a8a-0dad43952404",
    "outputId": "39d721e4-4b18-49f8-9c73-a12ad21f38ed"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "x, y = make_test_data(500, 0.4)\n",
    "px.scatter(x, y, title='Datos de Prueba',template='simple_white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d222aa-b357-4842-954f-ca0dda66a48c",
   "metadata": {
    "id": "61d222aa-b357-4842-954f-ca0dda66a48c"
   },
   "source": [
    "Donde nuestra función de perdida viene dada por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbad0e-b303-4994-96f9-aef0afd3cfee",
   "metadata": {
    "executionInfo": {
     "elapsed": 710,
     "status": "ok",
     "timestamp": 1745246741734,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "19dbad0e-b303-4994-96f9-aef0afd3cfee"
   },
   "outputs": [],
   "source": [
    "class MSELoss():\n",
    "    '''User-Defined Squared Error Loss'''\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y, preds):\n",
    "        return np.mean((y - preds)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_gradient(y, preds):\n",
    "        return y - preds\n",
    "\n",
    "\n",
    "gbm = GradientBoostingMachine(n_trees=100,\n",
    "                              learning_rate=0.5,\n",
    "                              max_depth=1,\n",
    "                              loss_function=MSELoss(),\n",
    "                              )\n",
    "gbm.fit(x, y)\n",
    "pred = gbm.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03fee02-f237-42be-91da-e9c7c8b7a24d",
   "metadata": {
    "id": "c03fee02-f237-42be-91da-e9c7c8b7a24d"
   },
   "source": [
    "Con los datos generamos un loop para visualizar como es el impacto que los estimadores dentro de la regresión, con esto obtenemos los siguientes gráficos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1416df-3aec-4512-bf19-773263b52bf3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5335,
     "status": "ok",
     "timestamp": 1745246747067,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "5a1416df-3aec-4512-bf19-773263b52bf3",
    "outputId": "9921393b-0cfc-472b-b61b-e097caa43e34"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regresion(x, y, y_pred, name=\"Datos con regresión\", color='r'):\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, pred, color='r')\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(1, 11):\n",
    "    gbm = GradientBoostingMachine(n_trees=i,\n",
    "                                  learning_rate=0.5,\n",
    "                                  max_depth=1,\n",
    "                                  loss_function=MSELoss()\n",
    "                                 )\n",
    "    gbm.fit(x, y)\n",
    "    pred = gbm.predict(x)\n",
    "    name_plot = f\"Datos con regresión con {i} estimadores\"\n",
    "    plot_regresion(x, y, pred, name=name_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d4dcfa-c425-4e73-9b0c-1c2a0620665a",
   "metadata": {
    "id": "f5d4dcfa-c425-4e73-9b0c-1c2a0620665a"
   },
   "source": [
    "Comprobemos si esto tiene sentido con el clasificador de boosting que viene en sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15c0b5-3418-4f66-b7b3-8292652f73dc",
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1745246747107,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "7d15c0b5-3418-4f66-b7b3-8292652f73dc"
   },
   "outputs": [],
   "source": [
    "sk_gbm = GradientBoostingRegressor(n_estimators=10,\n",
    "                                   learning_rate=0.5,\n",
    "                                   max_depth=1)\n",
    "sk_gbm.fit(x, y)\n",
    "sk_pred = sk_gbm.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c525d-d947-4cf1-bdb8-aea91ecbc855",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745246747132,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "d13c525d-d947-4cf1-bdb8-aea91ecbc855",
    "outputId": "060bce6a-680c-4b1d-963e-f6f3d7331232"
   },
   "outputs": [],
   "source": [
    "print_model_loss_scores(MSELoss(), y, pred, sk_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1649bf-b1be-47bd-b14a-b78cae35e035",
   "metadata": {
    "id": "2d1649bf-b1be-47bd-b14a-b78cae35e035"
   },
   "source": [
    "De los resultados se observa que a medida que aumentamos el número de estimadores es posible generar una regresión más similar a la tendencia que señalan los datos, por lo que la adición de weak-learners a nuestro modelo fue efectiva!\n",
    "\n",
    "## Retrospectiva\n",
    "\n",
    "\t\t❓ Pregunta: ¿añadir muchos estimadores que puede provocar?\n",
    "\t\t❓ Pregunta: Sabemos que los árboles son interpretables, ¿que sucede con los algoritmos de ensemble?\n",
    "\t\t❓ Pregunta: ¿Comó podemos medir el sobre ajuste de estos modelos si son multiples árboles?\n",
    "\t\t❓ Pregunta: ¿A nivel general son buenos estimadores los GBM?\n",
    "\n",
    "\n",
    "Links de interés:\n",
    "https://explained.ai/gradient-boosting/descent.html\n",
    "https://www.cienciadedatos.net/documentos/py09_gradient_boosting_python.html\n",
    "https://explained.ai/gradient-boosting/L2-loss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da94600-2837-4b22-85c4-c49e517ca672",
   "metadata": {
    "id": "8da94600-2837-4b22-85c4-c49e517ca672"
   },
   "source": [
    "## Restricciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae850e-e0b4-4636-bc73-d7990d023fa4",
   "metadata": {
    "id": "60ae850e-e0b4-4636-bc73-d7990d023fa4"
   },
   "source": [
    "<center>\n",
    "<img src='https://media0.giphy.com/media/xT5LMQSg7kWT4zqsVy/200w.gif?cid=6c09b952x4akl84fpyhlm0306y58pyc5ju6yarl6ifo7h0ev&ep=v1_internal_gif_by_id&rid=200w.gif&ct=g' width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2455263-d46b-450e-a86c-9574bdcf673c",
   "metadata": {
    "id": "b2455263-d46b-450e-a86c-9574bdcf673c"
   },
   "source": [
    "### Qué es una función Monotóna?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce76ee5-2295-43e1-913b-cbbfb4d83657",
   "metadata": {
    "id": "8ce76ee5-2295-43e1-913b-cbbfb4d83657"
   },
   "source": [
    "Diferentes variaciones entre una característica X e Y se reconocen como una función no monótona, y podemos representar esto de la siguiente manera:\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915081517.png?raw=true' width=350 />\n",
    "\n",
    "Por otro lado, si la relación aumenta, tenemos una función monótonamente creciente:\n",
    "\n",
    "$$f(x_{1}, x_{2}, x, \\dots, x_{n}) \\geq f(x_{1}, x_{2}, x', \\dots, x_{n})$$\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915081603.png?raw=true' width=350 />\n",
    "\n",
    "Finalmente, otro caso es la función monótonamente decreciente, dada por una relación decreciente entre X e Y:\n",
    "\n",
    "$$f(x_{1}, x_{2}, x, \\dots, x_{n}) \\leq f(x_{1}, x_{2}, x', \\dots, x_{n})$$\n",
    "    \n",
    "<centering>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915081659.png?raw=true' width=350 />\n",
    "</centering>\n",
    "\n",
    "\n",
    "Matemáticamente, si podemos establecer **restricciones que nos permitan generar una salida positiva o negativa para cada variable**, esto nos permitirá **generar un mejor modelo** para el negocio.\n",
    "\n",
    "Estas son las ideas básicas que necesitamos comprender para saber cómo utilizar esto. El siguiente paso es definir cómo podemos obtener o visualizar el comportamiento monótono entre las variables (si no tenemos una idea clara de la relación). La mejor manera de hacerlo es mediante el uso de Gráficos de Dependencia Parcial (PDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38d6cc-7b77-4484-9bd8-fae5ac7b8471",
   "metadata": {
    "id": "5b38d6cc-7b77-4484-9bd8-fae5ac7b8471"
   },
   "source": [
    "### Monoticidad en Modelos\n",
    "#### Motivación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a580d1-614a-49c0-94ea-5ac0ea01521e",
   "metadata": {
    "id": "78a580d1-614a-49c0-94ea-5ac0ea01521e"
   },
   "source": [
    "**Si conocemos la relación entre dos variables y el objetivo** (por ejemplo, desde un punto de vista teórico), **deberíamos generar o ayudar al modelo a generar un modelo con este comportamiento**. Agregar restricciones al modelo nos permitirá obtener **modelos mejores y con menos sesgo** en sus decisiones. Recuerda, si podemos corregir el sesgo en un modelo, podremos tomar decisiones mejores en el futuro.\n",
    "\n",
    "Por ejemplo: Supongamos que tenemos pocos datos para una variable $X_1$ que sabemos tiene un comportamiento monótonamente negativo con el objetivo Y. Sin embargo, el comportamiento en el conjunto de datos de entrenamiento nos muestra una historia diferente (por ejemplo, por pocos datos). Si observáramos la relación entre el objetivo y la característica, veríamos una tendencia monótonamente positiva. ¿Cómo podemos resolver esto?... mediante la imposición de restricciones de monotonía en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475972e6-994b-4e82-ae31-4cc2e924b60c",
   "metadata": {
    "id": "475972e6-994b-4e82-ae31-4cc2e924b60c"
   },
   "source": [
    "![image4](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915093309.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c84ff1-f824-4791-8928-d57216503c6d",
   "metadata": {
    "id": "52c84ff1-f824-4791-8928-d57216503c6d"
   },
   "source": [
    "¿Qué logramos al aplicar estas restricciones a nuestro modelo? Básicamente, resolvimos 3 puntos durante el modelado: **Equidad**, **consistencia** y **confiabilidad**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63e9e2-8bc9-46f5-b7fe-71797e358ce4",
   "metadata": {
    "id": "ec63e9e2-8bc9-46f5-b7fe-71797e358ce4"
   },
   "source": [
    "![](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915095556.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef02125-2689-42d6-9193-8f0b3f421156",
   "metadata": {
    "id": "2ef02125-2689-42d6-9193-8f0b3f421156"
   },
   "source": [
    "#### Dependencias Parciales\n",
    "\n",
    "¿Qué es un **Gráfico de Dependencias Parciales**? Básicamente, es un **gráfico que nos ayuda a comprender el comportamiento de una variable en relación con la variable objetivo**. Estos gráficos nos permiten ver el **comportamiento esperado (promedio**) de la variable con respecto al objetivo y se representan como un gráfico de líneas donde X es la variable que deseamos analizar e Y es la variable objetivo.\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915082448.png?raw=true' width=800 />\n",
    "</center>\n",
    "    \n",
    "#### ¿Cómo se implementa esto?\n",
    "\n",
    "La función parcial $\\hat{f}_S$ se estima calculando promedios en los datos de entrenamiento, también conocido como **método de Monte Carlo**:\n",
    "\n",
    "$$\\hat{f}_S(x_S) = \\frac{1}{n} \\sum_{i=1}^n \\hat{f} (x_S, x_C^{(i)})$$\n",
    "\n",
    "La función parcial nos dice cuál es el **efecto marginal promedio** en la predicción para un valor dado (o valores) de las características S. En esta fórmula, $x_C^{(i)}$ son los valores reales de las características del conjunto de datos para las características en las que no estamos interesados, y n es el número de instancias en el conjunto de datos. Se asume en el PDP que las características en C no están correlacionadas con las características en S.\n",
    "\n",
    "#### ¿Por qué es relevante?\n",
    "\n",
    "Una comparación de gráficos de dependencias parciales puede proporcionar una visión importante, por ejemplo, revelar la estabilidad de la predicción del modelo. Algunas otras características que estos modelos permiten son:\n",
    "\n",
    "1. Se **pueden crear perfiles para todas las observaciones en el conjunto**, así como para la división en relación con otras variables. Por ejemplo, podemos ver cómo se comporta una variable específica cuando se diferencia por género, raza u otros factores.\n",
    "2. Podemos **detectar algunas relaciones complicadas entre variables**. Por ejemplo, tenemos perfiles de dependencia parcial para dos modelos y podemos ver que uno de los modelos simples (regresión lineal) no detecta ninguna dependencia, mientras que el perfil de un modelo de caja negra (bosque aleatorio) nota una diferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f38da0-be78-4a23-891e-55d6d0fbe936",
   "metadata": {
    "id": "b3f38da0-be78-4a23-891e-55d6d0fbe936"
   },
   "source": [
    "### Restricciones de Interacción\n",
    "\n",
    "Otro **problema** que puede surgir en los modelos que generamos es la **creación de modelos sesgados**. Estos casos pueden ocurrir cuando observamos(por ejemplo, con SHAP) que **nuestro modelo relaciona variables que no deberían estar relacionadas**, generando problemas de **discriminación** en el camino. Por ejemplo, la figura a continuación muestra cómo la raza interactúa más intensamente con la duración de la estancia, el grupo de edad y los antecedentes por año. Estas interacciones desaparecerían una vez que eliminamos la raza. Sin embargo, dada esta observación, se debe prestar especial atención si estas características no tienen **sesgo racial incorporado**. La investigación respalda la necesidad del grupo de edad y los antecedentes por año, lo que deja la duración de la estancia como candidata para un escrutinio.\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915100939.png?raw=true' width=500 />\n",
    "</center>\n",
    "    \n",
    "Cuando la profundidad del árbol es mayor que uno, muchas variables interactúan únicamente con el fin de minimizar la pérdida de entrenamiento, y el árbol de decisión resultante puede capturar una relación espuria (ruido) en lugar de una relación legítima que generalice en diferentes conjuntos de datos. Las **restricciones de interacción de características** permiten a los usuarios decidir qué variables pueden interactuar y cuáles no.\n",
    "\n",
    "<center>\n",
    "<img src='https://xgboost.readthedocs.io/en/stable/_images/feature_interaction_illustration1.svg' width=500 />\n",
    "</center>\n",
    "    \n",
    "Ajustando las restricciones de interacción en el conjunto de datos anterior y evitando la relación entre las variables raciales y el resto de los datos, podemos obtener los siguientes resultados.\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915101644.png?raw=true' width=500 />\n",
    "</center>\n",
    "    \n",
    "Los posibles beneficios incluyen:\n",
    "- **Mejor rendimiento predictivo** al centrarse en las interacciones que funcionan.\n",
    "- Menos ruido en las predicciones -> **mejor generalización**.\n",
    "- Mayor **control para el usuario** sobre lo que el modelo puede ajustar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff408ab-bb4f-4dab-b4cc-1776d7e4fbaa",
   "metadata": {
    "id": "6ff408ab-bb4f-4dab-b4cc-1776d7e4fbaa"
   },
   "source": [
    "## Modelos de Boosting más conocidos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bbab6-81ea-40cf-a58b-f1b4b03c2ffe",
   "metadata": {
    "id": "935bbab6-81ea-40cf-a58b-f1b4b03c2ffe"
   },
   "source": [
    "<center>\n",
    "<img src='https://media1.tenor.com/m/X2uSWnvzEIoAAAAC/simpsons-monkey-knife-fight.gif' width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b43a1c8-23f3-48fb-8c6c-5ae387c39bd5",
   "metadata": {
    "id": "1b43a1c8-23f3-48fb-8c6c-5ae387c39bd5"
   },
   "source": [
    "**XGBoost**, **LightGBM** y **CatBoost** son algoritmos de aprendizaje supervisado que utilizan técnicas de boosting con árboles de decisión como weak learners. Son populares debido a su eficiencia en el manejo de grandes cantidades de datos y su capacidad para manejar diversas formas de datos tabulares.\n",
    "\n",
    "Aunque la idea y el rendimiento son bastante similares entre estos modelos, es crucial entender ciertos aspectos fundamentales que los caracterizan. Por esta razón, a continuación exploraremos algunos de los supuestos clave que definen a cada uno:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7122d8-32c6-45fe-88cd-f181707447e0",
   "metadata": {
    "id": "fd7122d8-32c6-45fe-88cd-f181707447e0"
   },
   "source": [
    "### XGBoost (Xtreme Gradient Boosting)\n",
    "\n",
    "- **Descripción**: XGBoost es un algoritmo de optimización basado en gradientes que utiliza modelos de árboles de decisión ensamblados. Es conocido por su rendimiento y velocidad.\n",
    "- **Características**:\n",
    "  - Manejo eficiente de los valores faltantes (los desvia hacía una de las ramas por defecto).\n",
    "  - Soporte para regularización (L1 y L2) que previene el sobreajuste.\n",
    "  - Alta flexibilidad, permite definir objetivos de optimización personalizados y funciones de evaluación.\n",
    "\n",
    "### LightGBM (Light Gradient Boosting Machine)\n",
    "- **Descripción**: LightGBM es un framework de boosting que utiliza árboles de decisión basados en gradientes. Es similar a XGBoost pero optimizado para ser más rápido y eficiente en memoria.\n",
    "- **Características**:\n",
    "  - Utiliza un algoritmo basado en el histograma para la construcción de árboles, lo que reduce el consumo de memoria y aumenta la velocidad.\n",
    "  - Soporta el crecimiento del árbol por hoja (leaf-wise) que tiende a ser más eficiente para conjuntos de datos grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af102f-3d80-42ee-9552-4fecde094486",
   "metadata": {
    "id": "b4af102f-3d80-42ee-9552-4fecde094486"
   },
   "source": [
    "<center>\n",
    "<img src='https://www.oreilly.com/api/v2/epubs/9781789346411/files/assets/1da830ed-b89a-4437-83bb-373cdfac49fb.png' width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf24b5-8fbd-4757-b121-3d79831e1f99",
   "metadata": {
    "id": "1edf24b5-8fbd-4757-b121-3d79831e1f99"
   },
   "source": [
    "> **Nota**: En el contexto de los algoritmos de árboles de decisión, la **forma en que se expanden los árboles durante el entrenamiento puede tener un impacto significativo en el rendimiento** y la eficiencia del modelo. Existen dos métodos principales para expandir árboles: el **crecimiento por mejor primero** (leaf-wise) y el **crecimiento por profundidad primero** (level-wise). Aunque ambos métodos pueden resultar en la construcción del mismo árbol si se permite que el árbol crezca hasta su máxima profundidad, las diferencias en el orden de expansión del árbol son cruciales, especialmente en escenarios prácticos donde no se suele permitir este crecimiento completo debido a la implementación de criterios de *early-stopping* y métodos de *prunning*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ec550-de8f-4153-916b-190f78d2332e",
   "metadata": {
    "id": "8a3ec550-de8f-4153-916b-190f78d2332e"
   },
   "source": [
    "### CatBoost\n",
    "- **Descripción**: CatBoost es un algoritmo de boosting también basado en árboles de decisión que maneja muy bien las variables categóricas sin necesidad de preprocesamiento extenso.\n",
    "- **Características**:\n",
    "  - Procesamiento automático de variables categóricas.\n",
    "  - Menos sensible a los parámetros de configuración, lo que facilita la puesta en marcha.\n",
    "  - Utiliza simetrización de árboles para reducir el sobreajuste.\n",
    "\n",
    "> **Nota**: La simetrización en CatBoost implica que durante la creación de los árboles, el algoritmo utiliza un **enfoque que garantiza que las decisiones tomadas en las etapas tempranas del árbol no se basen demasiado en características específicas**, ayudando a que el modelo sea más general y menos propenso a ajustarse excesivamente a los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c5284-2b3b-4088-9cb2-40f931c48325",
   "metadata": {
    "id": "5b9c5284-2b3b-4088-9cb2-40f931c48325"
   },
   "source": [
    "### Diferencias Principales\n",
    "- **Manejo de datos**: CatBoost maneja automáticamente las variables categóricas, mientras que en XGBoost y LightGBM se necesita un preprocesamiento previo (a la fecha esto ya fue implementado por los otros algoritmos).\n",
    "- **Velocidad y uso de memoria**: LightGBM es generalmente más rápido y utiliza menos memoria que XGBoost debido a su algoritmo basado en histogramas.CatBoost, aunque eficiente, puede ser más lento que los otros dos debido a su tratamiento de las variables categóricas.\n",
    "- **Facilidad de uso**: CatBoost es menos sensible a la configuración de parámetros, lo que lo hace más fácil de usar para principiantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa1ac7-9e1d-492e-bfac-3a4bde28f9e8",
   "metadata": {
    "id": "0faa1ac7-9e1d-492e-bfac-3a4bde28f9e8"
   },
   "source": [
    "### Ejemplo Básico de Uso con Scikit-Learn\n",
    "A continuación, un pequeño ejemplo de cómo se usa cada uno de estos algoritmos con los wrappers de clasificación en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SQvNAmM6rW0I",
   "metadata": {
    "executionInfo": {
     "elapsed": 25962,
     "status": "ok",
     "timestamp": 1745246773096,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "SQvNAmM6rW0I"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U xgboost\n",
    "!pip install -q -U lightgbm\n",
    "!pip install -q -U catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5d426-1972-48d8-a181-6bcaf16d42bb",
   "metadata": {
    "id": "24f5d426-1972-48d8-a181-6bcaf16d42bb"
   },
   "source": [
    "**XGBoost**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c063f-91cc-4809-aab1-d186693709be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1745246773384,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "450c063f-91cc-4809-aab1-d186693709be",
    "outputId": "61039062-6cd4-4639-e981-af427f92d52b"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdac585-4d68-42e7-a81d-8fb975fea54b",
   "metadata": {
    "id": "9bdac585-4d68-42e7-a81d-8fb975fea54b"
   },
   "source": [
    "**LGBM**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8bc37-b4d2-4122-a1d7-254942a57a91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4018,
     "status": "ok",
     "timestamp": 1745246777403,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "73c8bc37-b4d2-4122-a1d7-254942a57a91",
    "outputId": "28e35adc-5806-49d6-bae3-62c04655be41"
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(verbose=-1) # `verbose=-1` para no mostrar mensajes de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7700e1-5553-48d1-aae8-41e8d733bf67",
   "metadata": {
    "id": "fb7700e1-5553-48d1-aae8-41e8d733bf67"
   },
   "source": [
    "**CatBoost**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d5bee7-ea24-49c9-b393-0d631fd76d94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2001,
     "status": "ok",
     "timestamp": 1745246779413,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "d1d5bee7-ea24-49c9-b393-0d631fd76d94",
    "outputId": "eba241cc-d8e3-40d0-fbab-80d71fece5e4"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(verbose=0)  # `verbose=0` para no mostrar mensajes de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f6cef3-daba-4aed-8cb0-66804ba846e5",
   "metadata": {
    "id": "21f6cef3-daba-4aed-8cb0-66804ba846e5"
   },
   "source": [
    "## Importancia de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71cdc2-5ed7-4aa0-8ee4-5f5845eed8d7",
   "metadata": {
    "id": "fb71cdc2-5ed7-4aa0-8ee4-5f5845eed8d7",
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2025-01/gbm/feature_importances.png?raw=true' width=750 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d50b62-06b9-4a4a-9473-e84854129162",
   "metadata": {
    "id": "10d50b62-06b9-4a4a-9473-e84854129162"
   },
   "source": [
    "Como sabemos, los algoritmos basados en árboles evalúan la relevancia de sus características observando cómo sus variables contribuyen a la división de los nodos. Por esta razón, algoritmos como XGBoost también incluyen métodos para determinar la importancia de las características. Una manera de visualizar la relevancia de las features en XGBoost es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f98003-8fd5-40b6-91e0-84de39ee172f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1745246779942,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "21f98003-8fd5-40b6-91e0-84de39ee172f",
    "outputId": "dead7612-dee4-42c0-9163-59504be264fe"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_importance(model, importance_type='weight'):\n",
    "    # Obtener las importancias de las características\n",
    "    importance_dict = model.get_booster().get_score(importance_type=importance_type)\n",
    "\n",
    "    # Convertir las importancias a un DataFrame para facilitar la visualización\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': importance_dict.keys(),\n",
    "        'Importance': importance_dict.values()\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Graficar las importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.gca().invert_yaxis()  # Invertir el eje y para tener la característica más importante en la parte superior\n",
    "    plt.show()\n",
    "\n",
    "# Asumiendo que X_train, y_train están predefinidos y son adecuados para el modelo\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plot_importance(model, importance_type='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f281aa5-0ef1-4428-b3f0-0f66ecd48f84",
   "metadata": {
    "id": "9f281aa5-0ef1-4428-b3f0-0f66ecd48f84"
   },
   "source": [
    "ok, pero qué sucede si ploteamos otra importancia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b072b62f-85c3-4219-9c86-b2b894f03700",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1745246780208,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "b072b62f-85c3-4219-9c86-b2b894f03700",
    "outputId": "f1aedde2-5dd5-4003-ed01-921acb58c9c2"
   },
   "outputs": [],
   "source": [
    "plot_importance(model, importance_type='gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b6264-3552-4446-90d6-340c56b99c5b",
   "metadata": {
    "id": "1d8b6264-3552-4446-90d6-340c56b99c5b"
   },
   "source": [
    "## ¿Cómo se ve el overfitting en estos modelos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e0cf2-7681-47aa-aecb-cfca21137dd6",
   "metadata": {
    "id": "678e0cf2-7681-47aa-aecb-cfca21137dd6"
   },
   "source": [
    "<center>\n",
    "<img src='https://media.tenor.com/dtWcrQzDfsAAAAAM/the-simpsons-homer-simpson.gif' width=350 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a21e0-d081-402f-9431-a4d71bc7fcf3",
   "metadata": {
    "id": "971a21e0-d081-402f-9431-a4d71bc7fcf3"
   },
   "source": [
    "Debido a que estos modelos se basan en la iteración tras iteración, consisten en una acumulación de estimadores débiles que se van añadiendo en cada ciclo. Este enfoque puede llevar a un considerable punto de controversia, ya que agregar **demasiados estimadores podría provocar un sobreajuste del modelo**.\n",
    "Aunque en la clase anterior observamos cómo identificar sobreajustes mediante curvas de aprendizaje, el **entrenamiento por iteraciones** facilita la visualización por etapas, lo que hace más intuitivos los problemas durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee060f-2511-4136-983f-d7a0793c0c1f",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745250656043,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "b9ee060f-2511-4136-983f-d7a0793c0c1f"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_training(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value, 2) for value in y_pred]\n",
    "\n",
    "    # Evaluamos los modelos\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "    results = model.evals_result()\n",
    "    epochs = len(results['validation_0']['merror'])\n",
    "\n",
    "    # Log Loss Plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(epochs)), y=results['validation_0']['mlogloss'],\n",
    "                             mode='lines', name='Train Log Loss'))\n",
    "    fig.add_trace(go.Scatter(x=list(range(epochs)), y=results['validation_1']['mlogloss'],\n",
    "                             mode='lines', name='Test Log Loss'))\n",
    "    fig.update_layout(title='XGBoost Log Loss Over Epochs',\n",
    "                      template='simple_white',\n",
    "                      xaxis_title='Epoch',\n",
    "                      yaxis_title='Log Loss',\n",
    "                      legend_title='Dataset')\n",
    "    fig.show()\n",
    "\n",
    "    # Classification Error Plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(epochs)), y=results['validation_0']['merror'],\n",
    "                             mode='lines', name='Train Error'))\n",
    "    fig.add_trace(go.Scatter(x=list(range(epochs)), y=results['validation_1']['merror'],\n",
    "                             mode='lines', name='Test Error'))\n",
    "    fig.update_layout(title='XGBoost Classification Error Over Epochs',\n",
    "                      template='simple_white',\n",
    "                      xaxis_title='Epoch',\n",
    "                      yaxis_title='Classification Error',\n",
    "                      legend_title='Dataset')\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7247e3-0c19-487f-88aa-5e8fd2778fb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1745250657171,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "7c7247e3-0c19-487f-88aa-5e8fd2778fb4",
    "outputId": "2c339fc6-998f-4022-9156-2e36b0ea2b9c"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(eval_metric=[\"merror\", \"mlogloss\"])\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e5e54-f0b7-4e73-b1df-7593e1a3c5aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 73,
     "status": "ok",
     "timestamp": 1745250659295,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "282e5e54-f0b7-4e73-b1df-7593e1a3c5aa",
    "outputId": "762679ae-edab-4abe-84e6-864ef1f07e86"
   },
   "outputs": [],
   "source": [
    "plot_training(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ea830-822a-4111-b6c1-fc08e2c454d3",
   "metadata": {
    "id": "920ea830-822a-4111-b6c1-fc08e2c454d3"
   },
   "source": [
    "### Early-Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8fdc7-47d9-4662-a297-a6b55eecd99e",
   "metadata": {
    "id": "29e8fdc7-47d9-4662-a297-a6b55eecd99e"
   },
   "source": [
    "El *early stopping* en XGBoost es una técnica muy útil para **prevenir el sobreajuste** y mejorar la eficiencia del entrenamiento. Este método permite **detener el proceso de entrenamiento de manera anticipada si no se observan mejoras en la métrica de evaluación después de un número específico de rondas consecutivas**.\n",
    "\n",
    "Cuando configuras XGBoost para utilizar *early stopping*, debes **especificar una métrica de evaluación**, como la tasa de error o log-loss para clasificación, y RMSE para regresión. También debes **definir el número de rondas sin mejora** después de las cuales el entrenamiento se detendrá. Este número se conoce como el parámetro `early_stopping_rounds`.\n",
    "\n",
    "Por ejemplo, si estableces `early_stopping_rounds=10`, XGBoost evaluará la métrica seleccionada en un conjunto de validación después de cada ronda de entrenamiento. Si la métrica no mejora después de 10 rondas consecutivas, XGBoost detiene el entrenamiento. Esto no solo previene el sobreajuste, sino que también ahorra tiempo y recursos computacionales.\n",
    "\n",
    "Además, **XGBoost permite continuar el entrenamiento desde la última iteración guardada** si se desea ajustar aún más el modelo. Esto es útil en casos donde las condiciones iniciales de *early stopping* fueron demasiado conservadoras. Utilizar *early stopping* es una práctica recomendada cuando se utilizan modelos avanzados de boosting como XGBoost, especialmente en competencias de modelado predictivo donde el rendimiento óptimo del modelo es crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef241c-d7b8-481c-8610-218babf922dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1745246781225,
     "user": {
      "displayName": "Sebastián Tinoco Pérez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "62ef241c-d7b8-481c-8610-218babf922dd",
    "outputId": "0590681e-96b6-4395-d04d-cdb2930d1a2a"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(early_stopping_rounds=1, eval_metric=[\"merror\", \"mlogloss\"])\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "model.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
    "plot_training(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
