{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6684655-91ae-488a-a543-03d28bbd81b9",
   "metadata": {
    "id": "c6684655-91ae-488a-a543-03d28bbd81b9"
   },
   "source": [
    "# Clase 09: Gradient boosting Machines (GBM)\n",
    "\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "**Objetivos**:\n",
    "- Entender el funcionamiento de **Gradient Boosting Machines** y sus ventajas/desventajas\n",
    "- Aprender a incorporar restricciones de modelo, como monoticidad y restricciones de interacci√≥n\n",
    "- Introducir el concepto de features importances\n",
    "- Reconocer y solucionar overfitting por medio de *Early Stopping*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GPum-gNFvRyN",
   "metadata": {
    "id": "GPum-gNFvRyN"
   },
   "source": [
    "## Contexto: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XQYP55LKv0V-",
   "metadata": {
    "id": "XQYP55LKv0V-"
   },
   "source": [
    "Para esta clase, necesitaremos tener un m√≠nimo entendimiento del funcionamiento de **√Årboles de Decisi√≥n** pues servir√°n como la unidad b√°sica sobre la cual se construye el algoritmo **Gradient Boosting**.\n",
    "\n",
    "Pueden encontrar una buena explicaci√≥n a este algoritmo en el siguiente enlace:\n",
    "[Medium](https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e/).\n",
    "\n",
    "<center>\n",
    "<img src='https://towardsdatascience.com/wp-content/uploads/2024/08/1-kVbTztm62HThXb6oy8arg-768x470.png' width=500 />\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6715d-90a9-4edc-a64e-3554fd759868",
   "metadata": {
    "id": "0ab6715d-90a9-4edc-a64e-3554fd759868"
   },
   "source": [
    "## Gradient boosting Machines (GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e3ccc-bc09-45f0-bfe8-1c9934f802a0",
   "metadata": {
    "id": "f97e3ccc-bc09-45f0-bfe8-1c9934f802a0",
    "tags": []
   },
   "source": [
    "Las GBM son algoritmos que se volvieron muy populares con la aparici√≥n de XGBoost en el 2016 para las tareas de clasificaci√≥n y regresi√≥n. En t√©rminos de funcionamiento, estos algoritmos utilizan m√∫ltiples **weak-learners** con los que a trav√©s de m√∫ltiples iteraciones corrigen los errores que presentan sus regresores/clasificadores en etapas anteriores.\n",
    "\n",
    "\t\t‚ùì Pregunta: ¬øQu√© caracter√≠sticas tienen los \"weak-learners\"?\n",
    "\n",
    "1. El objetivo no es crear clasificadores potentes con ellos, si no como su nombre lo dice queremos generar \"aprendices d√©biles\" que se potencian con otros.\n",
    "2. En general se utilizan como weak learners √°rboles de decisi√≥n por su simpleza y versatilidad que nos ofrecen para regresi√≥n y clasificaci√≥n.\n",
    "3. Pueden ser entrenados con una muestra de la totalidad para obtener entrenamientos r√°pidos sobre ellos.\n",
    "4. Su uso en un algoritmo implica la comprensi√≥n de estos elementos, ya que se heredan sus problemas en una estructura m√°s compleja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef55d0-679c-40f4-93fc-d861ad65f08d",
   "metadata": {
    "id": "74ef55d0-679c-40f4-93fc-d861ad65f08d"
   },
   "source": [
    "‚ùì Pregunta: ¬øQu√© problemas ten√≠an los √°rboles de decisi√≥n?\n",
    "\n",
    "<center>\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1702/1*KbRVJC5B0EgO8YAyeCrLkA.png' width=500 />\n",
    "</center>\n",
    "\n",
    "Recordar que los √°rboles de decisi√≥n son elementos que tienen una alta facilidad de sobre-ajustarse si aumentamos la profundidad o n√∫mero de hijos por nodo. Esto puede causar problemas en un algoritmo de GBM y por ello, este debe ser un valor que debemos controlar de forma directa o indirectamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a42f5ff-80b5-490a-924e-5fef5dfc8102",
   "metadata": {
    "id": "6a42f5ff-80b5-490a-924e-5fef5dfc8102"
   },
   "source": [
    "Adentr√°ndonos m√°s en detalles en el algoritmo de gradient boosting, estos forman parte de los algoritmos de **ensemble**, es decir, algoritmos que **combinan m√∫ltiples modelos para el proceso de predicci√≥n**. Una forma de esto es a trav√©s de **modelos aditivos**, donde entrenando m√∫ltiples algoritmos simples de forma independiente, utilizamos la suma de sus salidas para generar un modelo m√°s potente para la regresi√≥n/clasificaci√≥n:\n",
    "$$F_M(x) = f_1(x)+...+f_M(x)=\\sum_{m=1}^M f_m(x)$$\n",
    "\n",
    "<center>\n",
    "<img src='https://machinelearningmastery.com/wp-content/uploads/2020/07/Example-of-Combining-Decision-Boundaries-Using-an-Ensemble.png' width=500 />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e28f15-95f2-4125-87f6-bdfa13312dc2",
   "metadata": {
    "id": "a6e28f15-95f2-4125-87f6-bdfa13312dc2"
   },
   "source": [
    "### GBM: Regresi√≥n\n",
    "\n",
    "La idea detr√°s de los GBM es realizar **descenso por gradiente en el espacio de funciones** para encontrar una funci√≥n $\\hat{f}(x)$ que minimice lo m√°s posible una funci√≥n de p√©rdida $L$. Formalmente, buscamos:\n",
    "\n",
    "$$\n",
    "\\hat{f} = \\arg \\min_f \\sum_{i=1}^N L(y_i, f(x_i))\n",
    "$$\n",
    "\n",
    "donde en el caso de la regresi√≥n, es usual fijar $\\mathcal{L} = \\frac{1}{2} (y - f(x_i))^2$.\n",
    "\n",
    "> **‚ùì Pregunta:** ¬øQu√© funci√≥n de p√©rdida es esta? ¬øDe donde viene el 1/2?\n",
    "\n",
    "El algoritmo se inicializa con una predicci√≥n **base y constante** $f_0(x)$ que minimiza la funci√≥n de p√©rdida con respecto a los datos:\n",
    "\n",
    "$$\\arg \\min_F \\sum_{i=1}^N L(y_i, F(x_i))$$\n",
    "\n",
    "donde en la pr√°ctica se puede demostrar que este valor es el **promedio** de los valores $y$.\n",
    "\n",
    "Luego, agregaremos $m$ √°rboles de decisi√≥n o weak learners (donde $m$ es un par√°metro definido por el usuario) repitiendo los siguientes pasos:\n",
    "\n",
    "- Primero, calcularemos los pseudo residuos $r_{im}$ con respecto al modelo actual:\n",
    "\n",
    "$$\n",
    "r_{im} = -\\left[\\dfrac{\\partial \\mathcal{L}(y_i, f(x_i))}{\\partial f(x_i)}\\right]_{f = f_{m-1}}\n",
    "$$\n",
    "\n",
    "Remplazando el error cuadr√°tico medio en la expresi√≥n anterior, obtenemos\n",
    "\n",
    "$$\n",
    "r_{im} = y - f(x_i)\n",
    "$$\n",
    "\n",
    "- Luego, se entrena el weak learner para predecir los residuos $r_{im}$:\n",
    "\n",
    "$$\\arg \\min_F \\sum_{i=1}^N L(r_{im}, F(x_i))$$\n",
    "$$F_m = \\arg \\min_F \\sum_{i=1}^N \\frac{1}{2}(r_{im}-F(x_i))^2$$\n",
    "\n",
    "Notar que $F(x_i)$ es un weak learner que se entrena en cada una de las iteraciones de nuestro algoritmo de GBM.\n",
    "\n",
    "- Por √∫ltimo, se agrega el weak learner $F_m$ al modelo:\n",
    "\n",
    "$$f_m = f_{m-1} + \\nu F_{m}$$\n",
    "Donde $\\nu$ se define como el largo del paso en la correcci√≥n. Visto de otra forma, podemos interpretar este valor como una **tasa de aprendizaje** que podremos fijar o optimizar en la funci√≥n, se√±alando cu√°nta correcci√≥n aplicaremos del proceso anterior en el proceso actual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106ab54-77ce-4806-994c-7a4dc0020537",
   "metadata": {
    "id": "0106ab54-77ce-4806-994c-7a4dc0020537"
   },
   "source": [
    "Finalmente el algoritmo se ver√≠a como:\n",
    "\n",
    "![](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/21_Ensamblaje/Pasted%20image%2020230527153450.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceffe09-45a9-4009-9131-c5f0c4a260b6",
   "metadata": {
    "id": "7ceffe09-45a9-4009-9131-c5f0c4a260b6"
   },
   "source": [
    "### GBM: Clasificaci√≥n\n",
    "\n",
    "\t\t‚ùì Pregunta: ¬øDeber√≠a ser diferente la clasificaci√≥n a la regresi√≥n?\n",
    "\n",
    "La respuesta directa a esto es \"s√≠\". Los cambios que presenta la clasificaci√≥n respecto a la regresi√≥n se dan netamente en la **elecci√≥n de la funci√≥n de p√©rdida** ($\\mathcal{L}$) y algunas consideraciones que se toman para las salidas de los nodos.\n",
    "\n",
    "Respecto a las funciones de p√©rdida, esto se debe a c√≥mo se define el problema de clasificaci√≥n supervisada, donde nuestro inter√©s no ser√° la reducci√≥n del error que tenemos del valor estimado respecto a un valor continuo. En su reemplazo son utilizadas **funciones de p√©rdida para clasificaci√≥n**, quienes tienen como principal objetivo disminuir el error al predecir una etiqueta. Una de las funciones de p√©rdidas m√°s conocidas/utilizadas para el problema de clasificaci√≥n se encuentra la **logistic-loss**/**cross-entropy**:\n",
    "$$\\mathcal{L}_i = -(y_i log(p_i) + (1-y_i)log(1-p_i))$$\n",
    "\t‚ùì Pregunta:  ¬øC√≥mo podemos interpretar esta funci√≥n de p√©rdida?\n",
    "\n",
    "Donde su derivada es:\n",
    "$$\\dfrac{\\partial \\mathcal{L}_i}{\\partial \\hat{y}}=p_i-y_i$$\n",
    "El segundo punto a considerar es que la estimaci√≥n de $y$ ser√° igual a:\n",
    "$$\\hat{y}=log(odds)=log(\\dfrac{p}{1-p})$$\n",
    "\t\t‚ùì Pregunta:  ¬øPor qu√© usamos odds?\n",
    "\n",
    "B√°sicamente debido a que los modelos de clasificaci√≥n con GBM son modelos de regresi√≥n, por lo que sus salidas no pueden ser consideradas como probabilidades, sino como scores que debemos transformar y normalizar.\n",
    "\n",
    "Finalmente para calcular las probabilidades de las salidas de nuestro GBM tendremos que calcular la funci√≥n softmax de $\\hat{y}$, la que viene dada por:\n",
    "$$\\text{softmax}(\\hat{y})=\\dfrac{1}{1+e^{-\\hat{y}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a3d69-63da-47bc-8468-c5517cec4274",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1745247279861,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "6a9a3d69-63da-47bc-8468-c5517cec4274",
    "outputId": "50774028-dcf8-4212-cfaf-e6d550ba60e3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Ejemplo de funci√≥n de p√©rdida: Caso con score alejado de la etiqueta\n",
    "p = 0.99 # salida del modelo\n",
    "y = 0 # etiqueta real\n",
    "-(y*math.log(p)+(1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d66a4-9905-40d6-9501-4b58f59d469c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745247281680,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "d63d66a4-9905-40d6-9501-4b58f59d469c",
    "outputId": "f5d2ebcb-2519-41e1-b110-8c9d720d085c"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de funci√≥n de p√©rdida: Caso con score cercano a la etiqueta\n",
    "p = 0.9 # salida del modelo\n",
    "y = 1 # etiqueta real\n",
    "-(y* math.log(p) + (1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36f6af-cde4-4944-b2c8-07b2df783e68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745247283408,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "2f36f6af-cde4-4944-b2c8-07b2df783e68",
    "outputId": "4d301ad8-e064-4e3a-ee21-2496bcac6f8f"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de funci√≥n de p√©rdida: Caso con score cercano a la etiqueta (etiqueta 0)\n",
    "p = 0.3 # salida del modelo\n",
    "y = 0 # etiqueta real\n",
    "-(y* math.log(p) + (1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef24a7",
   "metadata": {
    "id": "00ef24a7"
   },
   "source": [
    "Podemos graficar el comportamiento de la loss function para diferentes valores de $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35087c5c-1382-4698-bfc5-b28334ea2811",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1745247297858,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "35087c5c-1382-4698-bfc5-b28334ea2811",
    "outputId": "64e8ab04-459c-4272-cd30-3cdcd8b75a99"
   },
   "outputs": [],
   "source": [
    "lis1, prob = [], []\n",
    "for p in range(1, 10):\n",
    "    y = 1\n",
    "    lis1.append(-(y* math.log(p/10) + (1-y)*math.log(1-p/10)))\n",
    "    prob.append(p/10)\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "px.line(x=prob, y=lis1, title=f'Loss with different predictions: y={y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753bb7e-4280-47c2-bae5-c3990a0ad0b9",
   "metadata": {
    "id": "4753bb7e-4280-47c2-bae5-c3990a0ad0b9"
   },
   "source": [
    "### Retrospectiva\n",
    "\n",
    "\t\t‚ùì Pregunta: ¬øPero c√≥mo podr√≠amos simplificar estas definiciones que acabamos de dar?\n",
    "\n",
    "Si visualizamos GBM en un juego de golf, tendr√≠amos un jugador que comienza con un tiro $f_0$, el cual a medida que va realizando cada tiro va aplicando una **correcci√≥n** $\\Delta_m$ en cada una de sus jugadas. Con esto, si el jugador corrige sus errores a trav√©s de una funci√≥n MSE, tendremos que el jugador ir√° optimizando esta funci√≥n hasta meter la pelota en el agujero (m√≠nimo de la funci√≥n de p√©rdida).\n",
    "\n",
    "$\\text{MSE LOSS} = \\mathcal{L}(y, F_M(X))=\\dfrac{1}{N} \\sum_{i=1}^N (y_i - F_M(x_i))^2$\n",
    "\n",
    "<center>\n",
    "<img src='https://explained.ai/gradient-boosting/images/golf-MSE.png' width=500 />\n",
    "</center>\n",
    "\n",
    "\t\t‚ùì ¬øC√≥mo podr√≠a el golfista corregir de mejor forma sus tiros? ¬øExiste alguna forma?.\n",
    "\n",
    "<center>\n",
    "<img src='https://explained.ai/gradient-boosting/images/1d-vectors.png' width=500 />\n",
    "</center>\n",
    "\n",
    "Como vimos durante el entrenamiento de GBM, este proceso considera un $\\nu$ conocido como shrinkage rate o **learning rate**. Este valor nos permitir√° realizar correcciones en las actualizaciones de nuestras funciones $f_m$, ya que podr√≠a darse el caso que el gradiente obtenido durante la optimizaci√≥n sea muy alto y nos permita obtener un m√≠nimo adecuado para el problema. De esta forma:\n",
    "$$f_m(x) = f_{m-1}(x) + \\nu F_m(x)$$\n",
    "Otra forma podr√≠a ser **cambiando la funci√≥n de p√©rdida** que definimos en el problema. Dependiendo si es un problema de clasificaci√≥n o regresi√≥n existen m√∫ltiples funciones de p√©rdida con diferentes interpretaciones que pueden mejorar significativamente el problema que deseamos resolver. Algunas de estas son:\n",
    "\n",
    "| Nombre         | Loss                         | Derivada           |  \n",
    "| -------------- | ---------------------------- | ------------------ |\n",
    "| Squared Error | $\\dfrac{1}{2}(y_i-f(x_i))^2$ | $y_i -f(x_i)$      |\n",
    "| Absolute Error | $y_i - f(x_i)$               | $sgn(y_i -f(x_i))$ |\n",
    "| Binary Logloss                | $log(1+e^{-y_if_i})$  | $y-\\sigma(2f(x_i))$    |\n",
    "\n",
    "En tercer lugar tenemos la adici√≥n de un **regularizador** a nuestra funci√≥n de p√©rdida, de esta forma podremos disminuir el sobreajuste de nuestro modelo, generando peque√±as modificaciones en los √°rboles que vamos generando.\n",
    "$$\\mathcal{L}(f) = \\sum_{i=1}^Nl(y_i,f(x))+\\Omega(f))$$\n",
    "donde $\\Omega$:\n",
    "$$\\Omega(f)= \\gamma J+\\dfrac{1}{2}\\lambda \\sum_{j=1}^J w^2_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b7e14-270b-4467-924a-10189151b12d",
   "metadata": {
    "id": "d41b7e14-270b-4467-924a-10189151b12d"
   },
   "source": [
    "### Ventajas & Desventajas de GBM\n",
    "\n",
    "**Ventajas**:\n",
    "\n",
    "- Capacidad para manejar tanto problemas de regresi√≥n como de clasificaci√≥n.\n",
    "- Puede capturar relaciones no lineales y caracter√≠sticas complejas en los datos.\n",
    "- Adaptabilidad a diferentes tipos de predictores, incluyendo variables num√©ricas y categ√≥ricas.\n",
    "- Robustez ante valores at√≠picos y datos ruidosos.\n",
    "- Capacidad para manejar grandes conjuntos de datos y escalabilidad.\n",
    "- Flexibilidad en la elecci√≥n de funciones de p√©rdida y m√©tricas de evaluaci√≥n.\n",
    "- Capacidad para manejar caracter√≠sticas faltantes y utilizar todas las observaciones disponibles.\n",
    "- Puede generar importancia de variables para ayudar en la interpretaci√≥n y selecci√≥n de caracter√≠sticas.\n",
    "\n",
    "**Desventajas**:\n",
    "\n",
    "- Mayor complejidad y tiempo de entrenamiento en comparaci√≥n con algoritmos m√°s simples.\n",
    "- Puede ser propenso a sobreajuste si no se controlan los hiperpar√°metros adecuadamente.\n",
    "- Requiere una configuraci√≥n cuidadosa de los hiperpar√°metros para obtener el mejor rendimiento.\n",
    "- Interpretaci√≥n m√°s dif√≠cil debido a la naturaleza de combinaci√≥n de m√∫ltiples √°rboles.\n",
    "- Sensible a datos desequilibrados, donde las clases minoritarias pueden no recibir suficiente atenci√≥n.\n",
    "- La importancia de las variables puede verse sesgada hacia las variables num√©ricas o con mayor cardinalidad.\n",
    "- No es adecuado para problemas con alta dimensionalidad o con muchos predictores.\n",
    "- Mayor consumo de recursos computacionales en comparaci√≥n con algoritmos m√°s simples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac118e-ce1c-438a-891d-ea51090f5278",
   "metadata": {
    "id": "1bac118e-ce1c-438a-891d-ea51090f5278"
   },
   "source": [
    "## Ejemplo a Mano ü§®\n",
    "\n",
    "Como ya sabemos como funciona por detr√°s un algoritmo simple de Gradient Boosting, veamos un ejemplo para ejecutar un peque√±o GBM a mano. Para esto consideremos los siguientes datos:\n",
    "\n",
    "| index | $feature_1$ | $feature_2$ | y     |  \n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1     | 1.12  | 1.4   | 1     |   \n",
    "| 2     | 5.45  | 3.1   | 0     |   \n",
    "| 3     | 3.54  | 1.2   | 1     |   \n",
    "\n",
    "Considerando los datos construir un modelo de GBM con 2 weak learners y un $\\nu$ igual a $0.1$.\n",
    "\n",
    "**Desarrollo:**\n",
    "Con la tabla anterior, comenzamos realizando nuestra primera iteraci√≥n (m=1), para ello se considera que todos los datos se encuentran en el mismo nodo. De esta forma, calcularemos nuestra predicci√≥n base a trav√©s de odds y los log(odds)::\n",
    "\n",
    "$$odds = \\dfrac{\\text{cantidad de casos exitosos (y=1)}}{\\text{cantidad de casos fallidos (y=0)}}$$\n",
    "\n",
    "$$odds = 2/1 = 2$$\n",
    "Luego para los $log_n(odds)$:\n",
    "$$log_n(odds)=log_n(2)=0.693$$\n",
    "Notar que debido a que todos los valores est√°n en el mismo nodo, todos los valores poseer√°n el mismo $log_n(odds)$, y por lo tanto, la misma predicci√≥n inicial:\n",
    "$$p=\\dfrac{1}{1+e^{-\\hat{y}}}=\\dfrac{1}{1+e^{-0.693}}=0.67$$\n",
    "Con este valor, calculamos los pseudo residuos para cada observaci√≥n:\n",
    "$$r_{10}=y-p_1=1-0.67=0.33$$\n",
    "$$r_{20}=y-p_2=0-0.67=-0.67$$\n",
    "$$r_{30}=y-p_3=1-0.67=0.33$$\n",
    "Tenemos nuestro primer estimador calculado, ahora el siguiente paso es generar un nuevo √°rbol. Para esto vamos a generar un √°rbol donde el nodo padre tiene la regla $feature_2>2$.\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/21_Ensamblaje/Flowchart%20Template.jpg?raw=true' width=400 />\n",
    "</center>\n",
    "    \n",
    "Considerando los resultados obtenidos en la iteraci√≥n anterior, podemos calcular las predicciones del √°rbol $\\gamma_i$:\n",
    "\n",
    "$$\\gamma_{i}=\\dfrac{\\sum_i y_i-p_{i}}{\\sum_i p_{i}(1-p_{i})}$$\n",
    "\n",
    "donde el √≠ndice $i \\in (1, 2)$ representa la hoja del √°rbol.\n",
    "\n",
    "Reemplazando cada valor:\n",
    "$$\\gamma_{1}=\\dfrac{0.33+0.33}{2*(0.67(1-0.67))}=1.49$$\n",
    "\n",
    "$$\\gamma_{2}=\\dfrac{-0.67}{0.67(1-0.67)}=-3.03$$\n",
    "\n",
    "Finalmente, con los valores obtenidos calculamos el $f_m$ resultante de las 2 iteraciones para cada una de las hojas:\n",
    "\n",
    "$$f_1(x)=f_{0}(x)+\\nu \\cdot \\gamma_{1}$$\n",
    "\n",
    "Reemplazamos para cada uno de los valores que tenemos por fila:\n",
    "\n",
    "$$\\hat{y_1}=f_1(x_1)=0.69+0.1*1.49=0.839$$\n",
    "$$\\hat{y_2}=f_1(x_2)=0.69+0.1*(-3.03)=0.387$$\n",
    "$$\\hat{y_3}=f_1(x_3)=0.69+0.1*1.49=0.839$$\n",
    "\n",
    "Como podemos ver, estos valores solamente representan un score que no nos indica la probabilidad, para esto aplicamos una softmax para normalizar las salidas:\n",
    "\n",
    "$$p_1=\\dfrac{1}{1+e^{-\\hat{y}}}=0.698$$\n",
    "$$p_2=0.595$$\n",
    "$$p_3=0.698$$\n",
    "Finalmente los residuales de cada una de las salidas es:\n",
    "\n",
    "$$r_{12} = r_{32} = 0.302$$\n",
    "$$r_{22} = -0.595$$\n",
    "\n",
    "Noten como el error de cada observaci√≥n disminuye :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03c8ee-e596-49dc-991d-491c3f944c32",
   "metadata": {
    "id": "ca03c8ee-e596-49dc-991d-491c3f944c32"
   },
   "source": [
    "## Ejemplo con C√≥digo üßê\n",
    "\n",
    "Como ya comprendimos que es un algoritmo de GBM de forma te√≥rica y como este se calcula a mano, el √∫ltimo paso es visualizar como funciona el entrenamiento a trav√©s de c√≥digo. Para realizar este ejercicio modificaremos desde 0 el algoritmo de GBM utilizando los `DecisionTreeClassifier` de scikit-learn.\n",
    "\n",
    "**Objetivo:** Codificar cada una de las partes que contiene un algoritmo de GBM en una clase de python.\n",
    "\n",
    "<center>\n",
    "<img src='https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Gradient-boosting-LightGBM-vs-XGBoost.png?resize=591%2C431&ssl=1' width=500 />\n",
    "</center>\n",
    "    \n",
    "Comenzamos definiendo el inicializador de nuestra clase `GBMClassifier`, para esto definimos los siguientes valores:\n",
    "- `n_estimators` = N√∫mero de √°rboles que vamos a entrenar en el entrenamiento secuencial e iterativo.\n",
    "- `max_depth` = M√°xima profundidad de los arboles que vamos a entrenar.\n",
    "- `lr` = Tasa de aprendizaje de nuestro GBM.\n",
    "- `loss` = Definimos la funci√≥n de p√©rdida que vamos a utilizar para el problema.\n",
    "\n",
    "```python\n",
    "class GBMClassifier:\n",
    "    def __init__(self, n_trees, learning_rate=0.1, max_depth=1, loss_function=None):\n",
    "        self.n_trees=n_trees\n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_depth=max_depth\n",
    "        self.loss_function = loss_function\n",
    "```\n",
    "\n",
    "En segundo lugar debemos definir el m√©todo que entrenar√° a nuestro modelo GBM. Para la construcci√≥n consideramos los siguientes pasos:  \n",
    "1. Generamos una **lista donde almacenaremos nuestros √°rboles** en cada una de las iteraciones, para esto generamos un atributo donde ser√° almacenado.\n",
    "2. En segundo lugar es necesario **obtener la predicci√≥n base** de la primera etapa de nuestro modelo utilizando la funci√≥n que calcula los gradientes de la funci√≥n de p√©rdida.\n",
    "3. Generamos un **loop en el que entrenaremos los $n$ √°rbole**s que definimos al inicializar la GBM y realizamos los siguientes pasos.\n",
    "\t1. **Obtener los gradientes ($\\gamma$)** de cada una de las etapas.\n",
    "\t2. **Entrenamos los √°rboles** con los gradientes obtenidos en 1.\n",
    "\t3. **Actualizamos las predicciones** realizadas por nuestros √°rboles.\n",
    "\t4. **Obtenemos $F_M$ utilizando las predicciones** y el learning rate.\n",
    "\t5. **Guardamos el √°rbol entrenado** en esta etapa en nuestra lista de √°rboles.\n",
    "\n",
    "```python\n",
    "def fit(self):\n",
    "    self.trees = []\n",
    "    self.base_prediction = self._argmin_fun(y=y)\n",
    "    current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
    "    for _ in range(self.n_trees):\n",
    "        pseudo_residuals = self.loss_function.negative_gradient(y, current_predictions)\n",
    "        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "        tree.fit(X, pseudo_residuals)\n",
    "        self._update_terminal_nodes(tree, X, y, current_predictions)\n",
    "        current_predictions += self.learning_rate * tree.predict(X)\n",
    "        self.trees.append(tree)\n",
    "```\n",
    "\n",
    "Para calcular los gradientes de nuestra funci√≥n utilizaremos una funci√≥n de minimizaci√≥n, de esta forma no nos preocupamos de las derivadas. En la definici√≥n de este m√©todo tendremos dos casos: el caso base, cuando no se tiene un $\\hat{y}$ y un segundo cuando se tiene las predicciones de un estimador.\n",
    "\n",
    "```python\n",
    "def _argmin_fun(self, y, y_hat=None):\n",
    "    if np.array(y_hat).all() == None:\n",
    "        fun = lambda c: self.loss_function.loss(y, c)\n",
    "    else:\n",
    "        fun = lambda c: self.loss_function.loss(y, y_hat+c)\n",
    "    c0 = y.mean()\n",
    "    return minimize(fun=fun, x0=c0).x[0]\n",
    "```\n",
    "\n",
    "Finalmente tenemos la actualizaci√≥n de los nodos del √°rbol entrenado, para esto comenzamos obtenido las `ids` de los nodos que contienen las predicciones y los recorremos en un loop:\n",
    "\n",
    "1. Dentro del loop encontramos todos los valores que est√©n la misma hoja y seleccionamos los `y` e $\\hat{y}$ que poseen estos valores.\n",
    "2. Calculamos el $\\gamma$ de estos valores y los reemplazamos en los √°rboles.\n",
    "\n",
    "```python\n",
    "def _update_terminal_nodes(self, tree, X, y, current_predictions):\n",
    "    leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
    "    leaf_node_for_each_sample = tree.apply(X)\n",
    "    for leaf in leaf_nodes:\n",
    "        samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
    "        y_in_leaf = y.take(samples_in_this_leaf, axis=0)\n",
    "        preds_in_leaf = current_predictions.take(samples_in_this_leaf, axis=0)\n",
    "        val = self._argmin_fun(y=y_in_leaf, y_hat=preds_in_leaf)\n",
    "        tree.tree_.value[leaf, 0, 0] = val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bc227-ecef-40fa-a0c4-e481915fe7b6",
   "metadata": {
    "id": "620bc227-ecef-40fa-a0c4-e481915fe7b6"
   },
   "source": [
    "### Definamos la clase que creamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917a2ec-2322-4c76-9e06-1d92a6606339",
   "metadata": {
    "executionInfo": {
     "elapsed": 1675,
     "status": "ok",
     "timestamp": 1745246740804,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "4917a2ec-2322-4c76-9e06-1d92a6606339"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class GradientBoostingMachine():\n",
    "    def __init__(self, n_trees, learning_rate=0.1, max_depth=1, loss_function=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Funci√≥n para entrenar GBM'''\n",
    "        self.trees = []\n",
    "        self.base_prediction = self._argmin_fun(y=y)  # obtener predicci√≥n base\n",
    "        current_predictions = self.base_prediction * np.ones_like(y)\n",
    "\n",
    "        for _ in range(self.n_trees):  # iterar para cada √°rbol a agregar\n",
    "            pseudo_residuals = self.loss_function.negative_gradient(y, current_predictions)\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, pseudo_residuals)\n",
    "            self._update_terminal_nodes(tree, X, y, current_predictions)\n",
    "            current_predictions += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _argmin_fun(self, y, y_hat=None):\n",
    "        '''Encuentra el mejor valor constante que minimiza la p√©rdida'''\n",
    "        if y_hat is None:\n",
    "            fun = lambda c: self.loss_function.loss(y, c * np.ones_like(y))\n",
    "        else:\n",
    "            fun = lambda c: self.loss_function.loss(y, y_hat + c)\n",
    "        c0 = np.array([y.mean()])\n",
    "        return minimize(fun=fun, x0=c0).x[0]\n",
    "\n",
    "    def _update_terminal_nodes(self, tree, X, y, current_predictions):\n",
    "        '''Actualiza los nodos terminales del √°rbol'''\n",
    "        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
    "        leaf_node_for_each_sample = tree.apply(X)\n",
    "        for leaf in leaf_nodes:\n",
    "            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
    "            if len(samples_in_this_leaf) == 0:\n",
    "                continue\n",
    "            y_in_leaf = y[samples_in_this_leaf]\n",
    "            preds_in_leaf = current_predictions[samples_in_this_leaf]\n",
    "            val = self._argmin_fun(y=y_in_leaf, y_hat=preds_in_leaf)\n",
    "            tree.tree_.value[leaf, 0, 0] = val\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''Genera predicci√≥n para un nuevo conjunto de datos X'''\n",
    "        return (self.base_prediction +\n",
    "                self.learning_rate *\n",
    "                np.sum([tree.predict(X) for tree in self.trees], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4732efd-d1e8-4fc1-8d20-71810335d08a",
   "metadata": {
    "id": "d4732efd-d1e8-4fc1-8d20-71810335d08a"
   },
   "source": [
    "#### Probemos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed80534-1480-4ff3-9bdf-e87931701bba",
   "metadata": {
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1745246740895,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "fed80534-1480-4ff3-9bdf-e87931701bba"
   },
   "outputs": [],
   "source": [
    "import numpy.random as rng\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# test data\n",
    "def make_test_data(n, noise_scale):\n",
    "    x = np.linspace(0, 10, 500).reshape(-1,1)\n",
    "    y = (np.where(x < 5, x, 5) + rng.normal(0, noise_scale, size=x.shape)).ravel()\n",
    "    return x, y\n",
    "\n",
    "# print model loss scores\n",
    "def print_model_loss_scores(obj, y, preds, sk_preds):\n",
    "    print(f'From Scratch Loss = {obj.loss(y, pred):0.4}')\n",
    "    print(f'Scikit-Learn Loss = {obj.loss(y, sk_pred):0.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4578ab0-647c-4feb-ba15-5551f209c185",
   "metadata": {
    "id": "c4578ab0-647c-4feb-ba15-5551f209c185"
   },
   "source": [
    "Comenzamos generando unos datos con cierto grado de relaci√≥n para generar una regresi√≥n simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be632ae1-a6f8-414e-8a8a-0dad43952404",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1745246741023,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "be632ae1-a6f8-414e-8a8a-0dad43952404",
    "outputId": "39d721e4-4b18-49f8-9c73-a12ad21f38ed"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "x, y = make_test_data(500, 0.4)\n",
    "px.scatter(x, y, title='Datos de Prueba',template='simple_white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d222aa-b357-4842-954f-ca0dda66a48c",
   "metadata": {
    "id": "61d222aa-b357-4842-954f-ca0dda66a48c"
   },
   "source": [
    "Donde nuestra funci√≥n de perdida viene dada por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbad0e-b303-4994-96f9-aef0afd3cfee",
   "metadata": {
    "executionInfo": {
     "elapsed": 710,
     "status": "ok",
     "timestamp": 1745246741734,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "19dbad0e-b303-4994-96f9-aef0afd3cfee"
   },
   "outputs": [],
   "source": [
    "class MSELoss():\n",
    "    '''User-Defined Squared Error Loss'''\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y, preds):\n",
    "        return np.mean((y - preds)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_gradient(y, preds):\n",
    "        return y - preds\n",
    "\n",
    "\n",
    "gbm = GradientBoostingMachine(n_trees=100,\n",
    "                              learning_rate=0.5,\n",
    "                              max_depth=1,\n",
    "                              loss_function=MSELoss(),\n",
    "                              )\n",
    "gbm.fit(x, y)\n",
    "pred = gbm.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03fee02-f237-42be-91da-e9c7c8b7a24d",
   "metadata": {
    "id": "c03fee02-f237-42be-91da-e9c7c8b7a24d"
   },
   "source": [
    "Con los datos generamos un loop para visualizar como es el impacto que los estimadores dentro de la regresi√≥n, con esto obtenemos los siguientes gr√°ficos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1416df-3aec-4512-bf19-773263b52bf3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5335,
     "status": "ok",
     "timestamp": 1745246747067,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "5a1416df-3aec-4512-bf19-773263b52bf3",
    "outputId": "9921393b-0cfc-472b-b61b-e097caa43e34"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regresion(x, y, y_pred, name=\"Datos con regresi√≥n\", color='r'):\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, pred, color='r')\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(1, 11):\n",
    "    gbm = GradientBoostingMachine(n_trees=i,\n",
    "                                  learning_rate=0.5,\n",
    "                                  max_depth=1,\n",
    "                                  loss_function=MSELoss()\n",
    "                                 )\n",
    "    gbm.fit(x, y)\n",
    "    pred = gbm.predict(x)\n",
    "    name_plot = f\"Datos con regresi√≥n con {i} estimadores\"\n",
    "    plot_regresion(x, y, pred, name=name_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d4dcfa-c425-4e73-9b0c-1c2a0620665a",
   "metadata": {
    "id": "f5d4dcfa-c425-4e73-9b0c-1c2a0620665a"
   },
   "source": [
    "Comprobemos si esto tiene sentido con el clasificador de boosting que viene en sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15c0b5-3418-4f66-b7b3-8292652f73dc",
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1745246747107,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "7d15c0b5-3418-4f66-b7b3-8292652f73dc"
   },
   "outputs": [],
   "source": [
    "sk_gbm = GradientBoostingRegressor(n_estimators=10,\n",
    "                                   learning_rate=0.5,\n",
    "                                   max_depth=1)\n",
    "sk_gbm.fit(x, y)\n",
    "sk_pred = sk_gbm.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c525d-d947-4cf1-bdb8-aea91ecbc855",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745246747132,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "d13c525d-d947-4cf1-bdb8-aea91ecbc855",
    "outputId": "060bce6a-680c-4b1d-963e-f6f3d7331232"
   },
   "outputs": [],
   "source": [
    "print_model_loss_scores(MSELoss(), y, pred, sk_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1649bf-b1be-47bd-b14a-b78cae35e035",
   "metadata": {
    "id": "2d1649bf-b1be-47bd-b14a-b78cae35e035"
   },
   "source": [
    "De los resultados se observa que a medida que aumentamos el n√∫mero de estimadores es posible generar una regresi√≥n m√°s similar a la tendencia que se√±alan los datos, por lo que la adici√≥n de weak-learners a nuestro modelo fue efectiva!\n",
    "\n",
    "## Retrospectiva\n",
    "\n",
    "\t\t‚ùì Pregunta: ¬øa√±adir muchos estimadores que puede provocar?\n",
    "\t\t‚ùì Pregunta: Sabemos que los √°rboles son interpretables, ¬øque sucede con los algoritmos de ensemble?\n",
    "\t\t‚ùì Pregunta: ¬øCom√≥ podemos medir el sobre ajuste de estos modelos si son multiples √°rboles?\n",
    "\t\t‚ùì Pregunta: ¬øA nivel general son buenos estimadores los GBM?\n",
    "\n",
    "\n",
    "Links de inter√©s:\n",
    "https://explained.ai/gradient-boosting/descent.html\n",
    "https://www.cienciadedatos.net/documentos/py09_gradient_boosting_python.html\n",
    "https://explained.ai/gradient-boosting/L2-loss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da94600-2837-4b22-85c4-c49e517ca672",
   "metadata": {
    "id": "8da94600-2837-4b22-85c4-c49e517ca672"
   },
   "source": [
    "## Restricciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae850e-e0b4-4636-bc73-d7990d023fa4",
   "metadata": {
    "id": "60ae850e-e0b4-4636-bc73-d7990d023fa4"
   },
   "source": [
    "<center>\n",
    "<img src='https://media0.giphy.com/media/xT5LMQSg7kWT4zqsVy/200w.gif?cid=6c09b952x4akl84fpyhlm0306y58pyc5ju6yarl6ifo7h0ev&ep=v1_internal_gif_by_id&rid=200w.gif&ct=g' width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2455263-d46b-450e-a86c-9574bdcf673c",
   "metadata": {
    "id": "b2455263-d46b-450e-a86c-9574bdcf673c"
   },
   "source": [
    "### Qu√© es una funci√≥n Monot√≥na?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce76ee5-2295-43e1-913b-cbbfb4d83657",
   "metadata": {
    "id": "8ce76ee5-2295-43e1-913b-cbbfb4d83657"
   },
   "source": [
    "Diferentes variaciones entre una caracter√≠stica X e Y se reconocen como una funci√≥n no mon√≥tona, y podemos representar esto de la siguiente manera:\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915081517.png?raw=true' width=350 />\n",
    "\n",
    "Por otro lado, si la relaci√≥n aumenta, tenemos una funci√≥n mon√≥tonamente creciente:\n",
    "\n",
    "$$f(x_{1}, x_{2}, x, \\dots, x_{n}) \\geq f(x_{1}, x_{2}, x', \\dots, x_{n})$$\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915081603.png?raw=true' width=350 />\n",
    "\n",
    "Finalmente, otro caso es la funci√≥n mon√≥tonamente decreciente, dada por una relaci√≥n decreciente entre X e Y:\n",
    "\n",
    "$$f(x_{1}, x_{2}, x, \\dots, x_{n}) \\leq f(x_{1}, x_{2}, x', \\dots, x_{n})$$\n",
    "    \n",
    "<centering>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915081659.png?raw=true' width=350 />\n",
    "</centering>\n",
    "\n",
    "\n",
    "Matem√°ticamente, si podemos establecer **restricciones que nos permitan generar una salida positiva o negativa para cada variable**, esto nos permitir√° **generar un mejor modelo** para el negocio.\n",
    "\n",
    "Estas son las ideas b√°sicas que necesitamos comprender para saber c√≥mo utilizar esto. El siguiente paso es definir c√≥mo podemos obtener o visualizar el comportamiento mon√≥tono entre las variables (si no tenemos una idea clara de la relaci√≥n). La mejor manera de hacerlo es mediante el uso de Gr√°ficos de Dependencia Parcial (PDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38d6cc-7b77-4484-9bd8-fae5ac7b8471",
   "metadata": {
    "id": "5b38d6cc-7b77-4484-9bd8-fae5ac7b8471"
   },
   "source": [
    "### Monoticidad en Modelos\n",
    "#### Motivaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a580d1-614a-49c0-94ea-5ac0ea01521e",
   "metadata": {
    "id": "78a580d1-614a-49c0-94ea-5ac0ea01521e"
   },
   "source": [
    "**Si conocemos la relaci√≥n entre dos variables y el objetivo** (por ejemplo, desde un punto de vista te√≥rico), **deber√≠amos generar o ayudar al modelo a generar un modelo con este comportamiento**. Agregar restricciones al modelo nos permitir√° obtener **modelos mejores y con menos sesgo** en sus decisiones. Recuerda, si podemos corregir el sesgo en un modelo, podremos tomar decisiones mejores en el futuro.\n",
    "\n",
    "Por ejemplo: Supongamos que tenemos pocos datos para una variable $X_1$ que sabemos tiene un comportamiento mon√≥tonamente negativo con el objetivo Y. Sin embargo, el comportamiento en el conjunto de datos de entrenamiento nos muestra una historia diferente (por ejemplo, por pocos datos). Si observ√°ramos la relaci√≥n entre el objetivo y la caracter√≠stica, ver√≠amos una tendencia mon√≥tonamente positiva. ¬øC√≥mo podemos resolver esto?... mediante la imposici√≥n de restricciones de monoton√≠a en el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475972e6-994b-4e82-ae31-4cc2e924b60c",
   "metadata": {
    "id": "475972e6-994b-4e82-ae31-4cc2e924b60c"
   },
   "source": [
    "![image4](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915093309.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c84ff1-f824-4791-8928-d57216503c6d",
   "metadata": {
    "id": "52c84ff1-f824-4791-8928-d57216503c6d"
   },
   "source": [
    "¬øQu√© logramos al aplicar estas restricciones a nuestro modelo? B√°sicamente, resolvimos 3 puntos durante el modelado: **Equidad**, **consistencia** y **confiabilidad**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63e9e2-8bc9-46f5-b7fe-71797e358ce4",
   "metadata": {
    "id": "ec63e9e2-8bc9-46f5-b7fe-71797e358ce4"
   },
   "source": [
    "![](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915095556.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef02125-2689-42d6-9193-8f0b3f421156",
   "metadata": {
    "id": "2ef02125-2689-42d6-9193-8f0b3f421156"
   },
   "source": [
    "#### Dependencias Parciales\n",
    "\n",
    "¬øQu√© es un **Gr√°fico de Dependencias Parciales**? B√°sicamente, es un **gr√°fico que nos ayuda a comprender el comportamiento de una variable en relaci√≥n con la variable objetivo**. Estos gr√°ficos nos permiten ver el **comportamiento esperado (promedio**) de la variable con respecto al objetivo y se representan como un gr√°fico de l√≠neas donde X es la variable que deseamos analizar e Y es la variable objetivo.\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915082448.png?raw=true' width=800 />\n",
    "</center>\n",
    "    \n",
    "#### ¬øC√≥mo se implementa esto?\n",
    "\n",
    "La funci√≥n parcial $\\hat{f}_S$ se estima calculando promedios en los datos de entrenamiento, tambi√©n conocido como **m√©todo de Monte Carlo**:\n",
    "\n",
    "$$\\hat{f}_S(x_S) = \\frac{1}{n} \\sum_{i=1}^n \\hat{f} (x_S, x_C^{(i)})$$\n",
    "\n",
    "La funci√≥n parcial nos dice cu√°l es el **efecto marginal promedio** en la predicci√≥n para un valor dado (o valores) de las caracter√≠sticas S. En esta f√≥rmula, $x_C^{(i)}$ son los valores reales de las caracter√≠sticas del conjunto de datos para las caracter√≠sticas en las que no estamos interesados, y n es el n√∫mero de instancias en el conjunto de datos. Se asume en el PDP que las caracter√≠sticas en C no est√°n correlacionadas con las caracter√≠sticas en S.\n",
    "\n",
    "#### ¬øPor qu√© es relevante?\n",
    "\n",
    "Una comparaci√≥n de gr√°ficos de dependencias parciales puede proporcionar una visi√≥n importante, por ejemplo, revelar la estabilidad de la predicci√≥n del modelo. Algunas otras caracter√≠sticas que estos modelos permiten son:\n",
    "\n",
    "1. Se **pueden crear perfiles para todas las observaciones en el conjunto**, as√≠ como para la divisi√≥n en relaci√≥n con otras variables. Por ejemplo, podemos ver c√≥mo se comporta una variable espec√≠fica cuando se diferencia por g√©nero, raza u otros factores.\n",
    "2. Podemos **detectar algunas relaciones complicadas entre variables**. Por ejemplo, tenemos perfiles de dependencia parcial para dos modelos y podemos ver que uno de los modelos simples (regresi√≥n lineal) no detecta ninguna dependencia, mientras que el perfil de un modelo de caja negra (bosque aleatorio) nota una diferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f38da0-be78-4a23-891e-55d6d0fbe936",
   "metadata": {
    "id": "b3f38da0-be78-4a23-891e-55d6d0fbe936"
   },
   "source": [
    "### Restricciones de Interacci√≥n\n",
    "\n",
    "Otro **problema** que puede surgir en los modelos que generamos es la **creaci√≥n de modelos sesgados**. Estos casos pueden ocurrir cuando observamos(por ejemplo, con SHAP) que **nuestro modelo relaciona variables que no deber√≠an estar relacionadas**, generando problemas de **discriminaci√≥n** en el camino. Por ejemplo, la figura a continuaci√≥n muestra c√≥mo la raza interact√∫a m√°s intensamente con la duraci√≥n de la estancia, el grupo de edad y los antecedentes por a√±o. Estas interacciones desaparecer√≠an una vez que eliminamos la raza. Sin embargo, dada esta observaci√≥n, se debe prestar especial atenci√≥n si estas caracter√≠sticas no tienen **sesgo racial incorporado**. La investigaci√≥n respalda la necesidad del grupo de edad y los antecedentes por a√±o, lo que deja la duraci√≥n de la estancia como candidata para un escrutinio.\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915100939.png?raw=true' width=500 />\n",
    "</center>\n",
    "    \n",
    "Cuando la profundidad del √°rbol es mayor que uno, muchas variables interact√∫an √∫nicamente con el fin de minimizar la p√©rdida de entrenamiento, y el √°rbol de decisi√≥n resultante puede capturar una relaci√≥n espuria (ruido) en lugar de una relaci√≥n leg√≠tima que generalice en diferentes conjuntos de datos. Las **restricciones de interacci√≥n de caracter√≠sticas** permiten a los usuarios decidir qu√© variables pueden interactuar y cu√°les no.\n",
    "\n",
    "<center>\n",
    "<img src='https://xgboost.readthedocs.io/en/stable/_images/feature_interaction_illustration1.svg' width=500 />\n",
    "</center>\n",
    "    \n",
    "Ajustando las restricciones de interacci√≥n en el conjunto de datos anterior y evitando la relaci√≥n entre las variables raciales y el resto de los datos, podemos obtener los siguientes resultados.\n",
    "\n",
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-02/bosting/Pasted%20image%2020230915101644.png?raw=true' width=500 />\n",
    "</center>\n",
    "    \n",
    "Los posibles beneficios incluyen:\n",
    "- **Mejor rendimiento predictivo** al centrarse en las interacciones que funcionan.\n",
    "- Menos ruido en las predicciones -> **mejor generalizaci√≥n**.\n",
    "- Mayor **control para el usuario** sobre lo que el modelo puede ajustar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff408ab-bb4f-4dab-b4cc-1776d7e4fbaa",
   "metadata": {
    "id": "6ff408ab-bb4f-4dab-b4cc-1776d7e4fbaa"
   },
   "source": [
    "## Modelos de Boosting m√°s conocidos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bbab6-81ea-40cf-a58b-f1b4b03c2ffe",
   "metadata": {
    "id": "935bbab6-81ea-40cf-a58b-f1b4b03c2ffe"
   },
   "source": [
    "<center>\n",
    "<img src='https://media1.tenor.com/m/X2uSWnvzEIoAAAAC/simpsons-monkey-knife-fight.gif' width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b43a1c8-23f3-48fb-8c6c-5ae387c39bd5",
   "metadata": {
    "id": "1b43a1c8-23f3-48fb-8c6c-5ae387c39bd5"
   },
   "source": [
    "**XGBoost**, **LightGBM** y **CatBoost** son algoritmos de aprendizaje supervisado que utilizan t√©cnicas de boosting con √°rboles de decisi√≥n como weak learners. Son populares debido a su eficiencia en el manejo de grandes cantidades de datos y su capacidad para manejar diversas formas de datos tabulares.\n",
    "\n",
    "Aunque la idea y el rendimiento son bastante similares entre estos modelos, es crucial entender ciertos aspectos fundamentales que los caracterizan. Por esta raz√≥n, a continuaci√≥n exploraremos algunos de los supuestos clave que definen a cada uno:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7122d8-32c6-45fe-88cd-f181707447e0",
   "metadata": {
    "id": "fd7122d8-32c6-45fe-88cd-f181707447e0"
   },
   "source": [
    "### XGBoost (Xtreme Gradient Boosting)\n",
    "\n",
    "- **Descripci√≥n**: XGBoost es un algoritmo de optimizaci√≥n basado en gradientes que utiliza modelos de √°rboles de decisi√≥n ensamblados. Es conocido por su rendimiento y velocidad.\n",
    "- **Caracter√≠sticas**:\n",
    "  - Manejo eficiente de los valores faltantes (los desvia hac√≠a una de las ramas por defecto).\n",
    "  - Soporte para regularizaci√≥n (L1 y L2) que previene el sobreajuste.\n",
    "  - Alta flexibilidad, permite definir objetivos de optimizaci√≥n personalizados y funciones de evaluaci√≥n.\n",
    "\n",
    "### LightGBM (Light Gradient Boosting Machine)\n",
    "- **Descripci√≥n**: LightGBM es un framework de boosting que utiliza √°rboles de decisi√≥n basados en gradientes. Es similar a XGBoost pero optimizado para ser m√°s r√°pido y eficiente en memoria.\n",
    "- **Caracter√≠sticas**:\n",
    "  - Utiliza un algoritmo basado en el histograma para la construcci√≥n de √°rboles, lo que reduce el consumo de memoria y aumenta la velocidad.\n",
    "  - Soporta el crecimiento del √°rbol por hoja (leaf-wise) que tiende a ser m√°s eficiente para conjuntos de datos grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af102f-3d80-42ee-9552-4fecde094486",
   "metadata": {
    "id": "b4af102f-3d80-42ee-9552-4fecde094486"
   },
   "source": [
    "<center>\n",
    "<img src='https://www.oreilly.com/api/v2/epubs/9781789346411/files/assets/1da830ed-b89a-4437-83bb-373cdfac49fb.png' width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf24b5-8fbd-4757-b121-3d79831e1f99",
   "metadata": {
    "id": "1edf24b5-8fbd-4757-b121-3d79831e1f99"
   },
   "source": [
    "> **Nota**: En el contexto de los algoritmos de √°rboles de decisi√≥n, la **forma en que se expanden los √°rboles durante el entrenamiento puede tener un impacto significativo en el rendimiento** y la eficiencia del modelo. Existen dos m√©todos principales para expandir √°rboles: el **crecimiento por mejor primero** (leaf-wise) y el **crecimiento por profundidad primero** (level-wise). Aunque ambos m√©todos pueden resultar en la construcci√≥n del mismo √°rbol si se permite que el √°rbol crezca hasta su m√°xima profundidad, las diferencias en el orden de expansi√≥n del √°rbol son cruciales, especialmente en escenarios pr√°cticos donde no se suele permitir este crecimiento completo debido a la implementaci√≥n de criterios de *early-stopping* y m√©todos de *prunning*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ec550-de8f-4153-916b-190f78d2332e",
   "metadata": {
    "id": "8a3ec550-de8f-4153-916b-190f78d2332e"
   },
   "source": [
    "### CatBoost\n",
    "- **Descripci√≥n**: CatBoost es un algoritmo de boosting tambi√©n basado en √°rboles de decisi√≥n que maneja muy bien las variables categ√≥ricas sin necesidad de preprocesamiento extenso.\n",
    "- **Caracter√≠sticas**:\n",
    "  - Procesamiento autom√°tico de variables categ√≥ricas.\n",
    "  - Menos sensible a los par√°metros de configuraci√≥n, lo que facilita la puesta en marcha.\n",
    "  - Utiliza simetrizaci√≥n de √°rboles para reducir el sobreajuste.\n",
    "\n",
    "> **Nota**: La simetrizaci√≥n en CatBoost implica que durante la creaci√≥n de los √°rboles, el algoritmo utiliza un **enfoque que garantiza que las decisiones tomadas en las etapas tempranas del √°rbol no se basen demasiado en caracter√≠sticas espec√≠ficas**, ayudando a que el modelo sea m√°s general y menos propenso a ajustarse excesivamente a los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c5284-2b3b-4088-9cb2-40f931c48325",
   "metadata": {
    "id": "5b9c5284-2b3b-4088-9cb2-40f931c48325"
   },
   "source": [
    "### Diferencias Principales\n",
    "- **Manejo de datos**: CatBoost maneja autom√°ticamente las variables categ√≥ricas, mientras que en XGBoost y LightGBM se necesita un preprocesamiento previo (a la fecha esto ya fue implementado por los otros algoritmos).\n",
    "- **Velocidad y uso de memoria**: LightGBM es generalmente m√°s r√°pido y utiliza menos memoria que XGBoost debido a su algoritmo basado en histogramas.CatBoost, aunque eficiente, puede ser m√°s lento que los otros dos debido a su tratamiento de las variables categ√≥ricas.\n",
    "- **Facilidad de uso**: CatBoost es menos sensible a la configuraci√≥n de par√°metros, lo que lo hace m√°s f√°cil de usar para principiantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa1ac7-9e1d-492e-bfac-3a4bde28f9e8",
   "metadata": {
    "id": "0faa1ac7-9e1d-492e-bfac-3a4bde28f9e8"
   },
   "source": [
    "### Ejemplo B√°sico de Uso con Scikit-Learn\n",
    "A continuaci√≥n, un peque√±o ejemplo de c√≥mo se usa cada uno de estos algoritmos con los wrappers de clasificaci√≥n en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SQvNAmM6rW0I",
   "metadata": {
    "executionInfo": {
     "elapsed": 25962,
     "status": "ok",
     "timestamp": 1745246773096,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "SQvNAmM6rW0I"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U xgboost\n",
    "!pip install -q -U lightgbm\n",
    "!pip install -q -U catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5d426-1972-48d8-a181-6bcaf16d42bb",
   "metadata": {
    "id": "24f5d426-1972-48d8-a181-6bcaf16d42bb"
   },
   "source": [
    "**XGBoost**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c063f-91cc-4809-aab1-d186693709be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1745246773384,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "450c063f-91cc-4809-aab1-d186693709be",
    "outputId": "61039062-6cd4-4639-e981-af427f92d52b"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdac585-4d68-42e7-a81d-8fb975fea54b",
   "metadata": {
    "id": "9bdac585-4d68-42e7-a81d-8fb975fea54b"
   },
   "source": [
    "**LGBM**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8bc37-b4d2-4122-a1d7-254942a57a91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4018,
     "status": "ok",
     "timestamp": 1745246777403,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "73c8bc37-b4d2-4122-a1d7-254942a57a91",
    "outputId": "28e35adc-5806-49d6-bae3-62c04655be41"
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(verbose=-1) # `verbose=-1` para no mostrar mensajes de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7700e1-5553-48d1-aae8-41e8d733bf67",
   "metadata": {
    "id": "fb7700e1-5553-48d1-aae8-41e8d733bf67"
   },
   "source": [
    "**CatBoost**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d5bee7-ea24-49c9-b393-0d631fd76d94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2001,
     "status": "ok",
     "timestamp": 1745246779413,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "d1d5bee7-ea24-49c9-b393-0d631fd76d94",
    "outputId": "eba241cc-d8e3-40d0-fbab-80d71fece5e4"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(verbose=0)  # `verbose=0` para no mostrar mensajes de entrenamiento\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f6cef3-daba-4aed-8cb0-66804ba846e5",
   "metadata": {
    "id": "21f6cef3-daba-4aed-8cb0-66804ba846e5"
   },
   "source": [
    "## Importancia de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71cdc2-5ed7-4aa0-8ee4-5f5845eed8d7",
   "metadata": {
    "id": "fb71cdc2-5ed7-4aa0-8ee4-5f5845eed8d7",
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2025-01/gbm/feature_importances.png?raw=true' width=750 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d50b62-06b9-4a4a-9473-e84854129162",
   "metadata": {
    "id": "10d50b62-06b9-4a4a-9473-e84854129162"
   },
   "source": [
    "Como sabemos, los algoritmos basados en √°rboles eval√∫an la relevancia de sus caracter√≠sticas observando c√≥mo sus variables contribuyen a la divisi√≥n de los nodos. Por esta raz√≥n, algoritmos como XGBoost tambi√©n incluyen m√©todos para determinar la importancia de las caracter√≠sticas. Una manera de visualizar la relevancia de las features en XGBoost es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f98003-8fd5-40b6-91e0-84de39ee172f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1745246779942,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "21f98003-8fd5-40b6-91e0-84de39ee172f",
    "outputId": "dead7612-dee4-42c0-9163-59504be264fe"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_importance(model, importance_type='weight'):\n",
    "    # Obtener las importancias de las caracter√≠sticas\n",
    "    importance_dict = model.get_booster().get_score(importance_type=importance_type)\n",
    "\n",
    "    # Convertir las importancias a un DataFrame para facilitar la visualizaci√≥n\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': importance_dict.keys(),\n",
    "        'Importance': importance_dict.values()\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Graficar las importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.gca().invert_yaxis()  # Invertir el eje y para tener la caracter√≠stica m√°s importante en la parte superior\n",
    "    plt.show()\n",
    "\n",
    "# Asumiendo que X_train, y_train est√°n predefinidos y son adecuados para el modelo\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "plot_importance(model, importance_type='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f281aa5-0ef1-4428-b3f0-0f66ecd48f84",
   "metadata": {
    "id": "9f281aa5-0ef1-4428-b3f0-0f66ecd48f84"
   },
   "source": [
    "ok, pero qu√© sucede si ploteamos otra importancia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b072b62f-85c3-4219-9c86-b2b894f03700",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1745246780208,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "b072b62f-85c3-4219-9c86-b2b894f03700",
    "outputId": "f1aedde2-5dd5-4003-ed01-921acb58c9c2"
   },
   "outputs": [],
   "source": [
    "plot_importance(model, importance_type='gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b6264-3552-4446-90d6-340c56b99c5b",
   "metadata": {
    "id": "1d8b6264-3552-4446-90d6-340c56b99c5b"
   },
   "source": [
    "## ¬øC√≥mo se ve el overfitting en estos modelos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e0cf2-7681-47aa-aecb-cfca21137dd6",
   "metadata": {
    "id": "678e0cf2-7681-47aa-aecb-cfca21137dd6"
   },
   "source": [
    "<center>\n",
    "<img src='https://media.tenor.com/dtWcrQzDfsAAAAAM/the-simpsons-homer-simpson.gif' width=350 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a21e0-d081-402f-9431-a4d71bc7fcf3",
   "metadata": {
    "id": "971a21e0-d081-402f-9431-a4d71bc7fcf3"
   },
   "source": [
    "Debido a que estos modelos se basan en la iteraci√≥n tras iteraci√≥n, consisten en una acumulaci√≥n de estimadores d√©biles que se van a√±adiendo en cada ciclo. Este enfoque puede llevar a un considerable punto de controversia, ya que agregar **demasiados estimadores podr√≠a provocar un sobreajuste del modelo**.\n",
    "Aunque en la clase anterior observamos c√≥mo identificar sobreajustes mediante curvas de aprendizaje, el **entrenamiento por iteraciones** facilita la visualizaci√≥n por etapas, lo que hace m√°s intuitivos los problemas durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee060f-2511-4136-983f-d7a0793c0c1f",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1745250656043,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "b9ee060f-2511-4136-983f-d7a0793c0c1f"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_training(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value, 2) for value in y_pred]\n",
    "\n",
    "    # Evaluamos los modelos\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "    results = model.evals_result()\n",
    "    epochs = len(results['validation_0']['merror'])\n",
    "\n",
    "    # Log Loss Plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(epochs)), y=results['validation_0']['mlogloss'],\n",
    "                             mode='lines', name='Train Log Loss'))\n",
    "    fig.add_trace(go.Scatter(x=list(range(epochs)), y=results['validation_1']['mlogloss'],\n",
    "                             mode='lines', name='Test Log Loss'))\n",
    "    fig.update_layout(title='XGBoost Log Loss Over Epochs',\n",
    "                      template='simple_white',\n",
    "                      xaxis_title='Epoch',\n",
    "                      yaxis_title='Log Loss',\n",
    "                      legend_title='Dataset')\n",
    "    fig.show()\n",
    "\n",
    "    # Classification Error Plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=list(range(epochs)), y=results['validation_0']['merror'],\n",
    "                             mode='lines', name='Train Error'))\n",
    "    fig.add_trace(go.Scatter(x=list(range(epochs)), y=results['validation_1']['merror'],\n",
    "                             mode='lines', name='Test Error'))\n",
    "    fig.update_layout(title='XGBoost Classification Error Over Epochs',\n",
    "                      template='simple_white',\n",
    "                      xaxis_title='Epoch',\n",
    "                      yaxis_title='Classification Error',\n",
    "                      legend_title='Dataset')\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7247e3-0c19-487f-88aa-5e8fd2778fb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1745250657171,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "7c7247e3-0c19-487f-88aa-5e8fd2778fb4",
    "outputId": "2c339fc6-998f-4022-9156-2e36b0ea2b9c"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(eval_metric=[\"merror\", \"mlogloss\"])\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e5e54-f0b7-4e73-b1df-7593e1a3c5aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 73,
     "status": "ok",
     "timestamp": 1745250659295,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "282e5e54-f0b7-4e73-b1df-7593e1a3c5aa",
    "outputId": "762679ae-edab-4abe-84e6-864ef1f07e86"
   },
   "outputs": [],
   "source": [
    "plot_training(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ea830-822a-4111-b6c1-fc08e2c454d3",
   "metadata": {
    "id": "920ea830-822a-4111-b6c1-fc08e2c454d3"
   },
   "source": [
    "### Early-Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e8fdc7-47d9-4662-a297-a6b55eecd99e",
   "metadata": {
    "id": "29e8fdc7-47d9-4662-a297-a6b55eecd99e"
   },
   "source": [
    "El *early stopping* en XGBoost es una t√©cnica muy √∫til para **prevenir el sobreajuste** y mejorar la eficiencia del entrenamiento. Este m√©todo permite **detener el proceso de entrenamiento de manera anticipada si no se observan mejoras en la m√©trica de evaluaci√≥n despu√©s de un n√∫mero espec√≠fico de rondas consecutivas**.\n",
    "\n",
    "Cuando configuras XGBoost para utilizar *early stopping*, debes **especificar una m√©trica de evaluaci√≥n**, como la tasa de error o log-loss para clasificaci√≥n, y RMSE para regresi√≥n. Tambi√©n debes **definir el n√∫mero de rondas sin mejora** despu√©s de las cuales el entrenamiento se detendr√°. Este n√∫mero se conoce como el par√°metro `early_stopping_rounds`.\n",
    "\n",
    "Por ejemplo, si estableces `early_stopping_rounds=10`, XGBoost evaluar√° la m√©trica seleccionada en un conjunto de validaci√≥n despu√©s de cada ronda de entrenamiento. Si la m√©trica no mejora despu√©s de 10 rondas consecutivas, XGBoost detiene el entrenamiento. Esto no solo previene el sobreajuste, sino que tambi√©n ahorra tiempo y recursos computacionales.\n",
    "\n",
    "Adem√°s, **XGBoost permite continuar el entrenamiento desde la √∫ltima iteraci√≥n guardada** si se desea ajustar a√∫n m√°s el modelo. Esto es √∫til en casos donde las condiciones iniciales de *early stopping* fueron demasiado conservadoras. Utilizar *early stopping* es una pr√°ctica recomendada cuando se utilizan modelos avanzados de boosting como XGBoost, especialmente en competencias de modelado predictivo donde el rendimiento √≥ptimo del modelo es crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef241c-d7b8-481c-8610-218babf922dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1745246781225,
     "user": {
      "displayName": "Sebasti√°n Tinoco P√©rez",
      "userId": "06104689937544930682"
     },
     "user_tz": 240
    },
    "id": "62ef241c-d7b8-481c-8610-218babf922dd",
    "outputId": "0590681e-96b6-4395-d04d-cdb2930d1a2a"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(early_stopping_rounds=1, eval_metric=[\"merror\", \"mlogloss\"])\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "model.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
    "plot_training(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
