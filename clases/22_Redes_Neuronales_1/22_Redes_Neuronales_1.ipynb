{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2787705-568d-4d9f-b316-3d26dda12bba",
   "metadata": {},
   "source": [
    "# Clase 22: Redes Neuronales\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci贸n Cient铆fica para Ciencia de Datos**\n",
    "\n",
    "**Profesor: Pablo Badilla**\n",
    "\n",
    "Basado en las clases de Nicol谩s Caro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140d11d-3dfb-4c84-a396-a42d5711ea26",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Introducci贸n\n",
    "\n",
    "La redes neuronales aparecen bajo el nombre de **perceptr贸n**, propuesta por [Rosenblatt en 1958](http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf). Este es un modelo de clasificaci贸n consistente en pesos $w$ y una funci贸n de output del tipo $f(w^t\\mathbf{x})$ para el input $\\mathbf{x}$. \n",
    "\n",
    "\n",
    "El percetr贸n implementa $f(\\cdot)$ predice en base a una funci贸n de salto, que entrega el valor 1 para argumentos mayores que 0 y retorna 0 en otro caso. \n",
    "\n",
    "El principal problema de este modelo consiste en su poca expresibilidad, pues solo permite hacer separaciones lineales. \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='./resources/perceptron.png' width=600/>\n",
    "</div>\n",
    "    \n",
    "    \n",
    "    \n",
    "<div align='center'>\n",
    "Fuente:\n",
    "    <a href='https://www.javatpoint.com/single-layer-perceptron-in-tensorflow'>javatpoint</a>.\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "<img src='./resources/step.png' width='400' />\n",
    "</div>\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b4479-9a7c-4ba0-9252-c7657168fffe",
   "metadata": {},
   "source": [
    "### Ejemplo a mano\n",
    "\n",
    "En este ejemplo haremos un perceptr贸n que clasifique si un vector representa o no a un gato.\n",
    "\n",
    "El vector est谩 compuesto por los siguientes atributos:\n",
    "\n",
    "\\[`tiene cola`, `tiene dos orejas`, `son adoptados como mascotas`, `maulla`\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e8b05-e545-464b-aac9-abb96795ab1e",
   "metadata": {},
   "source": [
    "Generamos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88262027-a1f6-4c67-bd4d-f9467425db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "gato = [1, 1, 1, 1]\n",
    "persona = [0, 1, 0, 0]\n",
    "arbol = [0, 0, 0, 0]\n",
    "perro = [1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4a3dd10-3a3f-43f1-833c-5acab33e6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = np.array([gato, persona, arbol, perro])\n",
    "\n",
    "# solo el primero lo etiquetamos como gato.\n",
    "etiquetas = np.array([1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2afc44f-4a62-4207-b4ac-a13ede13b20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [1, 1, 1, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cf0a19f-7b63-48e0-9dc2-37f49fa6b553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510ccbc-3d9e-4b27-9d97-7e29df7ec02f",
   "metadata": {},
   "source": [
    "Seleccionamos un ejemplo a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c04433c2-0e8f-4605-af22-79dfdb0bfbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo = perro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc12700-a90e-4f48-92ae-ddd47fa0e215",
   "metadata": {},
   "source": [
    "Definimos nuestros pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b349938-21f3-4579-ae1b-2613a05ad1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos = np.array([0.0, -0.01, 0.0, 0.03])\n",
    "sesgo = -0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01064075-6ff9-4499-b107-6c845dba2a56",
   "metadata": {},
   "source": [
    "Calculamos la suma ponderada de los pesos por la entrada. Comunmente a esto le llamamos *acumulaci贸n de informaci贸n* o *carga*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c261661-21e6-4a67-8865-26dce9cdc730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.  , -0.01,  0.  ,  0.  ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos * ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7679e15-8b4e-4b19-8013-3f96f491d525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pesos * ejemplo) + sesgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6a895f-7097-4f71-9926-5ba739efd16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(pesos, ejemplo) + sesgo  # mucho m谩s simple usar np.dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a056293-08ec-4120-89c1-b9b73e9e3ef2",
   "metadata": {},
   "source": [
    "Noten que el sesgo define que hace el perceptr贸n en el caso que toda la entrada vale 0. En general, desplaza la funci贸n de entrada hacia arriba o hacia abajo.\n",
    "\n",
    "Otra forma de verlo (cuando es negativo) es cuanta informaci贸n acumulada m铆nima requiero para activar la neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc205b6-a9cb-425d-af87-31220c0d1772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acumulacion_de_informacion = np.dot(pesos, ejemplo) + sesgo\n",
    "acumulacion_de_informacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbd78a-a4f2-410e-a7db-9c38f790e07c",
   "metadata": {},
   "source": [
    "Por 煤ltimo, clasificamos. \n",
    "\n",
    "- Si la suma es mayor que 0, retornamos 1.\n",
    "- Si no, retornamos 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47b278e-6424-4b3b-a73e-3c5931d6b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion_de_activacion(x):\n",
    "    return 1 if x > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "136fd562-0d74-411f-a13e-398dd99165fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediccion = funcion_de_activacion(acumulacion_de_informacion)\n",
    "prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0130982-f823-4827-8157-433b942f317e",
   "metadata": {},
   "source": [
    "### Aprendizaje en un Perceptr贸n y Descenso del Gradiente\n",
    "\n",
    "Entrenar un perceptron consiste en ir ajustando los pesos a medida que vemos los datos de entrenamiento. Comunmente, esto se logra a trav茅s del descenso del gradiente.\n",
    "\n",
    "Para explicar esta idea, primero definimos una funci贸n de p茅rdida o loss $\\mathcal{L}$. Esta funci贸n se encarga de cuantificar cuanto nos equivocamos al predecir nuestros ejemplos de entrenamiento.\n",
    "\n",
    "El descenso del gradiente simplemente es un **m茅todo para minimizar una funci贸n de p茅rdida** a partir de datos de entrenamiento:\n",
    "\n",
    "$$w_{n+1} = w_{n} - \\alpha \\cdot \\Delta \\mathcal{L} $$\n",
    "\n",
    "En este caso:\n",
    "- $w_n$ representa los pesos actuales. $w_{n+1}$ los pesos actualizados.\n",
    "- $\\alpha$ se denomina como tasa de aprendizaje o Learning Rate. Es un hiperpar谩metro que ajusta tanto se mueven los pesos en cada iteraci贸n de entrenamiento.\n",
    "- $\\Delta\\mathcal{L}$ es el gradiente de la funci贸n de p茅rdida $\\mathcal{L}$ (i.e., su derivada con respecto cada par谩metro)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc31584-aa4a-4dd3-a09b-8f32ac456930",
   "metadata": {},
   "source": [
    "#### Par茅ntesis: Tasa de Aprendizaje\n",
    "\n",
    "<div align='center'>\n",
    "<img src='./resources/learning_rates.png' width=900/>\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='http://www.bdhammel.com/learning-rates/'>http://www.bdhammel.com/learning-rates/</a>\n",
    "</div>\n",
    "\n",
    "Veamos que sucede caso a caso. Una tasa de aprendizaje...\n",
    "\n",
    "- muy baja tomar谩 mucho tiempo en converger.\n",
    "- alta saltar谩 las mejores configuraciones \n",
    "- muy alta incluso podr铆a diverger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc07686-8b44-4c10-a4a7-3f135188fe3e",
   "metadata": {},
   "source": [
    "\n",
    "#### Relacionado: Proyecciones de Funciones de Loss de Distintas Redes Neuronales\n",
    "\n",
    "<div align='center'>\n",
    "<img alt='Proyecciones de Funciones de Loss de Distintas Redes Neuronales' src='./resources/proyecciones_loss.png' />\n",
    "</div>\n",
    "    \n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://www.cs.umd.edu/~tomg/projects/landscapes/'>https://www.cs.umd.edu/~tomg/projects/landscapes/</a>\n",
    "</div>\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061faa8-8070-442a-9e8b-99029a43edc7",
   "metadata": {},
   "source": [
    "> **Pregunta:** Viendo las im谩genes anteriores, 驴es posible que el descenso del gradiente no encuentre el optimo global?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8dcb2-82ef-4a29-af65-c4180326b8bb",
   "metadata": {},
   "source": [
    "#### Siguiendo con el Descenso del Gradiente\n",
    "\n",
    "\n",
    "El descenso del gradiente simplemente es un m茅todo para minimizar una funci贸n de p茅rdida a partir de datos de entrenamiento:\n",
    "\n",
    "$$w_{n+1} = w_{n} - \\alpha \\cdot \\Delta \\mathcal{L} $$\n",
    "\n",
    "En este caso:\n",
    "- $w_n$ representa los pesos actuales. $w_{n+1}$ los pesos actualizados.\n",
    "- $\\alpha$ se denomina como tasa de aprendizaje o Learning Rate. Es un hiperpar谩metro que ajusta tanto se mueven los pesos en cada iteraci贸n de entrenamiento.\n",
    "- $\\Delta\\mathcal{L}$ es el gradiente de la funci贸n de p茅rdida $\\mathcal{L}$ (i.e., su derivada con respecto cada par谩metro)\n",
    "\n",
    "\n",
    "\n",
    "En nuestro caso, usaremos la funci贸n de p茅rdida *suma de los errores cuadr谩ticos* o *sum of squared errors* **SSE**: \n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{2}\\sum_i (y - \\hat{y})^2$$\n",
    "\n",
    "Por lo tanto (suponiendo que en cada iteraci贸n solo vemos un ejemplo (i.e., omitimos la sumatoria)), su gradiente (para cualquier peso $j$) es:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{j}}\\mathcal{L} = y - \\hat{y}$$\n",
    "\n",
    "\n",
    "As铆, el descenso del gradiente para entrenar nuestro perceptr贸n es:\n",
    "\n",
    "$$w_{n+1} = w_{n} - \\alpha \\cdot (y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d952c-3517-4c95-b520-e1940618848d",
   "metadata": {},
   "source": [
    "Para ejemplificar lo dicho anteriormente, implementaremos un perceptr贸n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10e3ed0-c902-4958-a971-a7d37e46a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basado en # https://medium.com/@thomascountz/19-line-line-by-line-python-perceptron-b6f113b161f3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def funcion_de_activacion(x):\n",
    "    return 1 if x > 0 else 0\n",
    "\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, dimensiones_input, epocas=100, tasa_aprendizaje=0.01):\n",
    "        self.epocas = epocas\n",
    "        self.tasa_aprendizaje = tasa_aprendizaje\n",
    "        # los pesos los guardamos como un arreglo que contiene el sesgo en el 铆ndice 0 y\n",
    "        # los pesos que multiplican al input desde el 铆ndice 1.\n",
    "        self.pesos = np.zeros(dimensiones_input + 1)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        acumulacion_de_informacion = np.dot(inputs, self.pesos[1:]) + self.pesos[0]\n",
    "        return funcion_de_activacion(acumulacion_de_informacion)\n",
    "\n",
    "    def fit(self, datos_entrenamiento, etiquetas):\n",
    "        # Una 茅poca es una pasada completa sobre el dataset de entrenamiento.\n",
    "        for _ in range(self.epocas):\n",
    "            for entrada, etiqueta in zip(datos_entrenamiento, etiquetas):\n",
    "                prediccion = self.predict(entrada)\n",
    "                self.pesos[1:] += (\n",
    "                    self.tasa_aprendizaje * (etiqueta - prediccion) * entrada\n",
    "                )\n",
    "                self.pesos[0] += self.tasa_aprendizaje * (etiqueta - prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41e184a8-cc94-4eae-a0f0-ed4e53776ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1acf8607-a7c1-40bf-b3ce-91ecc36f281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.fit(entradas, etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f48c593-0191-40cc-87ef-453086e86c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.predict(gato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23ed00be-f72f-47c9-9337-5a15ceda0c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.predict(perro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "537d246a-c2a0-45da-ba0c-e69c0e34f559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01,  0.  , -0.01,  0.  ,  0.03])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a311b-6b35-4066-b33a-d6acfb695784",
   "metadata": {},
   "source": [
    "> **Nota**: Una 茅poca es una pasada completa sobre el dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17f01c-c39a-4468-9d2b-a0a268092ac4",
   "metadata": {},
   "source": [
    "> **Pregunta:** 驴Cuales son los hiperpar谩metros de nuestro perceptr贸n? 驴Podemos variar la funci贸n de p茅rdida? 驴Podemos tambi茅n variar la forma en que entrenamos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a74dce-3602-4f87-bb2a-ea7b7f5dd3ae",
   "metadata": {},
   "source": [
    "### Cantidad de datos que le entregamos al Descenso del Gradiente:\n",
    "\n",
    "El entrenamiento por medio del descenso del gradiente puede variar seg煤n la cantidad de datos que le entregamos en cada iteraci贸n. A continuaci贸n se muestran 3 enfoques:\n",
    "\n",
    "\n",
    "- Descenso del gradiente en lotes (o **batch**): En este caso entregamos todos los datos en una sola pasada. Puede estancar el aprendizaje debido a que siempre usaremos todas las muestras.\n",
    "\n",
    "\n",
    "- Descenso del Gradiente Estoc谩stico (Stochastic Gradient Descent o **SGD**): Usamos una muestra aleatoria en cada iteraci贸n. El gradiente y la actualizaci贸n de pesos se calcula en relaci贸n a esa muestra en particular, lo que dificulta el estancamiento. Sin embargo, implica lentitud en el proceso.\n",
    "\n",
    "- Descenso del gradiente Estoc谩stico en mini-lotes (**mini-batch**): Mezcla entre las opciones anteriores. Ingresa un peque帽o batch y actualiza los pesos sobre el gradiente calculado a partir de ese batch. Los ejemplos del batch son elegidos aleatoriamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59344092-8264-42d4-8d7f-303fdad4a8db",
   "metadata": {},
   "source": [
    "> **Pregunta**: 驴Por qu茅 se usa descenso de gradiente y no otros m茅todos de optimizaci贸n (como derivar e igualar a cero...)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397500a5-2231-4258-a929-4f2f593e613a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Perceptr贸n Multicapa\n",
    "\n",
    "\n",
    "Sobre el modelo anterior, podemos construir uno m谩s complejo llamado *perceptron multicapa* o *MultiLayer Perceptron* (**MLP**). Un MLP consiste en m煤ltiples modelos de perceptron intecomunicadas. Cada uno de estos modelos se denota como *unidad* (*unit*) y se organizan en capas secuenciales.\n",
    "\n",
    "<div align='center'>\n",
    "<img alt='MLP esquema simple' src='./resources/mlp_simple.png' width=600/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f/'>https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f/</a>\n",
    "</div>\n",
    "    \n",
    "\n",
    "En la figura:\n",
    "\n",
    "- La *capa input* corresponde a un conjunto de perceptrones que operan sobre el vector de entrada que distribuyen los valores del vector a la siguiente capa.\n",
    "\n",
    "- Las siguientes capas se denotan como *capas ocultas* y se contruyen de manera an谩loga a la capa input (o inicial). La primera capa oculta consiste en un conjunto de percetrones que operan sobre el output de la capa input. Sobre esta primera capa oculta pueden haber multiples capas ocultas que operan sobre el output de la capa oculta anterior. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172188a-da21-4ead-a85a-84f2935bc39b",
   "metadata": {},
   "source": [
    "#### Propagaci贸n Hacia Adelante o *Feedforward*\n",
    "\n",
    "\n",
    "El proceso de generar una salida sobre la entrada de una capa anterior se denota *propagaci贸n hacia adelante* (*feedforward*) la propagaci贸n termina en una 煤ltima capa denominada *capa output* que entrega el resultado final de la clasificaci贸n. \n",
    "\n",
    "Bajo el punto de vista del aprendizaje autom谩tico, las capas ocultas de un perceptron multicapa (red neuronal) generan abstracciones o caracter铆sticas a partir de los datos sobre los cuales operan. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c896644-3964-47cc-b6e5-4279f63232f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### `Pytorch`\n",
    "\n",
    "<div align='center'>\n",
    "<img alt='MLP esquema simple' src='./resources/pytorch_logo.png' width=600/>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Para trabajar con redes neuronales haremos uso de la librer铆a **`Pytorch`**. Esta librer铆a consiste en un conjunto de herramientas dise帽adas para generar modelos basados en redes neuronales utilizando las capacidades de computo distribuido que ofrecen las GPU (unidades de procesamiento gr谩fico / tarjetas de video). Esto permite operaciones de vectorizaci贸n aceleradas y distribuidas. Esta librer铆a se importa como `torch`.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Se implementa una red neuronal simple utilizando Pytorch. Para esto, se importa el m贸dulo y se indica una semilla aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e87b5f70-bd8a-4c7b-abe8-2f2a13f16c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd72a2fa770>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Se asigna un valor de reproductibilidad\n",
    "torch.manual_seed(6202)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efdb149-521b-4b75-93cd-d7b9c03f9334",
   "metadata": {},
   "source": [
    "#### Dataset de Ejemplo: Vino \n",
    "\n",
    "Se carga el dataset sobre el cual trabajaremos, en este caso ser谩 un conjunto de datos de vinos. Este conjunto consta de 13 atributos continuos, cada vino posee un identificador dentro de 3 clases. La idea es asignar un tipo de vino a cada observaci贸n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45c9d4a8-b7ed-4275-9c23-9dd0dbe67c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names = [\n",
    "    \"class\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\",\n",
    "]\n",
    "\n",
    "wine_data = pd.read_csv(\n",
    "    \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\",\n",
    "    names=names,\n",
    ")\n",
    "\n",
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f38cc7-0ed8-4a88-8520-b467e9037a20",
   "metadata": {},
   "source": [
    "Se procede a hacer una separaci贸n en entrenamiento y test, se har谩 es una codificaci贸n dummy para la variable de respuesta y se estandarizan las variables num茅ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1521b84a-6a79-4b5e-8767-ba816365ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "response = [\"class\"]\n",
    "num_cols = wine_data.loc[:, \"Alcohol\":].columns.values\n",
    "\n",
    "data_transform = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"normaliza\", StandardScaler(), num_cols),\n",
    "        (\"codifica\", OrdinalEncoder(), response),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e500e-c5ee-461f-93d4-1d7a8363fd4b",
   "metadata": {},
   "source": [
    "Para evitar fuga de informaci贸n, se hace una separaci贸n en train y test para luego transformar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d0036d6-745e-433f-8778-e909618fd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test = train_test_split(wine_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336fb10-89a6-4a3d-bd6b-d75f5c517d67",
   "metadata": {},
   "source": [
    "Se aplica el preprocesamiento sobre el conjunto train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "415bf9b0-b631-4aa6-9cfd-7f97edcd2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_transform.fit_transform(data_train)\n",
    "\n",
    "# Se obtienen las variables numericas y de respuesta\n",
    "\n",
    "X_train = data[:, :-1].copy()\n",
    "y_train = data[:, -1:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "947c48fa-8781-4766-973e-093a4dda442f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 13)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1258f11-6650-4a29-9b4e-70cba81f8868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d53fa7-5d8e-45e7-a3bc-10dae80651d5",
   "metadata": {},
   "source": [
    "#### MLP en `Pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513a481-bb23-4163-a61d-30bd8f770909",
   "metadata": {},
   "source": [
    "El perceptron multicapa que crearemos consiste en 10 capas ocultas y su funci贸n de activaci贸n tiene la forma:\n",
    "$$f(x) = max(0,x)$$\n",
    "\n",
    "Esta es una funci贸n de activaci贸n conocida y se denota como **ReLU (Rectified Linear Unit)**. \n",
    "\n",
    "Se definen los par谩metros, para la funci贸n de activaci贸n se utiliza el m贸dulo de redes neuronales de Pytorch, se puede acceder a este m贸dulo por medio de `torch.nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f9c4b0c-c6ad-476c-99be-b57ed881f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero, definimos el tama帽o de la  capa de entrada que es igual al tama帽o del vector de la entrada\n",
    "# y el tama帽o de la salida, que es igual al n煤mero de clases.\n",
    "input_dim, output_dim = (X_train.shape[1], 3)\n",
    "\n",
    "# Equivalente al output de la capa input\n",
    "input_dim_capas_ocultas = 5\n",
    "\n",
    "# por 煤ltimo definimos la funci贸n de activaci贸n.\n",
    "f_activation = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1dda06-1f6f-4818-a91d-eab45164beb0",
   "metadata": {},
   "source": [
    "#### Capa de Entrada\n",
    "\n",
    "Para definir la primera capa, tenemos que tener en cuenta que buscamos una transformaci贸n de la forma $x \\mapsto w^t x$ que luego es clasificada por medio de $f(w^t x)$. Esto quiere decir que los par谩metros a entrenar ser谩n los vectores de peso $w$ asociados a cada unidad (perceptron) de cada capa (input, ocultas y output).\n",
    "\n",
    "`Pytorch` ofrece una abstracci贸n para capas formadas por perceptrones de la forma $f(w^t x)$. Esta abstracci贸n es la clase `Linear` (observe que $x \\mapsto w^t x$ es una transformaci贸n lineal).\n",
    "\n",
    "A continuaci贸n definimos la capa de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9459b4f-eeb2-4820-8d95-399c8a99004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "# in_features: tama帽o del vector de entrada.\n",
    "# out_features: tama帽o de la primera capa oculta.\n",
    "capa_input = Linear(in_features=input_dim, out_features=input_dim_capas_ocultas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146b483-4c40-4bd6-95f0-2518787369df",
   "metadata": {},
   "source": [
    "#### Capas Ocultas\n",
    "\n",
    "Luego se definen las capas ocultas, estas tambi茅n corresponden a transformaciones lineales sobre sus capas predecesoras. En este caso, cada capa oculta tendr谩 dimensi贸n 5 como input y output. A continuaci贸n, definimos como ejemplo una capa oculta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ac6b14e-6eac-40a2-bf01-02f49722edbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=5, bias=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear(input_dim_capas_ocultas, input_dim_capas_ocultas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d74238-6f9d-47c7-877a-1f163e132098",
   "metadata": {},
   "source": [
    "### Arquitectura de la Red\n",
    "\n",
    "\n",
    "La arquitectura de esta red es de naturaleza secuencial y corresponde al siguiente algoritmo:\n",
    "\n",
    "1. Input: x de dimensi贸n 13\n",
    "2. Opera x en cada unidad de la capa lineal input, es decir, calcula $w_i^t x$ para $i = 1, \\ldots, 5$ (dimensi贸n de salida de la capa input).\n",
    "3. Opera RelU($w_i^t x$) para $i = 1, \\ldots, 5$, se obtiene un vector de dimensi贸n 5. \n",
    "\n",
    "4. Para cada una de las 2 capas ocultas y de manera secuencial:\n",
    "    1. Genera una transformaci贸n lineal sobre los outputs de la capa anterior.\n",
    "    2. Aplica ReLU sobre las transformaciones lineales.\n",
    "    3. En la 煤ltima capa oculta el vector resultante se pasa a la capa output.\n",
    "\n",
    "5. Se recibe el resultado de la 煤ltima capa oculta, se transforma de manera lineal sobre 3 perceptrones para finalmente ser transformada por medio de una funci贸n softmax (dimensi贸n del output es 3). \n",
    "\n",
    "Esto se reduce a: \n",
    "```\n",
    "X  -> capa input -> ReLU() -> Co_1 -> ReLU() -> Co_2 -> ReLU() -> capa output -> predicci贸n\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fbd55-e727-49a4-9254-7fcb8e4c506e",
   "metadata": {},
   "source": [
    "Para programar el esquema anterior se hace uso de un diccionario ordenado `OrderedDict` de la librer铆a `collections`. Este tipo de objetos opera de manera similar a las Pipelines de Scikit-learn, pues reciben un conjunto de tuplas del tipo `(identificador,objeto)`. Se procede a generar la arquitectura de la red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bf8958f-e758-4ab2-b13b-4e78383946d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('capa input', Linear(in_features=13, out_features=5, bias=True)),\n",
       "             ('capa oculta_0',\n",
       "              Linear(in_features=5, out_features=5, bias=True)),\n",
       "             ('relu_0', ReLU()),\n",
       "             ('capa oculta_1',\n",
       "              Linear(in_features=5, out_features=5, bias=True)),\n",
       "             ('relu_1', ReLU()),\n",
       "             ('capa ouput', Linear(in_features=5, out_features=3, bias=True)),\n",
       "             ('Softmax', Softmax(dim=1))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "n_capas_ocultas = 2\n",
    "\n",
    "# capa input\n",
    "input_ = (\"capa input\", capa_input)\n",
    "relu = (\"relu\", f_activation)\n",
    "\n",
    "# etapas (similar al pipeline)\n",
    "steps = [input_]\n",
    "\n",
    "# capas ocultas\n",
    "for i in range(n_capas_ocultas):\n",
    "\n",
    "    capa_i = (\n",
    "        f\"capa oculta_{i}\",\n",
    "        Linear(input_dim_capas_ocultas, input_dim_capas_ocultas),\n",
    "    )\n",
    "    relu_i = (f\"relu_{i}\", f_activation)\n",
    "\n",
    "    steps += [capa_i, relu_i]\n",
    "\n",
    "# capa output\n",
    "output = (\"capa ouput\", Linear(input_dim_capas_ocultas, output_dim))\n",
    "\n",
    "steps.extend([(output), (\"Softmax\", torch.nn.Softmax(dim=1))])\n",
    "\n",
    "# Se utiliza la estructura de diccionario ordenado\n",
    "steps = OrderedDict(steps)\n",
    "\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b1ab7-ff74-420e-9a9b-ea50f01322c5",
   "metadata": {},
   "source": [
    "Luego, cuando ya se posee la arquitectura, se inicializa un objeto `Sequential` del m贸dulo `nn`, este objeto permite modelar una red neuronal multicapa recibiendo como input los componentes de la arquitectura de manera ordenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66e16432-5fe9-40cb-bff4-7b487999191e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (capa input): Linear(in_features=13, out_features=5, bias=True)\n",
       "  (capa oculta_0): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (relu_0): ReLU()\n",
       "  (capa oculta_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (relu_1): ReLU()\n",
       "  (capa ouput): Linear(in_features=5, out_features=3, bias=True)\n",
       "  (Softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import Sequential\n",
    "\n",
    "# MLP -> multi layer perceptron\n",
    "mlp = Sequential(steps)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b84e66-bb10-43ef-8fb7-ad277603130a",
   "metadata": {},
   "source": [
    "El resultado entregado por la capa output corresponde a un vector de tres dimensiones, se asigna la clase predicha a aquella componente con el mayor valor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e053b-5f97-44cb-9ea9-fae978643582",
   "metadata": {},
   "source": [
    "### Funciones de P茅rdida y Entrop铆a Cruzada\n",
    "\n",
    "Luego de definir la red, es necesario definir la funci贸n de perdida. Al igual que las funciones de activaci贸n, tambi茅n tenemos una serie de funciones de p茅rdida recomendadas para ciertos problemas en particular.\n",
    "\n",
    "<div align='center'>\n",
    "<img alt='MLP esquema simple' src='./resources/tipos_de_funciones_loss.png' width=600/>\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "Fuente: <a href='https://towardsdatascience.com/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8'>https://towardsdatascience.com/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8</a>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "En este caso se utiliza la entrop铆a cruzada. Este criterio permite comparar dos distribuciones de probabilidad $q$ (aproximaci贸n) y $p$ (real) en t茅rminos de la diferencia de informaci贸n esperada (en bits por ejemplo) al utilizar la distribuci贸n $q$ para describir un eventos codificados, optimizados para $p$. Dado que se trabaja en un problema de clasificaci贸n (supervisado) se conoce la distribuci贸n real $p$ para una etiqueta  (ej: etiqueta (1,0,0), distribuci贸n (100%, 0, 0) ), por otra parte, la distribuci贸n aproximada viene dada por nuestro modelo. La entrop铆a cruzada entre $p$ y $q$ se expresa seg煤n:\n",
    "$$\n",
    "H(p, q)=-\\sum_{x \\in \\mathcal{X}} p(x) \\log q(x)\n",
    "$$\n",
    "\n",
    "Se implementa mediante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52ddd04a-9ec1-4eb6-9bc5-f4015bcd9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterio = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7138327-90dc-4ac0-8627-dc46be7db422",
   "metadata": {},
   "source": [
    "Se debe seleccionar el optimizador a utilizar, en este caso ser谩 **descenso de gradiente estoc谩stico**.  Se inicializa entregando dichos par谩metros y los coeficientes sobre los que opera, en este caso, los par谩metros de la red `mpl` a los cuales se acceede por medio del m茅todo `.parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea0b91d8-3574-4492-87e7-fd6d992324e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizador = torch.optim.SGD(mlp.parameters(), lr=0.05, momentum=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35567c6e-8fac-4b51-a4dc-c30ddee2f47f",
   "metadata": {},
   "source": [
    "Finalmente, se puede entrenar la red definida, para ello se define una cantidad de *茅pocas* (*epochs*), esto se refiere a la cantidad de veces que se entrena utilizando el conjunto de entrenamiento. Este proceso tiene el siguiente orden:\n",
    "\n",
    "1. Genera un conjunto de inputs en un formato compatible.\n",
    "2. En cada 茅poca:\n",
    "    1. Inicializa los gradientes asociados al optimizador, esto evita que se acumulen gradientes entre 茅pocas.\n",
    "    2. Se opera sobre los inputs para obtener las predicciones.\n",
    "    3. Se calcula la funci贸n de loss.\n",
    "    4. Se calcula el gradiente para cada par谩metro. Este proceso se denomina como *propagaci贸n hacia atr谩s* (*backpropagation*).\n",
    "    5. Se actualizan los par谩metros seg煤n los gradientes usados.\n",
    "    \n",
    "Se implementa el esquema anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d67185ad-a8c4-4d8c-8dd5-019bcba5423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso A - Inicializa los gradientes asociados al optimizador\n",
    "datos_input = torch.autograd.Variable(torch.Tensor(X_train))\n",
    "labels = torch.autograd.Variable(torch.Tensor(y_train.reshape([-1,])).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0bfec8dc-e035-4dd1-804c-2e54cacbd294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([142, 13])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "346a371f-dd8f-408e-a453-2a3675f208c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([142])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "384e2c83-33df-494c-a43b-470bc622c61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca:  10 Loss:  tensor(0.5520)\n",
      "Epoca:  20 Loss:  tensor(0.5520)\n",
      "Epoca:  30 Loss:  tensor(0.5519)\n",
      "Epoca:  40 Loss:  tensor(0.5519)\n",
      "Epoca:  50 Loss:  tensor(0.5519)\n",
      "Epoca:  60 Loss:  tensor(0.5518)\n",
      "Epoca:  70 Loss:  tensor(0.5518)\n",
      "Epoca:  80 Loss:  tensor(0.5518)\n",
      "Epoca:  90 Loss:  tensor(0.5518)\n",
      "Epoca:  100 Loss:  tensor(0.5518)\n"
     ]
    }
   ],
   "source": [
    "epocas = 100\n",
    "for ep in range(epocas):\n",
    "    # Paso A - Reiniciar los gradientes almacenados en el caso que existan\n",
    "    optimizador.zero_grad()\n",
    "    for i in range(20):\n",
    "        # Paso B - Calcular etiquetas seg煤n los pesos actuales\n",
    "        out = mlp(datos_input)\n",
    "        out.requires_grad_(True)\n",
    "        \n",
    "        # Paso C - Se calcula la funci贸n de loss.\n",
    "        loss = criterio(out, labels)\n",
    "\n",
    "        # Paso D - Ejecutar Backpropagation: Se calcula cuanto debe cambiar cada par谩metro.\n",
    "        # seg煤n el valor de la loss.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Paso E - Se actualizan los par谩metros.\n",
    "        optimizador.step()\n",
    "\n",
    "    if ((ep + 1) % 10) == 0:\n",
    "        print(\"Epoca: \", ep + 1, \"Loss: \", loss.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7619d-f0d2-4047-b56e-068881912f80",
   "metadata": {},
   "source": [
    "Se estudia el rendimiento en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f980596f-e9d7-40e8-804b-fa1a60e19851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte train : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        47\n",
      "         1.0       1.00      1.00      1.00        58\n",
      "         2.0       1.00      1.00      1.00        37\n",
      "\n",
      "    accuracy                           1.00       142\n",
      "   macro avg       1.00      1.00      1.00       142\n",
      "weighted avg       1.00      1.00      1.00       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt_test = data_transform.transform(data_test)\n",
    "\n",
    "X_test = dt_test[:, :-1]\n",
    "y_test = dt_test[:, -1:]\n",
    "\n",
    "f = lambda x: torch.argmax(mlp(x), dim=1)\n",
    "\n",
    "# Train error\n",
    "datos_input = torch.Tensor(X_train).float()\n",
    "preds_train = f(datos_input)\n",
    "\n",
    "print(\"Reporte train : \\n\", classification_report(y_train.reshape([-1,]), preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a74f3ff-2752-4b7d-850e-499e73d7957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte test : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        12\n",
      "         1.0       1.00      1.00      1.00        13\n",
      "         2.0       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Error\n",
    "datos_input = torch.Tensor(X_test).float()\n",
    "preds_test = f(datos_input)\n",
    "\n",
    "print(\"Reporte test : \\n\", classification_report(y_test.reshape([-1,]), preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7f35f-53ba-44f1-b7dc-6adaa2f5abc6",
   "metadata": {},
   "source": [
    "Viendo estos resultados, se puede decir que el clasificador entrenado fue un 茅xito."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bf7d3-b51b-41f9-92d2-d0c58b5bb833",
   "metadata": {},
   "source": [
    "Las redes neuronales pueden ser descritas como un modelo matem谩tico de procesamiento de informaci贸n. En general, una red neuronal puede considerarse como un sistema con las siguientes caracter铆sticas:\n",
    "\n",
    "1. El procesamiento de la informaci贸n ocurre en unidades llamadas neuronas.\n",
    "2. Las neuronas est谩n conectadas e intercambian informaci贸n (o se帽ales) por medio de sus conexiones.\n",
    "3. Las conexiones entre neuronas pueden ser fuertes o d茅biles, dependiendo de como se procesa la informaci贸n.\n",
    "4. Cada neurona tiene un estado interno determinado por todas las conexiones que posee.\n",
    "5. Cada neurina tiene una funci贸n de activaci贸n que opera sobre su estado, esta funci贸n determina la informaci贸n que se comparte a otras neuronas.\n",
    "\n",
    "En t茅rminos operativos, una red neuronal posee una **arquitectura** que describe el conjunto de conexiones entre neurona y un proceso de **aprendizaje** asociado, que describe el proceso entrenamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad14b1-142c-4412-b632-fbedc26aef0f",
   "metadata": {},
   "source": [
    "Las **neuronas** por tanto, pueden ser definidas por medio de la siguiente relaci贸n:\n",
    "$$\n",
    "y=f\\left(\\sum_{i} x_{i} w_{i}+b\\right)\n",
    "$$\n",
    "\n",
    "Ac谩 se hace el calcul贸 $w^t x + b = \\sum_i x_i w_i + b$ sobre los inputs $x_i$ y los pesos $w_i$. Estos 煤ltimos, son valores num茅ricos que representan las conexiones entre neuronas, el peso $b$ se denomina *bias*. Luego se calcula el resultado de aplicar la funci贸n de activaci贸n $f(\\cdot)$. Existen distintos tipos de funciones de activaci贸n dentro de estas se pueden nombrar:\n",
    "\n",
    "* $f(x)=x$ la funci贸n identidad.\n",
    "* $f(x)=\\left\\{\\begin{array}{l}1 \\text { if } x \\geq 0 \\\\ 0 \\text { if } x<0\\end{array}\\right.$ la funci贸n de activaci贸n de umbral.\n",
    "* $f(x)=\\frac{1}{1+\\exp (-x)}$ la funci贸n sigmoide logistica, es una de las m谩s utilizadas.\n",
    "* $f(x) =\\frac{1-\\exp (-x)}{1+\\exp (-x)}$ la funci贸n sigmoide bipolar, esta corresponde a una sigmoide escalada a $(-1,1)$.\n",
    "* $f(x) = \\frac{1-\\exp (-2 x)}{1+\\exp (-2 x)}$ la tangente hiperb贸lica. \n",
    "\n",
    "* $f(x)=\\left\\{\\begin{array}{l}x\\text { if } x \\geq 0 \\\\ 0 \\text { if } x<0\\end{array}\\right.$ La funci贸n de activaci贸n ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1f660-6374-44dc-a962-68584bc751ce",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "1. Existen variantes de la funci贸n de activaci贸n ReLU, investigue al menos 3 y compare sus diferencias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
