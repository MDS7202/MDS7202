{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 22: Selecci√≥n de Caracter√≠sticas, Reducci√≥n de Dimensionalidad y Ajuste de Hiperpar√°metros\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "**Profesor: Pablo Badilla**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T00:14:43.200431Z",
     "start_time": "2020-09-20T00:14:43.195445Z"
    }
   },
   "source": [
    "## Objetivos de esta clase\n",
    "\n",
    "- Comprender la importancia de seleccionar caracter√≠sticas y reducir dimensionalidad.\n",
    "- Seleccionar Caracter√≠sticas relevantes.\n",
    "- Reducci√≥n de dimensionalidad.\n",
    "- Integrar estas t√©cnicas con `Pipeline`\n",
    "- Buscar la mejor configuraci√≥n de hiperpar√°metros con `GridSearch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:34:37.165586Z",
     "start_time": "2020-09-22T14:34:36.094174Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"./resources/descriptores_musica.csv\")\n",
    "df = df.astype({\"time_signature\": str, \"key\": str, \"mode\": str})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problema de Hoy: üé∏ü§ò Caracterizaci√≥n Musical üéºüéµ \n",
    "\n",
    "    \n",
    "Los atributos son: \n",
    "\n",
    "- `key`: escala de la canci√≥n. 0 = C, 1 = C‚ôØ/D‚ô≠, 2 = D...  [Mas informaci√≥n](https://en.wikipedia.org/wiki/Pitch_class).\n",
    "- `modo`: 1 si la escala es mayor, 0 si es menor.\n",
    "- `time_signature`: cu√°ntos pulsos hay en cada comp√°s. (4, 3,...).\n",
    "- `loudness`: Volumen de la canci√≥n (rango -60, 0).\n",
    "\n",
    "\n",
    "- `acousticness`: Probabilidad de que la canci√≥n sea solo ac√∫stica. Valores cercanos a 1 indican que la canci√≥n es probablemente ac√∫stica.\n",
    "- `danceability`: Describe que tan bailable es la canci√≥n. Valores cercanos a 1 indican que la canci√≥n es muy bailable.\n",
    "- `energy`: Mide que tan energ√©tica es una canci√≥n. Mide cosas como la rapidez, el volumen y el ruido. Valores cercanos a 1 indican que la canci√≥n es muy en√©grica.\n",
    "- `instrumentalness`: Probabilidad que la canci√≥n contenga voces. 1 es muy probable que contenga voz.\n",
    "- `liveness`: Probabilidad de que la canci√≥n fuese grabada en vivo. 1 es muy probable que la canci√≥n haya sido grabada en vivo.\n",
    "- `speechiness`: Probabilidad de que la canci√≥n contenga palabras habladas (ejemplo: podcast : 1). \n",
    "- `valence`: Sentimiento de la canci√≥n (rango 0, 1). 1 -> felicidad, alegria, euforia. 0 -> Tristeza, enojo, depresi√≥n.\n",
    "- `tempo` : Pulsos por minuto de la canci√≥n (BPM). \n",
    "\n",
    "\n",
    "La variable a predecir es: \n",
    "\n",
    "- `genre`: G√©nero de la canci√≥n.\n",
    "\n",
    "\n",
    "**Pregunta**: A simple vista,\n",
    "\n",
    "- ¬øHay car√°cter√≠sticas que podr√≠an estas repetidas? (**Irrelevante**)\n",
    "- ¬øhay caracter√≠sticas que nos dicen mas o menos lo mismo? (**Redundante**)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "df = pd.read_csv(\"./resources/descriptores_musica.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ejemplo(idx):\n",
    "    \"\"\"\n",
    "    Obtiene un ejemplo y lo formatea como columna.\n",
    "    \"\"\"\n",
    "    ejemplo = (\n",
    "        df.loc[\n",
    "            idx,\n",
    "            [\n",
    "                \"danceability\",\n",
    "                \"energy\",\n",
    "                \"speechiness\",\n",
    "                \"acousticness\",\n",
    "                \"instrumentalness\",\n",
    "                \"valence\",\n",
    "                \"name\",\n",
    "                \"artist\",\n",
    "                \"genre\",\n",
    "            ],\n",
    "        ]\n",
    "        .to_frame()\n",
    "        .reset_index()\n",
    "    )\n",
    "    ejemplo.columns = [\"Descriptor\", \"Valor\"]\n",
    "    return ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pueden cambiar el √≠ndice de alguno de estos ejemplos para\n",
    "# mostrar otra canci√≥n en la visualizaci√≥n\n",
    "ejemplo1 = get_ejemplo(102)\n",
    "ejemplo2 = get_ejemplo(385)\n",
    "ejemplo3 = get_ejemplo(15)\n",
    "ejemplo4 = get_ejemplo(484)\n",
    "\n",
    "ejemplos = [ejemplo1, ejemplo2, ejemplo3, ejemplo4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spider/Radar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    specs=[\n",
    "        [{\"type\": \"polar\"}, {\"type\": \"polar\"}],\n",
    "        [{\"type\": \"polar\"}, {\"type\": \"polar\"}],\n",
    "    ],\n",
    "    subplot_titles=[\n",
    "        ejemplo1.loc[6, \"Valor\"],\n",
    "        ejemplo3.loc[6, \"Valor\"],\n",
    "        ejemplo2.loc[6, \"Valor\"],\n",
    "        ejemplo4.loc[6, \"Valor\"],\n",
    "    ],\n",
    ")\n",
    "\n",
    "for i, ejemplo in enumerate(ejemplos):\n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(\n",
    "            r=ejemplo.loc[0:5, \"Valor\"],\n",
    "            theta=ejemplo.loc[0:5, \"Descriptor\"],\n",
    "            fill=\"toself\",\n",
    "            name=f\"{ejemplo.loc[6, 'Valor']} - {ejemplo.loc[7, 'Valor']} ({ejemplo.loc[8, 'Valor']})\",\n",
    "        ),\n",
    "        col=i // 2 + 1,\n",
    "        row=i % 2 + 1,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),\n",
    "    showlegend=False,\n",
    "    title=\"ScatterPolar/Radar/Spider Chart/ Descripci√≥n de Ejemplos\",\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x=\"duration_ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x=\"loudness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x=\"tempo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_to_hists = df.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"valence\",\n",
    "        \"liveness\",\n",
    "        \"genre\",\n",
    "    ],\n",
    "].melt(id_vars=[\"genre\"], var_name=\"variable\", value_name=\"valor\")\n",
    "\n",
    "px.histogram(\n",
    "    dt_to_hists, x=\"valor\", color=\"variable\", facet_col=\"variable\", facet_col_wrap=4\n",
    ").update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from umap import UMAP\n",
    "\n",
    "projection_pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"Column Transformer\",\n",
    "            ColumnTransformer(\n",
    "                [(\"MinMax\", MinMaxScaler(), [\"duration_ms\", \"loudness\"])],\n",
    "                remainder=\"passthrough\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"Normalize\", Normalizer()),\n",
    "        (\"UMAP\", UMAP(random_state=88, n_neighbors=20, min_dist=0.15)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "projections = projection_pipe.fit_transform(\n",
    "    df.loc[\n",
    "        :,\n",
    "        [\n",
    "            \"danceability\",\n",
    "            \"energy\",\n",
    "            \"speechiness\",\n",
    "            \"acousticness\",\n",
    "            \"instrumentalness\",\n",
    "            \"valence\",\n",
    "            \"liveness\",\n",
    "            \"duration_ms\",\n",
    "            \"loudness\",\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "df_proj = pd.DataFrame(projections, columns=[\"x\", \"y\"])\n",
    "\n",
    "\n",
    "df_fig = df.copy()\n",
    "df_fig = pd.concat([df_fig, df_proj], axis=1)\n",
    "df_fig[\"hover_name\"] = df_fig[\"artist\"] + \" - \" + df_fig[\"name\"]\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_fig,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"genre\",\n",
    "    hover_name=\"hover_name\",\n",
    "    hover_data=[\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"valence\",\n",
    "    ],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivaci√≥n \n",
    "\n",
    "### Detalle Interesante 1: Correlaci√≥n entre las variables.\n",
    "\n",
    "\n",
    "$$corr (X, Y) = \\frac{1}{(s_{x} s_{y})} \\sum_{i=1}^{m} (x_i - \\overline{x})(y_i - \\overline{y}) $$\n",
    "\n",
    "\n",
    "Los valores varian entre -1 y 1\n",
    "\n",
    "- Positivo: Relaci√≥n directa: Crece una, crece la otra. Mientras mayor, mas similares son las variables.\n",
    "- Negativo: Relaci√≥n inversa: Crece una, decrece la otra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:34:38.631229Z",
     "start_time": "2020-09-22T14:34:38.584365Z"
    }
   },
   "outputs": [],
   "source": [
    "correlations_df = df[\n",
    "    [\n",
    "        \"danceability\",\n",
    "        \"energy\",\n",
    "        \"loudness\",\n",
    "        \"speechiness\",\n",
    "        \"acousticness\",\n",
    "        \"instrumentalness\",\n",
    "        \"liveness\",\n",
    "        \"valence\",\n",
    "        \"tempo\",\n",
    "    ]\n",
    "]\n",
    "correlations = correlations_df.corr()\n",
    "\n",
    "\n",
    "px.imshow(\n",
    "    correlations,\n",
    "    labels=dict(x=\"\", y=\"\", color=\"Correlation\"),\n",
    "    x=correlations_df.columns,\n",
    "    y=correlations_df.columns,\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    color_continuous_scale=\"Inferno\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detalle Interesante 2: Correlaci√≥n entre las variables de Entrada y la variable por Predecir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.loc[:, [\"genre\"]]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "\n",
    "encoded_labels_df = pd.DataFrame(encoded_labels, columns=encoder.get_feature_names_out())\n",
    "encoded_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# juntamos correlations_df con encoded_labels_df\n",
    "\n",
    "correlations_labels_df = pd.concat([correlations_df, encoded_labels_df], axis=1)\n",
    "\n",
    "correlations_labels = correlations_labels_df.corr(\"pearson\").iloc[9:, 0:9]\n",
    "\n",
    "px.imshow(\n",
    "    correlations_labels,\n",
    "    labels=dict(x=\"\", y=\"\", color=\"Correlation\"),\n",
    "    x=correlations_labels.columns,\n",
    "    y=correlations_labels.index,\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    color_continuous_scale=\"PRgn\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detalle Interesante 3: ¬øUsamos Artista?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"artist\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(\n",
    "    df,\n",
    "    \"artist\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artista_ohe = OneHotEncoder(sparse=False)\n",
    "artista_encoded = artista_ohe.fit_transform(df[[\"artist\"]])\n",
    "artista_cols = artista_ohe.get_feature_names_out()\n",
    "\n",
    "\n",
    "artista_df = pd.DataFrame(artista_encoded, columns=artista_cols)\n",
    "artista_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artista_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Esta transformaci√≥n produce 519 dimensiones. Una locura...**\n",
    "\n",
    "> **Pregunta:** ¬øLa cantidad de dimensiones influir√° en la calidad de clasificaci√≥n que logremos?\n",
    "\n",
    "Imag√≠nense ahora las correlaciones de cada una de estas viariables con respecto a la variable de salida."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maldici√≥n de la Dimensionalidad\n",
    "\n",
    "La maldici√≥n de la dimensionalidad es el problema que consiste en que a medida que aumentan las dimensiones, los datos tienden a hacerse cada vez m√°s *sparse*/escasos sobre las dimensiones en las cuales est√°n representados. Una simple analog√≠a para entender esto es que:\n",
    "\n",
    "> *A medida que aumenta la cantidad de features, aumenta el volumen en donde se encuentran los datos, haciendo que estos se separen bastante entre ellos. *\n",
    "\n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='./resources/curse.png' width=600/>\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://www.researchgate.net/figure/The-effect-of-the-curse-of-dimensionality-when-projected-in-1-one-dimension-2-two_fig3_342638066'> A comprehensive survey of anomaly detection techniques for high dimensional big data en Research Gate.</a>\n",
    "</div>\n",
    "\n",
    "\n",
    "Esto implica que, para poder seguir distinguiendo correctamente los datos, se debe aumentar masivamente su cantidad a medida que se aumentan las dimensiones.\n",
    "\n",
    "**¬øEn qu√© nos afecta esto?**\n",
    "\n",
    "Induce comunmente a una reducci√≥n del rendimiento de los clasificadores/regresores.\n",
    "\n",
    "\n",
    "M√°s en Wikipedia: [Curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejorando la Clasificaci√≥n\n",
    "\n",
    "\n",
    "Entonces, hasta ac√° tenemos 3 problemas: \n",
    "\n",
    "1. Hay caracter√≠sticas que \"aportan\" m√°s o menos la misma informaci√≥n.\n",
    "2. Hay caracter√≠sticas que podr√≠an no aportar informaci√≥n para poder clasificar correctamente.\n",
    "3. Tenemos una gran cantidad de dimensiones, lo cual podr√≠a entorpecer la clasificaci√≥n.\n",
    "\n",
    "\n",
    "Por ende, ser√≠a ideal eliminar un par de dimensiones con el fin de mejorar la clasificaci√≥n. \n",
    "\n",
    "\n",
    "Para comparar las mejoras, utilizaremos un **Baseline**, el cual no es m√°s que es un modelo inicial al cual , a medida que vayamos generando mejores modelos, nos iremos comparando (para ver si mejoramos y cuanto). Durante toda esta clase, nuestro **baseline** (modelo que compararemos) ser√° **Tree** entrenado con todas las features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:34:38.731466Z",
     "start_time": "2020-09-22T14:34:38.700222Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"Scale\",\n",
    "            MinMaxScaler(),\n",
    "            [\n",
    "                \"duration_ms\",\n",
    "                \"tempo\",\n",
    "                \"loudness\",\n",
    "                \"time_signature\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"One Hot Encoding\",\n",
    "            OneHotEncoder(sparse=False, handle_unknown=\"ignore\"),\n",
    "            [\n",
    "                \"key\",\n",
    "                \"mode\",\n",
    "                \"artist\",\n",
    "                \"time_signature\",\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Creamos nuestro baseline pipeline\n",
    "baseline_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"Preprocessing\", preprocessing),\n",
    "        (\"Tree\", DecisionTreeClassifier(random_state=42)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos **holdout** al dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = df.loc[:, \"genre\"]\n",
    "features = df.drop(columns=[\"genre\", \"id\", \"name\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=labels,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, definimos el ciclo de Entrenamiento y Evaluaci√≥n.\n",
    "La m√©trica con la cu√°l evaluaremos el desempe√±o del clasificador ser√° `f1_score`.\n",
    "\n",
    "Para esto, definiremos la funci√≥n `train_and_evaluate` que dado un pipeline y conjuntos de entrenamiento y prueba, entrena un clasificador y retorna su evaluaci√≥n:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    pipe, print_=True, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test\n",
    "):\n",
    "\n",
    "    # notar que los datasets son par√°metros por defecto.\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    if print_:\n",
    "        print(\"Matriz de confusi√≥n: \\n\")\n",
    "        print(confusion_matrix(y_test, y_pred, labels=pipe.classes_))\n",
    "        print(\"\\nReporte de Clasificaci√≥n: \\n\")\n",
    "        print(\n",
    "            classification_report(y_test, y_pred, target_names=pipe.classes_),\n",
    "        )\n",
    "\n",
    "    return f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "\n",
    "train_and_evaluate(baseline_pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta:‚ùì** ¬øC√≥mo s√© que mi modelo es mejor que uno que clasifica al azar?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modelos Dummy\n",
    "\n",
    "El [DummyClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier) es un clasificador que ignora todas las features de entrada y genera salidas aleatorias como respuesta a las predicciones.\n",
    "\n",
    "Permite saber si los modelos que estamos implementando son mejores que clasificar al azar. Por lo general, es una de las primeros chequeos que hacemos ya que permite anticipadamente saber si estamos generando modelos que aprenden o no.\n",
    "\n",
    "Para la regresi√≥n, existe [DummyRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"Preprocessing\", preprocessing),\n",
    "        (\"Tree\", DummyClassifier(strategy=\"stratified\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_and_evaluate(dummy_pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-------------\n",
    "\n",
    "## Selecci√≥n de atributos\n",
    "\n",
    "\n",
    "Entonces, recordando los 3 problemas que hab√≠amos mencionado previamente: \n",
    "\n",
    "1. Hay caracter√≠sticas que \"aportan\" m√°s o menos la misma informaci√≥n.\n",
    "2. Hay caracter√≠sticas que podr√≠an no aportar informaci√≥n para poder clasificar correctamente.\n",
    "3. Tenemos una gran cantidad de dimensiones, lo cual podr√≠a entorpecer la clasificaci√≥n.\n",
    "\n",
    "La idea es aplicar m√©todos de selecci√≥n de caracter√≠sticas para que, de forma automatizada, se encuentre un subconjunto de atributos que mejoren los resultados de los modelos de clasificaci√≥n/regresi√≥n implementados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheme independent o M√©todo de Filtro .\n",
    "\n",
    "\n",
    "Compara las caracter√≠sticas con las etiquetas a trav√©s de test estad√≠sticos simples e ignora la relaci√≥n entre las caracter√≠sticas en si. Para ocupar este tipo de t√©cnicas se requiere una m√©trica y una estrategia\n",
    "\n",
    "#### M√©tricas\n",
    "\n",
    "- Varias m√©tricas para clasificaci√≥n: \n",
    "    - Anova (`f_classif`). La idea es calcular un estad√≠stico F-score, el cu√°l indica que tan f√°cil es para un atributo distinguir entre clases. M√°s informaci√≥n [aqu√≠](https://datascience.stackexchange.com/questions/74465/how-to-understand-anova-f-for-feature-selection-in-python-sklearn-selectkbest-w).\n",
    "    - Mutual information (`mutual_info_classif`) es un estad√≠stico que mide la independencia entre dos variables aleatorias. 0 indica independencia entre variables. Valores m√°s altos indican mayor depencia. En general da mejores resultados que `f_classif`, pero su implementaci√≥n es m√°s lenta.\n",
    "    - Chi squared (`chi2`) realiza un test estad√≠stico $\\chi^2$ que, al igual que la funci√≥n anterior, mide la dependencia entre distintas variables. Sirve solo con variables categ√≥ricas (en OneHot) o conteos (como Bag of Words).\n",
    "    \n",
    "- Para regresi√≥n, se usan otro tipo de m√©tricas especializadas en ellas.\n",
    "- Referencia: [Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection).\n",
    "\n",
    "#### Estrategias\n",
    "\n",
    "Definen como se seleccionar√°n las mejores caracter√≠sticas: \n",
    "\n",
    "- `SelectKBest` selecciona las features con los mejores valores. Hay que especificar el n√∫mero de features que seleccionaremos.\n",
    "\n",
    "- `SelectPercentile` selecciona el porcentaje con mejores valores. Hay que especificar con cuanto porcentaje quedarse.\n",
    "\n",
    "#### Integrar la Selecci√≥n de Caracter√≠sticas al Pipeline\n",
    "\n",
    "La b√∫squeda de mejores caracter√≠sticas se realiza al momento de entrenar un pipeline. Luego, al momento de predecir, el selector de caracter√≠sticas simplemente descarta las caracter√≠sticas no utilizadas antes de pasar a la siguiente etapa. El siguiente ejemplo muestra lo anteriormente dicho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:34:39.000932Z",
     "start_time": "2020-09-22T14:34:38.969690Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    SelectPercentile,\n",
    "    f_classif,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "\n",
    "# Creamos nuestro baseline pipeline\n",
    "selection_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"Preprocessing\", preprocessing),\n",
    "        # Conservamos el 70% mejor seg√∫n la m√©trica seleccionada\n",
    "        (\"Selection\", SelectPercentile(f_classif, percentile=70)),\n",
    "        (\"Tree\", DecisionTreeClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_and_evaluate(selection_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro baseline pipeline\n",
    "selection_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"Preprocessing\", preprocessing),\n",
    "        (\"Selection\", SelectPercentile(f_classif, percentile=20)),\n",
    "        (\"Tree\", DecisionTreeClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_and_evaluate(selection_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro baseline pipeline\n",
    "selection_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"Preprocessing\", preprocessing),\n",
    "        (\"Selection\", SelectPercentile(mutual_info_classif, percentile=20)),\n",
    "        (\"Tree\", DecisionTreeClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_and_evaluate(selection_pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta: ‚ùì** ¬øC√≥mo elegir el mejor porcentaje de features por conservar? ¬øY la m√©trica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_pipeline.steps[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_pipeline.steps[1][1].percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = []\n",
    "for i in range(10, 101, 10):\n",
    "    selection_pipeline.steps[1][1].percentile = i\n",
    "    f1.append([i, train_and_evaluate(selection_pipeline, print_=False)])\n",
    "f1 = np.array(f1)\n",
    "\n",
    "px.line(\n",
    "    x=f1[:, 0],\n",
    "    y=f1[:, 1],\n",
    "    title=\"F1 seg√∫n cantidad de Features Conservadas\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "\n",
    "Si bien el ciclo anterior nos permiti√≥ encontrar el mejor valor para el porcentaje de la selecci√≥n de atributos, es bastante trabajo implementarlo, pensando m√°s a√∫n que comunmente se quieren optimizar varias partes del pipeline y no solo un paso en espec√≠fico.\n",
    "\n",
    "En el caso anterior, un ejemplo de esto podr√≠a ser variar el porcentaje como la m√©trica usada, teniendo una malla de b√∫squeda del estilo:\n",
    "\n",
    "| `f_classif` | `mutual_info_classif` |\n",
    "|---|---|\n",
    "| 10 | 10 |\n",
    "| 20 | 20 |\n",
    "| 30 | 30 |\n",
    "| 40 | 40 |\n",
    "| 50 | 50 |\n",
    "| 60 | 60 |\n",
    "| 70 | 70 |\n",
    "| 80 | 80 |\n",
    "| 90 | 90 |\n",
    "| 100 | 100 |\n",
    "\n",
    "\n",
    "Por esto, la idea es tener un mecanismo para el cual podamos pasarle una lista de hiperpar√°metros, que este lo pruebe todos y que retorne el mejor modelo. \n",
    "Este mecanismo en `scikit-learn` es conocido como B√∫squeda de grilla o `Grid-search`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar los Par√°metros Disponibles para Modificar\n",
    "\n",
    "En general, cualquier clase de scikit-learn implementa la funci√≥n `get_params`, la cual muestra los par√°metros disponibles para probar y modificar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectPercentile().get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de una `Pipeline`, muestra las `steps` de la pipeline m√°s los par√°metros de cada una de las steps. \n",
    "Noten que los par√°metros de cada `step` siguen la notaci√≥n: `{nombre_step}__{par√°metro_step}`\n",
    "\n",
    "Ejemplo: N√∫mero de porcentajes que escogeremos - `Selection__percentile`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"Preprocessing\", preprocessing),\n",
    "        (\"Selection\", SelectPercentile(f_classif, percentile=20)),\n",
    "        (\"Tree\", DecisionTreeClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "selection_pipeline.get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es definir un grilla de hiperpar√°metros para que GridSearch los explore y elija el mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\"Selection__percentile\": range(10, 101, 5)}\n",
    "]\n",
    "param_grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y luego invocar GridSearch con el Pipeline, la grilla de hiperpar√°metros, la m√©trica (pueden ver las m√©tricas disponibles [aqu√≠](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)) y la cantidad de cores de su CPU que deseen usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(selection_pipeline, param_grid, n_jobs=-1, scoring=\"f1_macro\")\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de la exploraci√≥n de grilla los pueden visualizar as√≠: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de Uso para Predicci√≥n de GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sacamos el vector de los atributos desde las features\n",
    "gs.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta ‚ùì**: ¬øY si ahora quiero usar otra m√©trica univariada para seleccionar atributos?\n",
    "\n",
    "Por ejemplo, [`Mutual information`](https://en.wikipedia.org/wiki/Mutual_information)\n",
    "Vamos nuevamente a buscar el nombre del atributo que queremos modificar y ejecutamos nuevamente grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"Selection__percentile\": range(5, 101, 5),\n",
    "        \"Selection__score_func\": [f_classif, mutual_info_classif],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(selection_pipeline, param_grid, n_jobs=-1, scoring=\"f1_macro\")\n",
    "train_and_evaluate(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Grid Search` y `CV` (Cross Validated)\n",
    "\n",
    "`Grid Search` recibe como par√°metro `X_train` y `y_train` y (a diferencia de la funci√≥n que definimos en el baseline) entrena un modelo usando cross validation sobre todos los par√°metros. \n",
    "\n",
    "<center>\n",
    "    <img src='./resources/kfold.png' width=400/>\n",
    "</center>\n",
    "\n",
    "Es decir, por cada fracci√≥n del Cross Validation entrena el modelo y luego promedia los scores para obtener el score final de la configuraci√≥n de hiperpar√°metros que estaba probando.\n",
    "\n",
    "\n",
    "\n",
    "Una vez terminada la b√∫squeda de los hiperpar√°metros que generan el mejor modelo sobre los k-folds, **por defecto**, selecciona el mejor modelo encontrado **y entrena con todos los datos utilizados.**\n",
    "\n",
    "**ESTO PUEDE CAUSAR DATA LEAKAGES**. \n",
    "\n",
    "Por eso es importante usar `GridSearchCV` con `X_train` y `y_train` y no con todo el dataset (aunque dentro de este se ejecute un Cross Validation). De todas formas, ustedes pueden controlar este comportamiento a trav√©s del par√°metro `refit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta ‚ùì**: ¬øPodemos entonces tambi√©n cambiar el clasificador y probar varios tipos?\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probando con m√°s clasificadores \n",
    "\n",
    "Efectivamente, la notaci√≥n permite generar distintas grillas de b√∫squeda para distintos clasificadores a trav√©s de la definici√≥n de distintos diccionarios dentro de la lista que se le provee a GridSearch:\n",
    "\n",
    "\n",
    "```\n",
    "[\n",
    "    # grilla 1\n",
    "    {\n",
    "     'modelo' : [Modelo1()],\n",
    "     'modelo__param1': ...\n",
    "    },\n",
    "    # grilla 2\n",
    "    {\n",
    "     'modelo' : [Modelo1()],\n",
    "     'modelo__param1': ...\n",
    "    }, \n",
    "    ...\n",
    "\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessing),\n",
    "        (\"selection\", SelectPercentile(f_classif)),\n",
    "        (\"model\", KNeighborsClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    # grilla 1: tree\n",
    "    {\n",
    "        \"selection__percentile\": range(10, 101, 10),\n",
    "        \"model\": [DecisionTreeClassifier()],\n",
    "        \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "    },\n",
    "    # grilla 2: knn\n",
    "    {\n",
    "        \"selection__percentile\": range(10, 101, 10),\n",
    "        \"model\": [KNeighborsClassifier()],\n",
    "        \"model__n_neighbors\": [2, 4, 5, 10],\n",
    "    },\n",
    "    # grilla 3: random forest\n",
    "    {\n",
    "        \"model\": [RandomForestClassifier()],\n",
    "        \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"model__bootstrap\": [True, False],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 = GridSearchCV(pipe, grid, n_jobs=-1, scoring=\"f1_macro\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "## Reducci√≥n de la dimensionalidad\n",
    "\n",
    "T√©cnicas que reducen el n√∫mero de caracter√≠sticas de forma no supervisadas.\n",
    "\n",
    "- Eliminan ruido.\n",
    "- Pueden mejorar el rendimiento de los modelos, sobre todo si se tienen muchas dimensiones.\n",
    "- Pueden proyectar datos en dos/tres dimensiones.\n",
    "\n",
    "**Problemas** con algoritmos de reducci√≥n de dimensionalidad: \n",
    "\n",
    "\n",
    "- Las dimensiones ya no son intepretables.\n",
    "\n",
    "\n",
    "\n",
    "### Principal Component Analysis\n",
    "\n",
    "\n",
    "Reduce dimensiones de nuestro dataset tratando de no perder mucha informaci√≥n.\n",
    "\n",
    "Objetivo. Encontrar direcci√≥nes tales que al proyectar nuestros datos, la varianza de los puntos proyectados se maximize."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='./resources/pca.jpg' width=600/>\n",
    "    </center>\n",
    "    \n",
    "<center>\n",
    "Fuente: <a href='https://devopedia.org/principal-component-analysis'>https://devopedia.org/principal-component-analysis</a>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:37:49.247462Z",
     "start_time": "2020-09-22T14:37:49.147222Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"energy\", y=\"loudness\", color=\"genre\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:37:49.269598Z",
     "start_time": "2020-09-22T14:37:49.247462Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:37:49.385453Z",
     "start_time": "2020-09-22T14:37:49.269598Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "components = pca.fit_transform(df[[\"energy\", \"loudness\"]])\n",
    "labels_ = {\n",
    "    [\"x\", \"y\"][i]: f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=-components[:, 0], y=-components[:, 0], color=df[\"genre\"], labels=labels_\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varianza explicada\n",
    "\n",
    "**¬øCu√°nta informaci√≥n mantenemos despu√©s de ejecutar PCA?**\n",
    "\n",
    "Cada componente explica una cierta cantidad de varianza de los datos.\n",
    "Esto esta determinado por los autovalores $\\lambda$\n",
    "\n",
    "Los componentes con mayor varianza incuir√°n mayor informaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:37:49.544764Z",
     "start_time": "2020-09-22T14:37:49.385453Z"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "components = pca.fit_transform(preprocessing.fit_transform(features))\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "labels_ = [\n",
    "    f\"PC {i+1} = {var:.1f}%\"\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "]\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    color=df[\"genre\"],\n",
    "    dimensions=range(n_components),\n",
    "    labels=labels_,\n",
    "    title=f\"Varianza Explicada Total: {total_var:.2f}%.<br>Varianza por Componente: {labels_}\",\n",
    "    height=800,\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¬øC√≥mo encontrar la mejor cantidad de dimensiones?\n",
    "\n",
    "Vamos viendo cuanta varianza acumula cada componente.\n",
    "\n",
    "En alg√∫n punto, la varianza marginal que acumula el siguiente es mucho mas baja que la anterior. \n",
    "Ese es el punto indicado en donde cortar.\n",
    "\n",
    "\n",
    "Esto lo podemos ver en el siguiente gr√°fico:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:37:49.645209Z",
     "start_time": "2020-09-22T14:37:49.544764Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(preprocessing.fit_transform(features))\n",
    "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "px.area(\n",
    "    x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "    y=exp_var_cumul,\n",
    "    labels={\"x\": \"N√∫mero de componentes\", \"y\": \"Varianza explicada\"},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas o menos en los 8 componentes principales ya se explican mas del 90% de la varianza del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:37:49.725459Z",
     "start_time": "2020-09-22T14:37:49.665262Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos nuestro baseline pipeline\n",
    "\n",
    "pca_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"Preprocessing\", preprocessing),\n",
    "        (\"Reduccion\", PCA()),\n",
    "        (\"Tree\", DecisionTreeClassifier(random_state=42)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\"Reduccion__n_components\": [10, 20, 30, 40, 50, 100, 200, 300, 400]}\n",
    "]\n",
    "gs_pca = GridSearchCV(pca_pipeline, param_grid=param_grid, scoring='f1_weighted', n_jobs=-1)\n",
    "\n",
    "train_and_evaluate(gs_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_pca.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mejoras a GridSearch\n",
    "\n",
    "#### `HalvingGridSearchCV`\n",
    "\n",
    "\n",
    "![](./resources/halvinggscv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "selection_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessing),\n",
    "        (\"selection\", SelectPercentile(f_classif, percentile=20)),\n",
    "        (\"model\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"selection__percentile\": range(5, 101, 10),\n",
    "        \"selection__score_func\": [f_classif, mutual_info_classif],\n",
    "        \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"model__bootstrap\": [True, False],\n",
    "    }\n",
    "]\n",
    "\n",
    "hgs = HalvingGridSearchCV(selection_pipeline, param_grid, n_jobs=-1, scoring='f1_macro')\n",
    "\n",
    "train_and_evaluate(hgs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Proyectos Relacionados\n",
    "\n",
    "https://scikit-learn.org/stable/related_projects.html\n",
    "\n",
    "- `scikit-optimize`: A library to minimize (very) expensive and noisy black-box functions. It implements several methods for sequential model-based optimization, and includes a replacement for GridSearchCV or RandomizedSearchCV to do cross-validated parameter search using any of these strategies.\n",
    "\n",
    "- `sklearn-deap` Use evolutionary algorithms instead of gridsearch in scikit-learn.\n",
    "\n",
    "\n",
    "- [`optuna`](https://optuna.readthedocs.io/en/stable/index.html): Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. Thanks to our define-by-run API, the code written with Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
