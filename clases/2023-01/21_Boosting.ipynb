{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f97e3ccc-bc09-45f0-bfe8-1c9934f802a0",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines (GBM)\n",
    "\n",
    "las maquinas de gradient boosting son algoritmos que se volvieron muy populares con la aparici√≥n de XGBoost en el 2016 para las tareas de clasificaci√≥n y regresi√≥n. En t√©rminos de funcionamiento, estos algoritmos utilizan multiples \"weak-learners\" con los que a trav√©s de multiples iteraciones corrigen los errores que presentan sus regresores/clasificadores en etapas anteriores.\n",
    "\n",
    "\t\t‚ùì Pregunta: ¬øQue caracteristicas tienen los \"weak-learners\"?\n",
    "\n",
    ">> 1.  El objetivo no es crear clasificadores potentes con ellos, si no como su nombre lo dice queremos generar \"aprendices d√©biles\" que se potencien con otros.\n",
    ">> 2. En general se utilizan como weak learners arboles de decisi√≥n por su simpleza y versatilidad que nos ofrecen para regresi√≥n y clasificaci√≥n.\n",
    ">> 3. Pueden ser entrenados con una muestra de la totalidad para obtener entrenamientos r√°pidos sobre ellos.\n",
    ">> 4. Su uso en un algoritmo implica la comprensi√≥n de estos elementos, ya que se heredan sus problemas en una estructura mas compleja.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74ef55d0-679c-40f4-93fc-d861ad65f08d",
   "metadata": {},
   "source": [
    "‚ùì Pregunta: ¬øQue problemas tenian los arboles de decisi√≥n?\n",
    "![3 Techniques to Avoid Overfitting of Decision Trees | by Satyam Kumar |  Towards Data Science](https://miro.medium.com/v2/resize:fit:1702/1*KbRVJC5B0EgO8YAyeCrLkA.png)\n",
    "\n",
    "Recordar que los √°rboles de decisi√≥n son elementos que tienen una alta facilidad de sobre-ajustarse si aumentamos la profundidad o n√∫mero de hijos por nodo. Esto puede causar problemas en un algoritmo de GBM y por ello, este debe ser un valor que debemos controlar de forma directa o indirectamente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a42f5ff-80b5-490a-924e-5fef5dfc8102",
   "metadata": {},
   "source": [
    "Adentr√°ndonos mas en detalles en el algoritmo de gradient boosting, estos forman parte de los algoritmos de ensemble, quienes basicamente son algoritmos que combinan multiples modelos para el proceso de predicci√≥n. Una forma de esto es a traves de modelos aditivos, donde entrenando multiples algoritmos simples de forma independiente, utilizamos la suma de sus salidas para generar un modelo mas potente para la regresi√≥n/clasificaci√≥n:\n",
    "$$F_M(x) = f_1(x)+...+f_M(x)=\\sum_{m=1}^M f_m(x)$$\n",
    "![Develop an Intuition for How Ensemble Learning Works -  MachineLearningMastery.com](https://machinelearningmastery.com/wp-content/uploads/2020/07/Example-of-Combining-Decision-Boundaries-Using-an-Ensemble.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e28f15-95f2-4125-87f6-bdfa13312dc2",
   "metadata": {},
   "source": [
    "### GBM: Regresi√≥n\n",
    "\n",
    "La idea que sigue por detras las GBM es realizar descenso del gradiente a una funci√≥n $\\hat{f}$ en el espacio de funciones. O sea, si utilizamos un conjunto $f$ de funciones deseamos disminuir, todo lo posible la funci√≥n de error que representa la funci√≥n de perdida ($\\mathcal{L}$) durante el entrenamiento. Formalmente:\t\n",
    "$$\\hat{f}=argmin_f \\, \\mathcal{L}(f)$$\n",
    "donde $f$:\n",
    "$$f = (f(x_1), ..., f(x_N))$$\n",
    "Donde en cada paso $m$ (n√∫mero de la iteraci√≥n) generaremos un gradiente $g_m$ de la funci√≥n $\\mathcal{L(f_m)}$ :\n",
    "$$g_m = [\\dfrac{\\partial{l}(y_i,f(x_i))}{\\partial{f}(x_i)}]_{f=f_{m-1}}$$\n",
    "En cada una de las iteraciones del modelo $g_m$ se encargara de actualizar el valor de $f_{m-1}$, de tal forma de generar la funci√≥n $f_m$ de la iteraci√≥n. $g_m$ es la expresi√≥n matematica que contiene la informaci√≥n de los errores de la predicci√≥n anterior que deseamos corregir en cada iteraci√≥n. \n",
    "\n",
    "Luego, para comprender la actualizaci√≥n situ√©monos en el paso $m$, en el tendremos:\n",
    "$$f_m = f_{m-1} - \\beta_{m}g_{m}$$\n",
    "Donde $\\beta_m$ se define como el largo del paso en la correcci√≥n. Visto de otra forma, podemos interpretar esta valor como un factor de aprendizaje que podremos fijar o optimizar en la funci√≥n, se√±alando cuanta correcci√≥n aplicaremos del proceso anterior en el proceso actual. \n",
    "\n",
    "Si bien en cada una de las iteraciones encontraremos un valor optimo para $f_m$ y $N$ puntos, esta por si sola no genera un clasificador global capaz de obtener mejores generalizaciones, por esta raz√≥n necesitamos definir una nueva funci√≥n con la misma idea pero que en cada iteraci√≥n se haga mas robusta. En esta modificaci√≥n, el algoritmo para ajustar los weak-learners a una aproximaci√≥n de la se√±al negativa del gradiente, donde en cada iteraci√≥n entrenara un nuevo weak learner, quedando la actualizaci√≥n de la funci√≥n como:\n",
    "$$F_m = argmin_F \\sum_{i=1}^N (-g_{im}-F(x_i))^2$$\n",
    "Notar que $F(x_i)$ es un weak learner que se entrena en cada una de las iteraciones de nuestro algoritmo de GBM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0106ab54-77ce-4806-994c-7a4dc0020537",
   "metadata": {},
   "source": [
    "Finalmente el algoritmo se ver√≠a como:\n",
    "\n",
    "![](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/21_Ensamblaje/Pasted%20image%2020230527153450.png?raw=true)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ceffe09-45a9-4009-9131-c5f0c4a260b6",
   "metadata": {},
   "source": [
    "### GBM: Clasificaci√≥n\n",
    "\n",
    "\t\t‚ùì Pregunta: ¬øDeber√≠a ser diferente la clasificaci√≥n a la regresi√≥n?\n",
    "\n",
    "La respuesta directa a esto es \"s√≠\". Los cambios que presenta la clasificaci√≥n respecto a la regresi√≥n se dan netamente en la elecci√≥n de la funci√≥n de perdida ($\\mathcal{L}$) y algunas consideraciones que se toman para las salidas de los nodos.\n",
    "\n",
    "Respecto a las funciones de perdida, esto se debe a como se define el problema de clasificaci√≥n supervisada, donde nuestro inter√©s no ser√° la reducci√≥n el error que tenemos del valor estimado respecto a un valor continuo. En su reemplazo son utilizadas funciones de perdida para clasificaci√≥n, quienes tienen como principal objetivo disminuir el error predecir una etiqueta. Una de las funciones de perdidas mas conocidas/utilizadas para el problema de clasificaci√≥n se encuentra la logistic-loss/cross-entropy:\n",
    "$$\\mathcal{L}_i = -(y_i log(p_i) + (1-y_i)log(1-p_i))$$\n",
    "\t‚ùì Pregunta:  ¬øComo podemos interpretar esta funci√≥n de perdida?\n",
    "\t\n",
    "Donde su derivada es:\n",
    "$$\\dfrac{\\partial \\mathcal{L}_i}{\\partial \\hat{y}}=p_i-y_i$$\n",
    "El segundo punto a considerar es que la estimaci√≥n de $y$ sera igual a:\n",
    "$$\\hat{y}=log(odds)=log(\\dfrac{p}{1-p})$$\n",
    "\t\t‚ùì Pregunta:  ¬øPor que usamos odds?\n",
    "\t\t\n",
    "B√°sicamente debido a que los modelos de clasificaci√≥n con GBM son modelos de regresi√≥n, por lo que sus salidas no pueden ser consideradas como probabilidades, sino como scores que debemos transformar y normalizar.\n",
    "\n",
    "Finalmente para calcular las probabilidades de las salidas de nuestro GBM tendremos que calcular la funci√≥n softmax de $\\hat{y}$, la que viene dada por:\n",
    "$$\\text{softmax}(\\hat{y})=\\dfrac{1}{1+e^{-\\hat{y}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a3d69-63da-47bc-8468-c5517cec4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "p = 0.99\n",
    "y = 0\n",
    "-(y*math.log(p)+(1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d66a4-9905-40d6-9501-4b58f59d469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.9\n",
    "y = 1\n",
    "-(y* math.log(p) + (1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36f6af-cde4-4944-b2c8-07b2df783e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.3\n",
    "y = 0\n",
    "-(y* math.log(p) + (1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30553b-8745-4aa9-a250-1d9702ede7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35087c5c-1382-4698-bfc5-b28334ea2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis1, prob = [], []\n",
    "for i in range(1, 10):\n",
    "    y = 0\n",
    "    lis1.append(-(y* math.log(i/10) + (1-y)*math.log(1-i/10)))\n",
    "    prob.append(i/10)\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "px.line(prob, lis1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4753bb7e-4280-47c2-bae5-c3990a0ad0b9",
   "metadata": {},
   "source": [
    "### Retrospectiva\n",
    "\n",
    "\t\t‚ùì Pregunta: ¬øPero como podriamos simplificar estas definiciones que acabamos de dar?\n",
    "\n",
    "Si visualizamos GBM en un juego de golf, tendriamos un jugador que comienza con un tiro $f_0$, el cual a medida que va realizando cada tiro va aplicando una correcci√≥n $\\Delta_m$ en cada una de sus jugadas. Con esto, si cerebralmente el jugador corrigue sus errores a traves de una funci√≥n MSE, tendremos que el jugador ira optimizando esta funci√≥n hasta meter la pelota en el agujero (minimo de la funci√≥n de perdida).\n",
    "\n",
    "$\\text{MSE LOSS} = \\mathcal{L}(y, F_M(X))=\\dfrac{1}{N} \\sum_{i=1}^N (y_i - F_M(x_i))^2$\n",
    "\n",
    "![](https://explained.ai/gradient-boosting/images/golf-MSE.png)\n",
    "\n",
    "\t\t‚ùì ¬øComo podr√≠a el golfista correguir de mejor forma sus tiros? ¬øexiste alguna forma?.\n",
    "\t\t\n",
    "![1d-vectors.png](https://explained.ai/gradient-boosting/images/1d-vectors.png)\n",
    "\n",
    "Como vimos durante el entrenamiento de GBM, este proceso considera un $\\nu$ conocido como shrinkage rate o learning rate. Este valor nos permitir√° realizar correcciones en las actualizaciones de nuestras funciones $f_m$, ya que podr√≠a darse el caso que el gradiente obtenido durante la optimizaci√≥n sea muy alto y nos permita obtener un minimo adecuado para el problema. De esta forma:\n",
    "$$f_m(x) = f_{m-1}(x) + \\nu F_m(x)$$\n",
    "Otra forma podr√≠a ser cambiando la funci√≥n de perdida que definimos en el problema. Dependiendo si es un problema de clasificaci√≥n o regresi√≥n existen multiples funciones de perdida con diferentes interpretaciones que pueden mejor significativamente el problema que deseamos resolver. Algunas de estas son:\n",
    "\n",
    "| Nombre         | Loss                         | Derivada           |  \n",
    "| -------------- | ---------------------------- | ------------------ |\n",
    "| Squeared Error | $\\dfrac{1}{2}(y_i-f(x_i))^2$ | $y_i -f(x_i)$      |\n",
    "| Absolute Error | $y_i - f(x_i)$               | $sgn(y_i -f(x_i))$ |\n",
    "| Binary Logloss                | $log(1+e^{-y_if_i})$  | $y-\\sigma(2f(x_i))$    |\n",
    "\n",
    "\n",
    "En tercer lugar tenemos la adici√≥n de un regularizador a nuestra funci√≥n de perdida, de esta forma podremos disminuir el sobreajuste de nuestro modelo, generando peque√±as modificaciones en los arboles que vamos generando.\n",
    "$$\\mathcal{L}(f) = \\sum_{i=1}^Nl(y_i,f(x))+\\Omega(f))$$\n",
    "donde $\\Omega$:\n",
    "$$\\Omega(f)= \\gamma J+\\dfrac{1}{2}\\lambda \\sum_{j=1}^J w^2_j$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d41b7e14-270b-4467-924a-10189151b12d",
   "metadata": {},
   "source": [
    "### Pros & Cons\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- Capacidad para manejar tanto problemas de regresi√≥n como de clasificaci√≥n.\n",
    "- Puede capturar relaciones no lineales y caracter√≠sticas complejas en los datos.\n",
    "- Adaptabilidad a diferentes tipos de predictores, incluyendo variables num√©ricas y categ√≥ricas.\n",
    "- Robustez ante valores at√≠picos y datos ruidosos.\n",
    "- Capacidad para manejar grandes conjuntos de datos y escalabilidad.\n",
    "- Flexibilidad en la elecci√≥n de funciones de p√©rdida y m√©tricas de evaluaci√≥n.\n",
    "- Capacidad para manejar caracter√≠sticas faltantes y utilizar todas las observaciones disponibles.\n",
    "- Puede generar importancia de variables para ayudar en la interpretaci√≥n y selecci√≥n de caracter√≠sticas.\n",
    "\n",
    "**Contras**:\n",
    "\n",
    "- Mayor complejidad y tiempo de entrenamiento en comparaci√≥n con algoritmos m√°s simples.\n",
    "- Puede ser propenso a sobreajuste si no se controlan los hiperpar√°metros adecuadamente.\n",
    "- Requiere una configuraci√≥n cuidadosa de los hiperpar√°metros para obtener el mejor rendimiento.\n",
    "- Interpretaci√≥n m√°s dif√≠cil debido a la naturaleza de combinaci√≥n de m√∫ltiples √°rboles.\n",
    "- Sensible a datos desequilibrados, donde las clases minoritarias pueden no recibir suficiente atenci√≥n.\n",
    "- La importancia de las variables puede verse sesgada hacia las variables num√©ricas o con mayor cardinalidad.\n",
    "- No es adecuado para problemas con alta dimensionalidad o con muchos predictores.\n",
    "- Mayor consumo de recursos computacionales en comparaci√≥n con algoritmos m√°s simples.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bac118e-ce1c-438a-891d-ea51090f5278",
   "metadata": {},
   "source": [
    "## Ejemplo a Mano ü§®\n",
    "\n",
    "Como ya sabemos como funciona port detras un algoritmo simple de Gradient Boosting, veamos un ejemplo para ejecutar un peque√±o GBM a mano. Para esto consideremos los siguientes datos:\n",
    "\n",
    "| index | $feature_1$ | $feature_2$ | y     |  \n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1     | 1.12  | 1.4   | 1     |   \n",
    "| 2     | 5.45  | 3.1   | 0     |   \n",
    "| 3     | 3.54  | 1.2   | 1     |   \n",
    "\n",
    "Considerando los datos construir un modelo de GBM con 2 estimadores y un $\\nu$ igual a $0.1$.\n",
    "\n",
    "**Desarrollo:**\n",
    "Con la tabla anterior, comenzamos realizando nuestra primera iteraci√≥n (m=1), para ello se considera que todos los datos se encuentran en el mismo nodo. De esta forma, calculando los odds y los log(odds) de los datos tendremos:\n",
    "\n",
    "$$odds = \\dfrac{\\text{cantidad de casos exitosos}}{\\text{cantidad de casos fallidos}}$$\n",
    "$$odds = 2/1 = 2$$\n",
    "Luego para los $log_n(odds)$:\n",
    "$$log_n(odds)=log_n(2)=0,693$$\n",
    "Notar que debido a que todos los valores estan en el mismo nodo, todos los valores poseeran el mismo log_n(odds). Luego calculamos las probabilidades para cada uno de los valores.\n",
    "$$p=\\dfrac{1}{1+e^{-\\hat{y}}}=\\dfrac{1}{1+e^{-0.693}}=0.67$$\n",
    "Finalmente $\\gamma$ sera:\n",
    "$$\\gamma_1=y-p_1=1-0.67=0.33$$\n",
    "$$\\gamma_2=y-p_2=0-0.67=-0,67$$\n",
    "$$\\gamma_3=y-p_3=1-0.67=0.33$$\n",
    "Tenemos nuestro primer estimador calculado, ahora el siguiente paso es generar un nuevo √°rbol. Para esto vamos a generar un √°rbol donde el nodo padre tiene la regla $feature_2>2$.\n",
    "\n",
    "![](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/21_Ensamblaje/Flowchart%20Template.jpg?raw=true)\n",
    "\n",
    "Considerando los resultados obtenidos en la iteraci√≥n anterior, tendremos que ahora los valores de $\\gamma_i$ estan dados por:\n",
    "$$\\gamma_{i}=\\dfrac{\\sum_i y_i-p_i}{\\sum_i p_i(1-p_i)}$$\n",
    "Reemplazando cada valor:\n",
    "$$\\gamma_{1m}=\\dfrac{0.33+0.33}{2*(0.67(1-0.67))}=1.49$$\n",
    "$$\\gamma_{2m}=\\dfrac{-0.67}{0.67(1-0.67)}=-3.03$$\n",
    "Finalmente, con los valores obtenidos calculamos el $F_m$ resultante de las 2 iteraciones para cada una de las hojas:\n",
    "\n",
    "$$F_m(x)=F_{m-1}(x)+\\nu \\gamma_m$$\n",
    "Reemplazamos para cada uno de los valores que tenemos por fila:\n",
    "$$\\hat{y_1}=F_1(x_1)=0.69+0.1*1.49=0.839$$\n",
    "$$\\hat{y_2}=F_2(x_2)=0.69+0.1*(-3.03)=0.387$$\n",
    "$$\\hat{y_3}=F_3(x_3)=0.69+0.1*1.49=0.839$$\n",
    "Como podemos ver, estos valores solamente representan un score que no nos indica la probabilidad, para esto aplicamos una softmax para normalizar las salidas:\n",
    "\n",
    "$$p_1=\\dfrac{1}{1+e^{-\\hat{y}}}=0.698$$\n",
    "$$p_2=0.595$$\n",
    "$$p_3=0.698$$\n",
    "Finalmente los residuales de cada una de las salidas es:\n",
    "\n",
    "$$\\gamma_1 = \\gamma_3 = 0.302$$\n",
    "$$\\gamma_2 = -0.595$$\n",
    "\t\t‚ùì Pregunta: ¬øel calculo tiene una dependencia de las features?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca03c8ee-e596-49dc-991d-491c3f944c32",
   "metadata": {},
   "source": [
    "## Ejemplo con C√≥digo üßê\n",
    "\n",
    "Como ya comprendimos que es un algoritmo de GBM de forma teorica y como este se calcula a mano, el ultimo paso es visualizar como funciona el entrenamiento a trav√©s de c√≥digo. Para realizar este ejercicio codificaremos desde 0 el algoritmo de GBM utilizando los `DecisionTreeClassifier` de scikit-learn. \n",
    "\n",
    "**Objetivo:** Codificar cada una de las partes que contiene un algoritmo de GBM en una clase de python.\n",
    "\n",
    "![XGBoost vs LightGBM: How Are They Different](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Gradient-boosting-LightGBM-vs-XGBoost.png?resize=591%2C431&ssl=1)\n",
    "\n",
    "Comenzamos definiendo el inicializador de nuestra clase `GBMClassifier`, para esto definimos los siguientes valores:\n",
    "- n_estimators = N√∫mero de arboles que vamos a entrenar en el entrenamiento secuencial e iterativo.\n",
    "- max_depth = Maxima profundidad de los arboles que vamos a entrenar.\n",
    "- lr = Tasa de aprendizaje de nuestro GBM.\n",
    "- loss = Definimos la funci√≥n de perdida que vamos a utilizar para el problema.\n",
    "\n",
    "```python\n",
    "class GBMClassifier:\n",
    "    def __init__(self, n_trees, learning_rate=0.1, max_depth=1, loss_function=None):\n",
    "            self.n_trees=n_trees\n",
    "            self.learning_rate=learning_rate\n",
    "            self.max_depth=max_depth\n",
    "            self.loss_function = loss_function\n",
    "```\n",
    "\n",
    "En segundo lugar debemos definir el m√©todo que entrenara a a nuestro modelo GBM. Para la construcci√≥n consideramos los siguientes pasos:  \n",
    "1. Generamos una lista donde almacenaremos nuestros arboles en cada una de las iteraciones, para esto generamos un atributo donde sera almacenado.\n",
    "2. En segundo lugar es necesario obtener obtener las predicciones bases de la primera etapa de nuestro modelo utilizando la funci√≥n que calcula los gradientes de la funci√≥n de perdida.\n",
    "3. Generamos un loop en el que entrenaremos los $n$ √°rboles que definimos al inicializar la GBM y realizamos los siguientes pasos.\n",
    "\t1. Obtener los gradientes ($\\gamma$) de cada una de las etapas.\n",
    "\t2. Entrenamos los √°rboles con los gradientes obtenidos en 1.\n",
    "\t3. Actualizamos las predicciones realizadas por nuestros √°rboles.\n",
    "\t4. Obtenemos $F_M$ a utilizando las predicciones y el learning rate.\n",
    "\t5. Agregamos el √°rbol entrenado en esta etapa a nuestra lista de √°rboles.\n",
    "\n",
    "```python \n",
    "def fit(self):\n",
    "self.trees = []\n",
    "        self.base_prediction = self._argmin_fun(y=y)\n",
    "        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
    "        for _ in range(self.n_trees):\n",
    "            pseudo_residuals = self.loss_function.negative_gradient(y, current_predictions)\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, pseudo_residuals)\n",
    "            self._update_terminal_nodes(tree, X, y, current_predictions)\n",
    "            current_predictions += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "```\n",
    "\n",
    "Para calcular los gradientes de nuestra funci√≥n utilizaremos una funci√≥n de minimizaci√≥n, de esta forma no nos preocuparemos de las derivadas. En la definici√≥n de este m√©todo tendremos dos casos: el caso base, cuando no se tiene un $\\hat{y}$ y un segundo cuando se tiene las predicciones de un estimador.\n",
    "\n",
    "```python\n",
    "def _argmin_fun(self, y, y_hat=None):\n",
    "        if np.array(y_hat).all() == None:\n",
    "            fun = lambda c: self.loss_function.loss(y, c)\n",
    "        else:\n",
    "            fun = lambda c: self.loss_function.loss(y, y_hat+c)\n",
    "        c0 = y.mean()\n",
    "        return minimize(fun=fun, x0=c0).x[0]\n",
    "```\n",
    "\n",
    "Finalmente tenemos la actualizaci√≥n de los nodos del √°rbol entrenado, para esto comenzamos obtenido las `ids` de los nodos que contienen las predicciones y los recorremos en un loop:\n",
    "\n",
    "1. Dentro del loop encontramos todos los valores que esten la misma hoja y seleccionamos los `y` e $\\hat{y}$ que poseen estos valores.\n",
    "2. Calculamos el $\\gamma$ de estos valores y reemplazamos en los √°rboles.\n",
    "\n",
    "```python\n",
    "def _update_terminal_nodes(self, tree, X, y, current_predictions):\n",
    "        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
    "        leaf_node_for_each_sample = tree.apply(X)\n",
    "        for leaf in leaf_nodes:\n",
    "            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
    "            y_in_leaf = y.take(samples_in_this_leaf, axis=0)\n",
    "            preds_in_leaf = current_predictions.take(samples_in_this_leaf, axis=0)\n",
    "            val = self._argmin_fun(y=y_in_leaf, y_hat=preds_in_leaf)\n",
    "            tree.tree_.value[leaf, 0, 0] = val\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "620bc227-ecef-40fa-a0c4-e481915fe7b6",
   "metadata": {},
   "source": [
    "### Definamos la clase que creamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917a2ec-2322-4c76-9e06-1d92a6606339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class GradientBoostingMachine():\n",
    "    \n",
    "    def __init__(self, n_trees, learning_rate=0.1, max_depth=1, loss_function=None):\n",
    "        self.n_trees=n_trees\n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_depth=max_depth\n",
    "        self.loss_function = loss_function\n",
    "    \n",
    "    def fit(self, X, y):        \n",
    "        self.trees = []\n",
    "        self.base_prediction = self._argmin_fun(y=y)\n",
    "        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
    "        for _ in range(self.n_trees):\n",
    "            pseudo_residuals = self.loss_function.negative_gradient(y, current_predictions)\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, pseudo_residuals)\n",
    "            self._update_terminal_nodes(tree, X, y, current_predictions)\n",
    "            current_predictions += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _argmin_fun(self, y, y_hat=None):\n",
    "        if np.array(y_hat).all() == None:\n",
    "            fun = lambda c: self.loss_function.loss(y, c)\n",
    "        else:\n",
    "            fun = lambda c: self.loss_function.loss(y, y_hat+c)\n",
    "        c0 = y.mean()\n",
    "        return minimize(fun=fun, x0=c0).x[0]\n",
    "        \n",
    "    def _update_terminal_nodes(self, tree, X, y, current_predictions):\n",
    "        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
    "        leaf_node_for_each_sample = tree.apply(X)\n",
    "        for leaf in leaf_nodes:\n",
    "            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
    "            y_in_leaf = y.take(samples_in_this_leaf, axis=0)\n",
    "            preds_in_leaf = current_predictions.take(samples_in_this_leaf, axis=0)\n",
    "            val = self._argmin_fun(y=y_in_leaf, y_hat=preds_in_leaf)\n",
    "            tree.tree_.value[leaf, 0, 0] = val\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return (self.base_prediction \n",
    "                + self.learning_rate \n",
    "                * np.sum([tree.predict(X) for tree in self.trees], axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4732efd-d1e8-4fc1-8d20-71810335d08a",
   "metadata": {},
   "source": [
    "#### Probemos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed80534-1480-4ff3-9bdf-e87931701bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rng\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# test data\n",
    "def make_test_data(n, noise_scale):\n",
    "    x = np.linspace(0, 10, 500).reshape(-1,1)\n",
    "    y = (np.where(x < 5, x, 5) + rng.normal(0, noise_scale, size=x.shape)).ravel()\n",
    "    return x, y\n",
    "    \n",
    "# print model loss scores\n",
    "def print_model_loss_scores(obj, y, preds, sk_preds):\n",
    "    print(f'From Scratch Loss = {obj.loss(y, pred):0.4}')\n",
    "    print(f'Scikit-Learn Loss = {obj.loss(y, sk_pred):0.4}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4578ab0-647c-4feb-ba15-5551f209c185",
   "metadata": {},
   "source": [
    "Comenzamos generando unos datos con cierto grado de relaci√≥n para generar una regresi√≥n simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be632ae1-a6f8-414e-8a8a-0dad43952404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "x, y = make_test_data(500, 0.4)\n",
    "px.scatter(x, y, title='Datos de Prueba',template='simple_white')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61d222aa-b357-4842-954f-ca0dda66a48c",
   "metadata": {},
   "source": [
    "Donde nuestra funci√≥n de perdida viene dada por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbad0e-b303-4994-96f9-aef0afd3cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss():\n",
    "    '''User-Defined Squared Error Loss'''\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(y, preds):\n",
    "        return np.mean((y - preds)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def negative_gradient(y, preds):\n",
    "        return y - preds\n",
    "    \n",
    "\n",
    "gbm = GradientBoostingMachine(n_trees=100,\n",
    "                                  learning_rate=0.5,\n",
    "                                  max_depth=1,\n",
    "                                  loss_function=MSELoss()\n",
    "                                 )\n",
    "gbm.fit(x, y)\n",
    "pred = gbm.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d96bfa-2f27-4074-ad8b-98473526ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c03fee02-f237-42be-91da-e9c7c8b7a24d",
   "metadata": {},
   "source": [
    "Con los datos generamos un loop para visualizar como es el impacto que los estimadores dentro de la regresi√≥n, con esto obtenemos los siguientes gr√°ficos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1416df-3aec-4512-bf19-773263b52bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regresion(x, y, y_pred, name=\"Datos con regresi√≥n\", color='r'):\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, pred, color='r')\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    \n",
    "for i in range(1, 11):\n",
    "    gbm = GradientBoostingMachine(n_trees=i,\n",
    "                                  learning_rate=0.5,\n",
    "                                  max_depth=1,\n",
    "                                  loss_function=MSELoss()\n",
    "                                 )\n",
    "    gbm.fit(x, y)\n",
    "    pred = gbm.predict(x)\n",
    "    name_plot = f\"Datos con regresi√≥n con {i} estimadores\"\n",
    "    plot_regresion(x, y, pred, name=name_plot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5d4dcfa-c425-4e73-9b0c-1c2a0620665a",
   "metadata": {},
   "source": [
    "Comprobemos si esto tiene sentido con el clasificador de boosting que viene en sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15c0b5-3418-4f66-b7b3-8292652f73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_gbm = GradientBoostingRegressor(n_estimators=10,\n",
    "                                   learning_rate=0.5,\n",
    "                                   max_depth=1)\n",
    "sk_gbm.fit(x, y)\n",
    "sk_pred = sk_gbm.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c525d-d947-4cf1-bdb8-aea91ecbc855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_loss_scores(MSELoss(), y, pred, sk_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d1649bf-b1be-47bd-b14a-b78cae35e035",
   "metadata": {},
   "source": [
    "De los resultados se observa que a medida que aumentamos el n√∫mero de estimadores es posible generar una regresi√≥n m√°s similar a la tendencia que se√±alan los datos, por lo que la adici√≥n de weak-learners a nuestro modelo fue efectivo!\n",
    "\n",
    "## Retrospectiva\n",
    "\n",
    "\t\t‚ùì Pregunta: ¬øa√±adir muchos estimadores que puede provocar?\n",
    "\t\t‚ùì Pregunta: Sabemos que los √°rboles son interpretables, ¬øque sucede con los algoritmos de ensemle?\n",
    "\t\t‚ùì Pregunta: ¬øCom√≥ podemos medir el sobre ajuste de estos modelos si son multiples √°rboles?\n",
    "\t\t‚ùì Pregunta: ¬øA nivel general son buenos estimadores los GBM?\n",
    "\t\t\n",
    "\n",
    "Links de interes:\n",
    "https://explained.ai/gradient-boosting/descent.html\n",
    "https://www.cienciadedatos.net/documentos/py09_gradient_boosting_python.html\n",
    "https://explained.ai/gradient-boosting/L2-loss.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
