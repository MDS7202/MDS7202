{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f97e3ccc-bc09-45f0-bfe8-1c9934f802a0",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines (GBM)\n",
    "\n",
    "las maquinas de gradient boosting son algoritmos que se volvieron muy populares con la aparición de XGBoost en el 2016 para las tareas de clasificación y regresión. En términos de funcionamiento, estos algoritmos utilizan multiples \"weak-learners\" con los que a través de multiples iteraciones corrigen los errores que presentan sus regresores/clasificadores en etapas anteriores.\n",
    "\n",
    "\t\t❓ Pregunta: ¿Que caracteristicas tienen los \"weak-learners\"?\n",
    "\n",
    ">> 1.  El objetivo no es crear clasificadores potentes con ellos, si no como su nombre lo dice queremos generar \"aprendices débiles\" que se potencien con otros.\n",
    ">> 2. En general se utilizan como weak learners arboles de decisión por su simpleza y versatilidad que nos ofrecen para regresión y clasificación.\n",
    ">> 3. Pueden ser entrenados con una muestra de la totalidad para obtener entrenamientos rápidos sobre ellos.\n",
    ">> 4. Su uso en un algoritmo implica la comprensión de estos elementos, ya que se heredan sus problemas en una estructura mas compleja.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74ef55d0-679c-40f4-93fc-d861ad65f08d",
   "metadata": {},
   "source": [
    "❓ Pregunta: ¿Que problemas tenian los arboles de decisión?\n",
    "![3 Techniques to Avoid Overfitting of Decision Trees | by Satyam Kumar |  Towards Data Science](https://miro.medium.com/v2/resize:fit:1702/1*KbRVJC5B0EgO8YAyeCrLkA.png)\n",
    "\n",
    "Recordar que los árboles de decisión son elementos que tienen una alta facilidad de sobre-ajustarse si aumentamos la profundidad o número de hijos por nodo. Esto puede causar problemas en un algoritmo de GBM y por ello, este debe ser un valor que debemos controlar de forma directa o indirectamente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a42f5ff-80b5-490a-924e-5fef5dfc8102",
   "metadata": {},
   "source": [
    "Adentrándonos mas en detalles en el algoritmo de gradient boosting, estos forman parte de los algoritmos de ensemble, quienes basicamente son algoritmos que combinan multiples modelos para el proceso de predicción. Una forma de esto es a traves de modelos aditivos, donde entrenando multiples algoritmos simples de forma independiente, utilizamos la suma de sus salidas para generar un modelo mas potente para la regresión/clasificación:\n",
    "$$F_M(x) = f_1(x)+...+f_M(x)=\\sum_{m=1}^M f_m(x)$$\n",
    "![Develop an Intuition for How Ensemble Learning Works -  MachineLearningMastery.com](https://machinelearningmastery.com/wp-content/uploads/2020/07/Example-of-Combining-Decision-Boundaries-Using-an-Ensemble.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e28f15-95f2-4125-87f6-bdfa13312dc2",
   "metadata": {},
   "source": [
    "### GBM: Regresión\n",
    "\n",
    "La idea que sigue por detras las GBM es realizar descenso del gradiente a una función $\\hat{f}$ en el espacio de funciones. O sea, si utilizamos un conjunto $f$ de funciones deseamos disminuir, todo lo posible la función de error que representa la función de perdida ($\\mathcal{L}$) durante el entrenamiento. Formalmente:\t\n",
    "$$\\hat{f}=argmin_f \\, \\mathcal{L}(f)$$\n",
    "donde $f$:\n",
    "$$f = (f(x_1), ..., f(x_N))$$\n",
    "Donde en cada paso $m$ (número de la iteración) generaremos un gradiente $g_m$ de la función $\\mathcal{L(f_m)}$ :\n",
    "$$g_m = [\\dfrac{\\partial{l}(y_i,f(x_i))}{\\partial{f}(x_i)}]_{f=f_{m-1}}$$\n",
    "En cada una de las iteraciones del modelo $g_m$ se encargara de actualizar el valor de $f_{m-1}$, de tal forma de generar la función $f_m$ de la iteración. $g_m$ es la expresión matematica que contiene la información de los errores de la predicción anterior que deseamos corregir en cada iteración. \n",
    "\n",
    "Luego, para comprender la actualización situémonos en el paso $m$, en el tendremos:\n",
    "$$f_m = f_{m-1} - \\beta_{m}g_{m}$$\n",
    "Donde $\\beta_m$ se define como el largo del paso en la corrección. Visto de otra forma, podemos interpretar esta valor como un factor de aprendizaje que podremos fijar o optimizar en la función, señalando cuanta corrección aplicaremos del proceso anterior en el proceso actual. \n",
    "\n",
    "Si bien en cada una de las iteraciones encontraremos un valor optimo para $f_m$ y $N$ puntos, esta por si sola no genera un clasificador global capaz de obtener mejores generalizaciones, por esta razón necesitamos definir una nueva función con la misma idea pero que en cada iteración se haga mas robusta. En esta modificación, el algoritmo para ajustar los weak-learners a una aproximación de la señal negativa del gradiente, donde en cada iteración entrenara un nuevo weak learner, quedando la actualización de la función como:\n",
    "$$F_m = argmin_F \\sum_{i=1}^N (-g_{im}-F(x_i))^2$$\n",
    "Notar que $F(x_i)$ es un weak learner que se entrena en cada una de las iteraciones de nuestro algoritmo de GBM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0106ab54-77ce-4806-994c-7a4dc0020537",
   "metadata": {},
   "source": [
    "Finalmente el algoritmo se vería como:\n",
    "\n",
    "![](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/21_Ensamblaje/Pasted%20image%2020230527153450.png?raw=true)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ceffe09-45a9-4009-9131-c5f0c4a260b6",
   "metadata": {},
   "source": [
    "### GBM: Clasificación\n",
    "\n",
    "\t\t❓ Pregunta: ¿Debería ser diferente la clasificación a la regresión?\n",
    "\n",
    "La respuesta directa a esto es \"sí\". Los cambios que presenta la clasificación respecto a la regresión se dan netamente en la elección de la función de perdida ($\\mathcal{L}$) y algunas consideraciones que se toman para las salidas de los nodos.\n",
    "\n",
    "Respecto a las funciones de perdida, esto se debe a como se define el problema de clasificación supervisada, donde nuestro interés no será la reducción el error que tenemos del valor estimado respecto a un valor continuo. En su reemplazo son utilizadas funciones de perdida para clasificación, quienes tienen como principal objetivo disminuir el error predecir una etiqueta. Una de las funciones de perdidas mas conocidas/utilizadas para el problema de clasificación se encuentra la logistic-loss/cross-entropy:\n",
    "$$\\mathcal{L}_i = -(y_i log(p_i) + (1-y_i)log(1-p_i))$$\n",
    "\t❓ Pregunta:  ¿Como podemos interpretar esta función de perdida?\n",
    "\t\n",
    "Donde su derivada es:\n",
    "$$\\dfrac{\\partial \\mathcal{L}_i}{\\partial \\hat{y}}=p_i-y_i$$\n",
    "El segundo punto a considerar es que la estimación de $y$ sera igual a:\n",
    "$$\\hat{y}=log(odds)=log(\\dfrac{p}{1-p})$$\n",
    "\t\t❓ Pregunta:  ¿Por que usamos odds?\n",
    "\t\t\n",
    "Básicamente debido a que los modelos de clasificación con GBM son modelos de regresión, por lo que sus salidas no pueden ser consideradas como probabilidades, sino como scores que debemos transformar y normalizar.\n",
    "\n",
    "Finalmente para calcular las probabilidades de las salidas de nuestro GBM tendremos que calcular la función softmax de $\\hat{y}$, la que viene dada por:\n",
    "$$\\text{softmax}(\\hat{y})=\\dfrac{1}{1+e^{-\\hat{y}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a3d69-63da-47bc-8468-c5517cec4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "p = 0.99\n",
    "y = 0\n",
    "-(y*math.log(p)+(1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d66a4-9905-40d6-9501-4b58f59d469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.9\n",
    "y = 1\n",
    "-(y* math.log(p) + (1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36f6af-cde4-4944-b2c8-07b2df783e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.3\n",
    "y = 0\n",
    "-(y* math.log(p) + (1-y)*math.log(1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30553b-8745-4aa9-a250-1d9702ede7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35087c5c-1382-4698-bfc5-b28334ea2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis1, prob = [], []\n",
    "for i in range(1, 10):\n",
    "    y = 0\n",
    "    lis1.append(-(y* math.log(i/10) + (1-y)*math.log(1-i/10)))\n",
    "    prob.append(i/10)\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "px.line(prob, lis1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4753bb7e-4280-47c2-bae5-c3990a0ad0b9",
   "metadata": {},
   "source": [
    "### Retrospectiva\n",
    "\n",
    "\t\t❓ Pregunta: ¿Pero como podriamos simplificar estas definiciones que acabamos de dar?\n",
    "\n",
    "Si visualizamos GBM en un juego de golf, tendriamos un jugador que comienza con un tiro $f_0$, el cual a medida que va realizando cada tiro va aplicando una corrección $\\Delta_m$ en cada una de sus jugadas. Con esto, si cerebralmente el jugador corrigue sus errores a traves de una función MSE, tendremos que el jugador ira optimizando esta función hasta meter la pelota en el agujero (minimo de la función de perdida).\n",
    "\n",
    "$\\text{MSE LOSS} = \\mathcal{L}(y, F_M(X))=\\dfrac{1}{N} \\sum_{i=1}^N (y_i - F_M(x_i))^2$\n",
    "\n",
    "![](https://explained.ai/gradient-boosting/images/golf-MSE.png)\n",
    "\n",
    "\t\t❓ ¿Como podría el golfista correguir de mejor forma sus tiros? ¿existe alguna forma?.\n",
    "\t\t\n",
    "![1d-vectors.png](https://explained.ai/gradient-boosting/images/1d-vectors.png)\n",
    "\n",
    "Como vimos durante el entrenamiento de GBM, este proceso considera un $\\nu$ conocido como shrinkage rate o learning rate. Este valor nos permitirá realizar correcciones en las actualizaciones de nuestras funciones $f_m$, ya que podría darse el caso que el gradiente obtenido durante la optimización sea muy alto y nos permita obtener un minimo adecuado para el problema. De esta forma:\n",
    "$$f_m(x) = f_{m-1}(x) + \\nu F_m(x)$$\n",
    "Otra forma podría ser cambiando la función de perdida que definimos en el problema. Dependiendo si es un problema de clasificación o regresión existen multiples funciones de perdida con diferentes interpretaciones que pueden mejor significativamente el problema que deseamos resolver. Algunas de estas son:\n",
    "\n",
    "| Nombre         | Loss                         | Derivada           |  \n",
    "| -------------- | ---------------------------- | ------------------ |\n",
    "| Squeared Error | $\\dfrac{1}{2}(y_i-f(x_i))^2$ | $y_i -f(x_i)$      |\n",
    "| Absolute Error | $y_i - f(x_i)$               | $sgn(y_i -f(x_i))$ |\n",
    "| Binary Logloss                | $log(1+e^{-y_if_i})$  | $y-\\sigma(2f(x_i))$    |\n",
    "\n",
    "\n",
    "En tercer lugar tenemos la adición de un regularizador a nuestra función de perdida, de esta forma podremos disminuir el sobreajuste de nuestro modelo, generando pequeñas modificaciones en los arboles que vamos generando.\n",
    "$$\\mathcal{L}(f) = \\sum_{i=1}^Nl(y_i,f(x))+\\Omega(f))$$\n",
    "donde $\\Omega$:\n",
    "$$\\Omega(f)= \\gamma J+\\dfrac{1}{2}\\lambda \\sum_{j=1}^J w^2_j$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d41b7e14-270b-4467-924a-10189151b12d",
   "metadata": {},
   "source": [
    "### Pros & Cons\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- Capacidad para manejar tanto problemas de regresión como de clasificación.\n",
    "- Puede capturar relaciones no lineales y características complejas en los datos.\n",
    "- Adaptabilidad a diferentes tipos de predictores, incluyendo variables numéricas y categóricas.\n",
    "- Robustez ante valores atípicos y datos ruidosos.\n",
    "- Capacidad para manejar grandes conjuntos de datos y escalabilidad.\n",
    "- Flexibilidad en la elección de funciones de pérdida y métricas de evaluación.\n",
    "- Capacidad para manejar características faltantes y utilizar todas las observaciones disponibles.\n",
    "- Puede generar importancia de variables para ayudar en la interpretación y selección de características.\n",
    "\n",
    "**Contras**:\n",
    "\n",
    "- Mayor complejidad y tiempo de entrenamiento en comparación con algoritmos más simples.\n",
    "- Puede ser propenso a sobreajuste si no se controlan los hiperparámetros adecuadamente.\n",
    "- Requiere una configuración cuidadosa de los hiperparámetros para obtener el mejor rendimiento.\n",
    "- Interpretación más difícil debido a la naturaleza de combinación de múltiples árboles.\n",
    "- Sensible a datos desequilibrados, donde las clases minoritarias pueden no recibir suficiente atención.\n",
    "- La importancia de las variables puede verse sesgada hacia las variables numéricas o con mayor cardinalidad.\n",
    "- No es adecuado para problemas con alta dimensionalidad o con muchos predictores.\n",
    "- Mayor consumo de recursos computacionales en comparación con algoritmos más simples.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bac118e-ce1c-438a-891d-ea51090f5278",
   "metadata": {},
   "source": [
    "## Ejemplo a Mano 🤨\n",
    "\n",
    "Como ya sabemos como funciona port detras un algoritmo simple de Gradient Boosting, veamos un ejemplo para ejecutar un pequeño GBM a mano. Para esto consideremos los siguientes datos:\n",
    "\n",
    "| index | $feature_1$ | $feature_2$ | y     |  \n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1     | 1.12  | 1.4   | 1     |   \n",
    "| 2     | 5.45  | 3.1   | 0     |   \n",
    "| 3     | 3.54  | 1.2   | 1     |   \n",
    "\n",
    "Considerando los datos construir un modelo de GBM con 2 estimadores y un $\\nu$ igual a $0.1$.\n",
    "\n",
    "**Desarrollo:**\n",
    "Con la tabla anterior, comenzamos realizando nuestra primera iteración (m=1), para ello se considera que todos los datos se encuentran en el mismo nodo. De esta forma, calculando los odds y los log(odds) de los datos tendremos:\n",
    "\n",
    "$$odds = \\dfrac{\\text{cantidad de casos exitosos}}{\\text{cantidad de casos fallidos}}$$\n",
    "$$odds = 2/1 = 2$$\n",
    "Luego para los $log_n(odds)$:\n",
    "$$log_n(odds)=log_n(2)=0,693$$\n",
    "Notar que debido a que todos los valores estan en el mismo nodo, todos los valores poseeran el mismo log_n(odds). Luego calculamos las probabilidades para cada uno de los valores.\n",
    "$$p=\\dfrac{1}{1+e^{-\\hat{y}}}=\\dfrac{1}{1+e^{-0.693}}=0.67$$\n",
    "Finalmente $\\gamma$ sera:\n",
    "$$\\gamma_1=y-p_1=1-0.67=0.33$$\n",
    "$$\\gamma_2=y-p_2=0-0.67=-0,67$$\n",
    "$$\\gamma_3=y-p_3=1-0.67=0.33$$\n",
    "Tenemos nuestro primer estimador calculado, ahora el siguiente paso es generar un nuevo árbol. Para esto vamos a generar un árbol donde el nodo padre tiene la regla $feature_2>2$.\n",
    "\n",
    "![](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/21_Ensamblaje/Flowchart%20Template.jpg?raw=true)\n",
    "\n",
    "Considerando los resultados obtenidos en la iteración anterior, tendremos que ahora los valores de $\\gamma_i$ estan dados por:\n",
    "$$\\gamma_{i}=\\dfrac{\\sum_i y_i-p_i}{\\sum_i p_i(1-p_i)}$$\n",
    "Reemplazando cada valor:\n",
    "$$\\gamma_{1m}=\\dfrac{0.33+0.33}{2*(0.67(1-0.67))}=1.49$$\n",
    "$$\\gamma_{2m}=\\dfrac{-0.67}{0.67(1-0.67)}=-3.03$$\n",
    "Finalmente, con los valores obtenidos calculamos el $F_m$ resultante de las 2 iteraciones para cada una de las hojas:\n",
    "\n",
    "$$F_m(x)=F_{m-1}(x)+\\nu \\gamma_m$$\n",
    "Reemplazamos para cada uno de los valores que tenemos por fila:\n",
    "$$\\hat{y_1}=F_1(x_1)=0.69+0.1*1.49=0.839$$\n",
    "$$\\hat{y_2}=F_2(x_2)=0.69+0.1*(-3.03)=0.387$$\n",
    "$$\\hat{y_3}=F_3(x_3)=0.69+0.1*1.49=0.839$$\n",
    "Como podemos ver, estos valores solamente representan un score que no nos indica la probabilidad, para esto aplicamos una softmax para normalizar las salidas:\n",
    "\n",
    "$$p_1=\\dfrac{1}{1+e^{-\\hat{y}}}=0.698$$\n",
    "$$p_2=0.595$$\n",
    "$$p_3=0.698$$\n",
    "Finalmente los residuales de cada una de las salidas es:\n",
    "\n",
    "$$\\gamma_1 = \\gamma_3 = 0.302$$\n",
    "$$\\gamma_2 = -0.595$$\n",
    "\t\t❓ Pregunta: ¿el calculo tiene una dependencia de las features?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca03c8ee-e596-49dc-991d-491c3f944c32",
   "metadata": {},
   "source": [
    "## Ejemplo con Código 🧐\n",
    "\n",
    "Como ya comprendimos que es un algoritmo de GBM de forma teorica y como este se calcula a mano, el ultimo paso es visualizar como funciona el entrenamiento a través de código. Para realizar este ejercicio codificaremos desde 0 el algoritmo de GBM utilizando los `DecisionTreeClassifier` de scikit-learn. \n",
    "\n",
    "**Objetivo:** Codificar cada una de las partes que contiene un algoritmo de GBM en una clase de python.\n",
    "\n",
    "![XGBoost vs LightGBM: How Are They Different](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Gradient-boosting-LightGBM-vs-XGBoost.png?resize=591%2C431&ssl=1)\n",
    "\n",
    "Comenzamos definiendo el inicializador de nuestra clase `GBMClassifier`, para esto definimos los siguientes valores:\n",
    "- n_estimators = Número de arboles que vamos a entrenar en el entrenamiento secuencial e iterativo.\n",
    "- max_depth = Maxima profundidad de los arboles que vamos a entrenar.\n",
    "- lr = Tasa de aprendizaje de nuestro GBM.\n",
    "- loss = Definimos la función de perdida que vamos a utilizar para el problema.\n",
    "\n",
    "```python\n",
    "class GBMClassifier:\n",
    "    def __init__(self, n_trees, learning_rate=0.1, max_depth=1, loss_function=None):\n",
    "            self.n_trees=n_trees\n",
    "            self.learning_rate=learning_rate\n",
    "            self.max_depth=max_depth\n",
    "            self.loss_function = loss_function\n",
    "```\n",
    "\n",
    "En segundo lugar debemos definir el método que entrenara a a nuestro modelo GBM. Para la construcción consideramos los siguientes pasos:  \n",
    "1. Generamos una lista donde almacenaremos nuestros arboles en cada una de las iteraciones, para esto generamos un atributo donde sera almacenado.\n",
    "2. En segundo lugar es necesario obtener obtener las predicciones bases de la primera etapa de nuestro modelo utilizando la función que calcula los gradientes de la función de perdida.\n",
    "3. Generamos un loop en el que entrenaremos los $n$ árboles que definimos al inicializar la GBM y realizamos los siguientes pasos.\n",
    "\t1. Obtener los gradientes ($\\gamma$) de cada una de las etapas.\n",
    "\t2. Entrenamos los árboles con los gradientes obtenidos en 1.\n",
    "\t3. Actualizamos las predicciones realizadas por nuestros árboles.\n",
    "\t4. Obtenemos $F_M$ a utilizando las predicciones y el learning rate.\n",
    "\t5. Agregamos el árbol entrenado en esta etapa a nuestra lista de árboles.\n",
    "\n",
    "```python \n",
    "def fit(self):\n",
    "self.trees = []\n",
    "        self.base_prediction = self._argmin_fun(y=y)\n",
    "        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
    "        for _ in range(self.n_trees):\n",
    "            pseudo_residuals = self.loss_function.negative_gradient(y, current_predictions)\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, pseudo_residuals)\n",
    "            self._update_terminal_nodes(tree, X, y, current_predictions)\n",
    "            current_predictions += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "```\n",
    "\n",
    "Para calcular los gradientes de nuestra función utilizaremos una función de minimización, de esta forma no nos preocuparemos de las derivadas. En la definición de este método tendremos dos casos: el caso base, cuando no se tiene un $\\hat{y}$ y un segundo cuando se tiene las predicciones de un estimador.\n",
    "\n",
    "```python\n",
    "def _argmin_fun(self, y, y_hat=None):\n",
    "        if np.array(y_hat).all() == None:\n",
    "            fun = lambda c: self.loss_function.loss(y, c)\n",
    "        else:\n",
    "            fun = lambda c: self.loss_function.loss(y, y_hat+c)\n",
    "        c0 = y.mean()\n",
    "        return minimize(fun=fun, x0=c0).x[0]\n",
    "```\n",
    "\n",
    "Finalmente tenemos la actualización de los nodos del árbol entrenado, para esto comenzamos obtenido las `ids` de los nodos que contienen las predicciones y los recorremos en un loop:\n",
    "\n",
    "1. Dentro del loop encontramos todos los valores que esten la misma hoja y seleccionamos los `y` e $\\hat{y}$ que poseen estos valores.\n",
    "2. Calculamos el $\\gamma$ de estos valores y reemplazamos en los árboles.\n",
    "\n",
    "```python\n",
    "def _update_terminal_nodes(self, tree, X, y, current_predictions):\n",
    "        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
    "        leaf_node_for_each_sample = tree.apply(X)\n",
    "        for leaf in leaf_nodes:\n",
    "            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
    "            y_in_leaf = y.take(samples_in_this_leaf, axis=0)\n",
    "            preds_in_leaf = current_predictions.take(samples_in_this_leaf, axis=0)\n",
    "            val = self._argmin_fun(y=y_in_leaf, y_hat=preds_in_leaf)\n",
    "            tree.tree_.value[leaf, 0, 0] = val\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "620bc227-ecef-40fa-a0c4-e481915fe7b6",
   "metadata": {},
   "source": [
    "### Definamos la clase que creamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917a2ec-2322-4c76-9e06-1d92a6606339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class GradientBoostingMachine():\n",
    "    \n",
    "    def __init__(self, n_trees, learning_rate=0.1, max_depth=1, loss_function=None):\n",
    "        self.n_trees=n_trees\n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_depth=max_depth\n",
    "        self.loss_function = loss_function\n",
    "    \n",
    "    def fit(self, X, y):        \n",
    "        self.trees = []\n",
    "        self.base_prediction = self._argmin_fun(y=y)\n",
    "        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
    "        for _ in range(self.n_trees):\n",
    "            pseudo_residuals = self.loss_function.negative_gradient(y, current_predictions)\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, pseudo_residuals)\n",
    "            self._update_terminal_nodes(tree, X, y, current_predictions)\n",
    "            current_predictions += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _argmin_fun(self, y, y_hat=None):\n",
    "        if np.array(y_hat).all() == None:\n",
    "            fun = lambda c: self.loss_function.loss(y, c)\n",
    "        else:\n",
    "            fun = lambda c: self.loss_function.loss(y, y_hat+c)\n",
    "        c0 = y.mean()\n",
    "        return minimize(fun=fun, x0=c0).x[0]\n",
    "        \n",
    "    def _update_terminal_nodes(self, tree, X, y, current_predictions):\n",
    "        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
    "        leaf_node_for_each_sample = tree.apply(X)\n",
    "        for leaf in leaf_nodes:\n",
    "            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
    "            y_in_leaf = y.take(samples_in_this_leaf, axis=0)\n",
    "            preds_in_leaf = current_predictions.take(samples_in_this_leaf, axis=0)\n",
    "            val = self._argmin_fun(y=y_in_leaf, y_hat=preds_in_leaf)\n",
    "            tree.tree_.value[leaf, 0, 0] = val\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return (self.base_prediction \n",
    "                + self.learning_rate \n",
    "                * np.sum([tree.predict(X) for tree in self.trees], axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4732efd-d1e8-4fc1-8d20-71810335d08a",
   "metadata": {},
   "source": [
    "#### Probemos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed80534-1480-4ff3-9bdf-e87931701bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rng\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# test data\n",
    "def make_test_data(n, noise_scale):\n",
    "    x = np.linspace(0, 10, 500).reshape(-1,1)\n",
    "    y = (np.where(x < 5, x, 5) + rng.normal(0, noise_scale, size=x.shape)).ravel()\n",
    "    return x, y\n",
    "    \n",
    "# print model loss scores\n",
    "def print_model_loss_scores(obj, y, preds, sk_preds):\n",
    "    print(f'From Scratch Loss = {obj.loss(y, pred):0.4}')\n",
    "    print(f'Scikit-Learn Loss = {obj.loss(y, sk_pred):0.4}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4578ab0-647c-4feb-ba15-5551f209c185",
   "metadata": {},
   "source": [
    "Comenzamos generando unos datos con cierto grado de relación para generar una regresión simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be632ae1-a6f8-414e-8a8a-0dad43952404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "x, y = make_test_data(500, 0.4)\n",
    "px.scatter(x, y, title='Datos de Prueba',template='simple_white')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61d222aa-b357-4842-954f-ca0dda66a48c",
   "metadata": {},
   "source": [
    "Donde nuestra función de perdida viene dada por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbad0e-b303-4994-96f9-aef0afd3cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss():\n",
    "    '''User-Defined Squared Error Loss'''\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(y, preds):\n",
    "        return np.mean((y - preds)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def negative_gradient(y, preds):\n",
    "        return y - preds\n",
    "    \n",
    "\n",
    "gbm = GradientBoostingMachine(n_trees=100,\n",
    "                                  learning_rate=0.5,\n",
    "                                  max_depth=1,\n",
    "                                  loss_function=MSELoss()\n",
    "                                 )\n",
    "gbm.fit(x, y)\n",
    "pred = gbm.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d96bfa-2f27-4074-ad8b-98473526ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c03fee02-f237-42be-91da-e9c7c8b7a24d",
   "metadata": {},
   "source": [
    "Con los datos generamos un loop para visualizar como es el impacto que los estimadores dentro de la regresión, con esto obtenemos los siguientes gráficos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1416df-3aec-4512-bf19-773263b52bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regresion(x, y, y_pred, name=\"Datos con regresión\", color='r'):\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, pred, color='r')\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    \n",
    "for i in range(1, 11):\n",
    "    gbm = GradientBoostingMachine(n_trees=i,\n",
    "                                  learning_rate=0.5,\n",
    "                                  max_depth=1,\n",
    "                                  loss_function=MSELoss()\n",
    "                                 )\n",
    "    gbm.fit(x, y)\n",
    "    pred = gbm.predict(x)\n",
    "    name_plot = f\"Datos con regresión con {i} estimadores\"\n",
    "    plot_regresion(x, y, pred, name=name_plot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5d4dcfa-c425-4e73-9b0c-1c2a0620665a",
   "metadata": {},
   "source": [
    "Comprobemos si esto tiene sentido con el clasificador de boosting que viene en sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15c0b5-3418-4f66-b7b3-8292652f73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_gbm = GradientBoostingRegressor(n_estimators=10,\n",
    "                                   learning_rate=0.5,\n",
    "                                   max_depth=1)\n",
    "sk_gbm.fit(x, y)\n",
    "sk_pred = sk_gbm.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c525d-d947-4cf1-bdb8-aea91ecbc855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_loss_scores(MSELoss(), y, pred, sk_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d1649bf-b1be-47bd-b14a-b78cae35e035",
   "metadata": {},
   "source": [
    "De los resultados se observa que a medida que aumentamos el número de estimadores es posible generar una regresión más similar a la tendencia que señalan los datos, por lo que la adición de weak-learners a nuestro modelo fue efectivo!\n",
    "\n",
    "## Retrospectiva\n",
    "\n",
    "\t\t❓ Pregunta: ¿añadir muchos estimadores que puede provocar?\n",
    "\t\t❓ Pregunta: Sabemos que los árboles son interpretables, ¿que sucede con los algoritmos de ensemle?\n",
    "\t\t❓ Pregunta: ¿Comó podemos medir el sobre ajuste de estos modelos si son multiples árboles?\n",
    "\t\t❓ Pregunta: ¿A nivel general son buenos estimadores los GBM?\n",
    "\t\t\n",
    "\n",
    "Links de interes:\n",
    "https://explained.ai/gradient-boosting/descent.html\n",
    "https://www.cienciadedatos.net/documentos/py09_gradient_boosting_python.html\n",
    "https://explained.ai/gradient-boosting/L2-loss.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
