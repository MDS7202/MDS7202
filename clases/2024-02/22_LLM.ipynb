{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogm_mtU_NDrK"
      },
      "source": [
        "# **Clase 22 - Large Language Models**\n",
        "\n",
        "MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\n",
        "\n",
        "## **Objetivos**\n",
        "\n",
        "- Conocer qu√© son los LLM y c√≥mo trabajarlos con LangChain\n",
        "- Aprender a generar chains de prompts\n",
        "- Comprender el paradigma de Agentes\n",
        "- Entender como generar una soluci√≥n RAG\n",
        "- Introducir al estudiante a soluciones multiagente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPdcLIhgM3KX"
      },
      "source": [
        "## **Configuraci√≥n Inicial üßê**\n",
        "\n",
        "Para esta clase necesitaremos configurar las credenciales de algunos servicios a utilizar, en espec√≠fico:\n",
        "\n",
        "### **Google AI Studio**\n",
        "\n",
        "Usaremos `Google AI Studio` para habilitar el uso de LLMs y Embeddings de Google. Simplemente deben registrarse con su cuenta google y obtener su API KEY desde el siguiente enlace: [Google AI Studio](https://aistudio.google.com/app/u/1/apikey).\n",
        "\n",
        "### **Tavily**\n",
        "\n",
        "En paralelo, utilizaremos `Tavily` como motor de b√∫squeda para potenciar las respuestas de nuestros agentes. Tal como en el paso anterior, solo deben registrarse y obtener su API KEY desde el siguiente enlace: [Tavily](https://tavily.com/).\n",
        "\n",
        "### **Configurar credenciales en ambiente**\n",
        "\n",
        "Una vez se tienen todas las credenciales, pasamos a activarlas en nuestro ambiente local por medio del siguiente c√≥digo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBo7Y3iIeU6P",
        "outputId": "a35af344-561c-4e2e-ae92-74c98ba41a5d"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "\n",
        "if \"TAVILY_API_KEY\" not in os.environ:\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcaUejQXfGHF"
      },
      "source": [
        "O si les sale m√°s f√°cil, tambi√©n pueden hacerlo a trav√©s de un archivo **.env** (esto funciona mejor cuando trabajan desde sus m√°quinas locales).\n",
        "\n",
        "S√≥lo debemos crearlo y escribir en √©l todas las credenciales:\n",
        "\n",
        "```python\n",
        "GOOGLE_API_KEY=\"<YOUR_GOOGLE_API_KEY>\"\n",
        "TAVILY_API_KEY=\"<YOUR_TAVILY_API_KEY>‚Äù\n",
        "```\n",
        "\n",
        "Luego, cargamos las credenciales al ambiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEsU_-lFrA63"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEy2eZRCq0_x",
        "outputId": "f1355e89-4286-40e3-9e54-1195b99a1562"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv() # cargar las variables guardadas en el archivo .env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnLfzEKAc94E"
      },
      "source": [
        "## **¬øQu√© son los Large Language Models? ü§î**\n",
        "\n",
        "Los **Large Language Models (LLM)** son modelos de lenguaje basados en **redes neuronales del tipo transformer** entrenados con una gran cantidad de texto. Estos modelos poseen las siguientes caracter√≠sticas distintivas:\n",
        "\n",
        "- Son modelos **entrenados con grandes vol√∫menes de lenguaje** (corpus del tama√±o de internet), permiti√©ndoles aprender patrones complejos del lenguaje y obtener una comprensi√≥n sem√°ntica profunda.\n",
        "\n",
        "- Son modelos con una **gran cantidad de par√°metros** (miles de millones!), lo que les permite captar sutilezas ling√º√≠sticas, estilo y tono en un nivel sin precedentes.\n",
        "\n",
        "Adicionalmente, existen 2 grandes tipos de LLM:\n",
        "\n",
        "- Modelos basados en **representaciones**: Son modelos orientados a **aprender representaciones** (o *embeddings*) del lenguaje. Un ejemplo notable de este tipo de modelos es [BERT](https://arxiv.org/abs/1810.04805).\n",
        "- Modelos basados en **completion**: Son modelos dise√±ados para **predecir la siguiente palabra** de un texto. Un ejemplo notable son los modelos GPT (Generative Pre-trained Transformers), como GPT3.5 o GPT4.\n",
        "\n",
        "De estos modelos, son los GPT los que han obtenido m√°s fama en el √∫ltimo tiempo, dando pie a una gran cantidad de soluciones basadas en esta tecnolog√≠a.\n",
        "Como podr√°n suponer, para esta clase nos concentraremos en los LLM de completion.\n",
        "\n",
        "> **Pregunta**: ¬øPor qu√© creen que que los GPT basados en completion tuvo mayor √©xito?\n",
        "\n",
        "<center>\n",
        "<img src='https://media4.giphy.com/media/qAtZM2gvjWhPjmclZE/giphy.gif' width=450  />\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_fsF3o2haSU"
      },
      "source": [
        "## **LangChain ü¶ú**\n",
        "\n",
        "`LangChain` es un framework de c√≥digo abierto dise√±ado para desarrollar aplicaciones basadas en LLM. Basa su funcionamiento en los siguientes componentes:\n",
        "\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/MDS7202/MDS7202/main/recursos/2024-01//LLM/langchain.png' width=450  />\n",
        "</center>\n",
        "\n",
        "**1. Prompts y Plantillas de Prompts**: Los prompts son consultas que enviamos a los LLMs, y su calidad impacta las respuestas. LangChain facilita la creaci√≥n y gesti√≥n de prompts mediante plantillas que combinan instrucciones, contenido y consultas.\n",
        "\n",
        "```python\n",
        "template = \"\"\"\n",
        "You are required to answer the following question in form of bullet points based on the provided context.\n",
        "The answer should be answer as a doctor and your language is spanish.:\n",
        "{context}\n",
        "Now based on above context answer the following question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "**2. Modelos**: LangChain no incluye LLMs, pero permite integrar f√°cilmente varios modelos de lenguaje (como GPT-3, BLOOM), de chat (como GPT-3.5-turbo) y de embedding de texto (de CohereAI, HuggingFace y OpenAI) mediante un simple framework donde solo necesitas la API Key o cargar el modelo en memoria.\n",
        "\n",
        "```python\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "openai = OpenAI(\n",
        "   openai_api_key=‚ÄùYOUR OPEN AI API KEY‚Äù,\n",
        "   model_name=\"gpt-3.5-turbo-16k\",\n",
        ")\n",
        "```\n",
        "\n",
        "**3. Chains**: LangChain permite crear flujos de trabajo, o \"chains,\" que son secuencias de llamadas a modelos de lenguaje o a herramientas externas. Estos flujos pueden incluir varias etapas de procesamiento, donde la salida de una etapa se convierte en la entrada de la siguiente. son el simil de pipelines de `scikit-learn`.\n",
        "\n",
        "```python\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "```\n",
        "\n",
        "**4. Memoria**: Por defecto, las cadenas en LangChain son sin estado, es decir, no guarda un registro de las interacciones anteriores hechas con el modelo. Para superar esto, LangChain nos asiste con *buffers* para almacenar de forma iterativa las interacciones realizadas.\n",
        "\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/MDS7202/MDS7202/main/recursos/2024-01//LLM/memory.png' width=600 />\n",
        "</center>\n",
        "\n",
        "**5. Agentes**: Los agentes en LangChain toman decisiones sobre acciones seg√∫n la entrada o el estado del flujo, como buscar informaci√≥n o llamar a una API, lo que permite crear aplicaciones interactivas y adaptables.\n",
        "\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/MDS7202/MDS7202/main/recursos/2024-01//LLM/agents.png' width=450  />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOc7Ks-AnJBY"
      },
      "source": [
        "## **Manos a la Obra! üë∑‚Äç‚ôÇÔ∏è**\n",
        "\n",
        "<center>\n",
        "<img src='https://www.yorokobu.es/src/uploads/2014/03/yes.gif' width=350  />\n",
        "</center>\n",
        "\n",
        "Ya que conocemos los conceptos b√°sicos de los LLM y LangChain, pasemos ahora a estudiar como implementar estos conceptos en nuestro c√≥digo. Primero instalamos algunas librer√≠as necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB_6E1GB54Uu",
        "outputId": "97366c5d-5125-4c89-91e5-189bc9339baa"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-4uHrBaoBxv"
      },
      "source": [
        "Luego, pasemos a usar un LLM. Para esta clase utilizaremos `gemini-1.5-flash` de `Google`, pero esto lo pueden cambiar a futuro si disponen de diferentes recursos (por ejemplo, por alg√∫n modelo de `OpenAI` o `Azure`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL4pOXiA5vH5",
        "outputId": "e41c0ca3-cd5c-4c9f-c60e-38c566584484"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\", # modelo de lenguaje\n",
        "    temperature=0, # probabilidad de \"respuestas creativas\"\n",
        "    max_tokens=None, # sin tope de tokens\n",
        "    timeout=None, # sin timeout\n",
        "    max_retries=2, # n√∫mero m√°ximo de intentos\n",
        ")\n",
        "\n",
        "llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7j0mQMovwj"
      },
      "source": [
        "Con el modelo ya levantado, podemos interactuar con √©l de la misma forma que lo hacemos con [ChatGPT](https://chatgpt.com/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tRnzqyLe5yyI",
        "outputId": "3cf77f2f-08ff-422e-8b8e-85957297f7d0"
      },
      "outputs": [],
      "source": [
        "question = \"hola!\"\n",
        "response = llm.invoke(question) # invoke para interactuar con el modelo\n",
        "response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QrI3KAfpIj-"
      },
      "source": [
        "Podemos mejorar la calidad de las respuestas mediante **prompts**, es decir, entregando instrucciones al modelo para que entregue respuestas de alguna manera espec√≠fica. Para esto, LangChain nos facilita la opci√≥n de crear **templates**, los cuales son **instrucciones pre establecidas** que el modelo deber√° seguir para responder una pregunta. Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTtBYqVI6vuA",
        "outputId": "718c68be-7552-42fd-ee09-2599b573a3c7"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# template: noten como la variable {question} va a ser completada con la pregunta del usuario.\n",
        "template = '''\n",
        "Eres un profesional experto en formula 1.\n",
        "Tu √∫nico rol es responder de la forma m√°s completa posible la pregunta del usuario.\n",
        "\n",
        "Pregunta: {question}\n",
        "Respuesta √∫til:\n",
        "'''\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xFbxHeOYEG"
      },
      "source": [
        "Noten como nuestro prompt comparte el m√©todo `.invoke`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVFPSdfmOJvW",
        "outputId": "6f24e265-d435-44a7-8eae-0cf4261b0a20"
      },
      "outputs": [],
      "source": [
        "print(prompt.invoke(\"hola\").text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxaJK2qPrQGY"
      },
      "source": [
        "Esto pasa porque tanto el `ChatModel` como `PromptTemplate` implementan la interfaz `LCEL` (Langchain Expressi√≥n Language). Con esto en consideraci√≥n, podemos juntar el LLM y el prompt creado!\n",
        "\n",
        "En `langchain` podemos utilizar el operador `|` para **concatenar** diferentes acciones que queremos que nuestra aplicaci√≥n realice (a esto se le llama **chains**). En este caso, deseamos que la LLM utilice una plantilla para responder. Esto lo hacemos de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eksr2ciNqGel",
        "outputId": "eacf7b39-f659-46f8-d4dd-47664602ac53"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm # definimos la cadena\n",
        "response = chain.invoke(\"hola!\") # interactuamos con ella a trav√©s de invoke\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ2ITR65tATp",
        "outputId": "73734924-87f5-463c-8b3a-bdd28735aca8"
      },
      "outputs": [],
      "source": [
        "print(response.content) # print a la respuesta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_aOYAoYM1vr"
      },
      "source": [
        "Noten como la respuesta del LLM es un `AIMessage`, el cual contiene la respuesta y metadata adicional con respecto al mensaje entregado.\n",
        "\n",
        "Para recuperar s√≥lo el mensaje, podemos usar `StrOutputParser` en la chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sn-znCYNXTm",
        "outputId": "f24d1894-e70c-414e-f43f-fda951f81db9"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = prompt | llm | StrOutputParser() # definimos la cadena\n",
        "response = chain.invoke(\"qui√©n es el mejor piloto de la formula 1?\") # interactuamos con ella a trav√©s de invoke\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1FRHW9Hsx7v"
      },
      "source": [
        "Similar al caso anterior, tambi√©n podemos definir **m√°s de una variable** de entrada en nuestro prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsFz_IJv66DP",
        "outputId": "5a3c51fe-b803-4da0-9404-5fd9289c058c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = '''\n",
        "Eres un profesional experto en formula 1.\n",
        "Tu √∫nico rol es responder de la forma m√°s completa posible la pregunta del usuario.\n",
        "Adem√°s, debes responder con el idioma que se te indique.\n",
        "\n",
        "Pregunta: {question}\n",
        "Idioma: {language}\n",
        "Respuesta √∫til:\n",
        "'''\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | llm\n",
        "response = chain.invoke({\"question\": \"hola!\", \"language\": \"chinese\"}) # noten como ahora invoke recibe un diccionario\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D_uSu_vtgxL"
      },
      "source": [
        "## Experimento: Conversaci√≥n filos√≥fica entre 2 LLM üò±"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu6eye4zxVp3"
      },
      "source": [
        "Ya que conocemos lo b√°sico para interactuar con LLMs a trav√©s de c√≥digo, podemos ejecutar algunos experimentos interesantes. Por ejemplo, **qu√© pasar√≠a si hacemos que 2 LLM conversen sobre filosof√≠a entre s√≠?**\n",
        "\n",
        "Probemos!\n",
        "\n",
        "<center>\n",
        "<img src='https://tvquotes.co/wp-content/uploads/2017/09/you-pass-butter.gif' width=400/>\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzg-TpqFtjiV",
        "outputId": "dea4c79c-fffb-4af6-e9fd-aa0d26b717e8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# instrucciones generales\n",
        "template = \"\"\"\n",
        "Eres un experto en filosof√≠a.\n",
        "\n",
        "Responde la conversaci√≥n de manera acorde.\n",
        "{question}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template) # prompt\n",
        "\n",
        "# chains\n",
        "llm_1 = (prompt\n",
        "         | llm\n",
        "         | StrOutputParser()\n",
        "         )\n",
        "\n",
        "llm_2 = (prompt\n",
        "         | llm\n",
        "         | StrOutputParser()\n",
        "         )\n",
        "\n",
        "initial_msg = \"me llamo sebasti√°n\" # mensaje inicial\n",
        "msg_1, msg_2 = None, None # mensaje de cada agente\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "    print(f\"INTERACCI√ìN {i}\")\n",
        "\n",
        "    msg_1 = llm_1.invoke(initial_msg if msg_2 is None else msg_2) # llm_1 recibe msg_2 y genera msg_1\n",
        "    msg_2 = llm_2.invoke(msg_1) # llm_2 recibe msg_1 y genera msg_2\n",
        "\n",
        "    # print de mensajes\n",
        "    print(\"Agente_1:\", msg_1)\n",
        "    print(\"Agente_2:\", msg_2)\n",
        "    print('----' * 50)\n",
        "\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRJUNJsJtbxX"
      },
      "source": [
        "Volvamos ahora a la clase.\n",
        "\n",
        "**Qu√© pasar√≠a si le pregunto al LLM alg√∫na pregunta de actualidad?** Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIVanvymGFT3",
        "outputId": "65c54aea-d035-49b8-b0b1-c01cfeea06bf"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(\"qu√© sabes sobre las elecciones municipales 2024 de Chile?\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDz_UFJLtxsQ"
      },
      "source": [
        "**Pregunta**: ¬øPorqu√© podr√≠a estar pasando esto? ¬øQu√© podemos hacer para arreglarlo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGi4E1NnF_Bd"
      },
      "source": [
        "## **Retrieval Augmented Generation (RAG) üîéüìñ**\n",
        "\n",
        "**Retrieval Augmented Generation (RAG)** es una t√©cnica para **ampliar el conocimiento** de los modelos de lenguaje grandes (LLMs) con **datos adicionales**, usualmente provenientes de **fuentes externas**. Si bien podemos hacer RAG con cualquier tipo de informaci√≥n externa, para esta secci√≥n nos concentraremos en hacer RAG sobre **documentos PDF**. En otras palabras, estaremos **nutriendo a nuestro LLM** con la informaci√≥n contenida en uno o m√°s **documentos con extensi√≥n .pdf**.\n",
        "\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/MDS7202/MDS7202/main/recursos/2024-01//LLM/rag-framework.webp' width=450/>\n",
        "</center>\n",
        "\n",
        "### **¬øC√≥mo funciona?**\n",
        "\n",
        "La idea principal de esta soluci√≥n es **responder la pregunta del usuario usando s√≥lo los fragmentos de texto relevantes** para la pregunta en cuesti√≥n.\n",
        "\n",
        "Con esto en consideraci√≥n, para implementar una soluci√≥n RAG sobre documentos PDF debemos pasar por 2 grandes etapas:\n",
        "\n",
        "**1. Indexing**: Es el proceso por el cual **transformamos nuestros documentos** (.pdf) a **representaciones vectoriales**. Este es un paso que **se realiza s√≥lo una vez**, pero fudamental para recuperar los fragmentos de texto relevantes.\n",
        "\n",
        " A su vez, para indexar un documento es necesario pasar por los siguientes pasos:\n",
        " - **Load**: **Ingesta del documento** al ambiente de trabajo (e.g: lectura y almacenamiento en strings).\n",
        " - **Split**: **Separaci√≥n del documento en chunks de texto**. Chunks muy grandes pueden diluir la informaci√≥n necesaria para responder, chunks muy chicos pueden inducir a que se pierda el contexto.\n",
        " - **Embed**: Cada uno de los **chunks generados se transforman en representaciones vectoriales**, los cuales tienen la capacidad de almacenar la sem√°ntica, sintaxis y el contexto del chunk. Para esto, se utilizan modelos de lenguaje especializados en este tipo de transformaciones.\n",
        " - **Store**: Con los documentos vectorizados, **se almacenan estas representaciones** en una *vector store*.\n",
        "\n",
        "<center>\n",
        "<img src='https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png' width=700/>\n",
        "</center>\n",
        "\n",
        "**2. Retrieval and Generation**: Es el proceso en la que proporcionamos informaci√≥n al LLM para que responda de manera acorde. Consta de las siguientes etapas:\n",
        "- **Retrieve**: Proceso por el cual **se buscan los *top k* chunks m√°s relevantes a la pregunta**. Para lograr esto, se utilizan t√©cnicas de recuperaci√≥n de informaci√≥n.\n",
        "- **Generate**: **Se entregan los *top k*** chunks al LLM y **se responde la pregunta** del usuario.\n",
        "\n",
        "<center>\n",
        "<img src='https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png' width=700/>\n",
        "</center>\n",
        "\n",
        "Ya que conocemos el funcionamiento b√°sico de una soluci√≥n RAG, pasemos a ilustrar esto con un ejemplo. Para esto, construiremos un RAG a partir del  [Informe de las Elecciones Municipales 2024](https://www.decidechile.cl/informes/informe-municipales-2024/) realizado por [Decide Chile](https://www.decidechile.cl/).\n",
        "\n",
        "Primero instalamos las librerias necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lLqj0Ia-C0x",
        "outputId": "2e82e444-8ce7-4783-ad2a-7e8e00ed6043"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet faiss-cpu langchain_community pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0VF5pRghDSr"
      },
      "source": [
        "### **Indexing**\n",
        "\n",
        "Recordando los pasos descritos al principio de esta secci√≥n, la primera tarea para implementar una soluci√≥n RAG es **indexar los documentos**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnzUiZDqggNg"
      },
      "source": [
        "#### **Load**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBHdPSAThBB0"
      },
      "source": [
        "Ya que nuestro documento es de extensi√≥n .pdf, comenzaremos **ingestando el documento** usando `PyPDFLoader` (pueden consultar m√°s loaders disponibles en la siguiente [p√°gina](https://python.langchain.com/docs/integrations/document_loaders/)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUvL_5GflzJz",
        "outputId": "ae735685-736f-4c9c-9591-668c98a6e5d2"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "file_path = \"https://gitlab.com/sebatinoco/datos/-/raw/main/Informe-DecideChile-Municipales-2024.pdf?inline=false\" # path al documento\n",
        "loader = PyPDFLoader(file_path) # inicializar loader de PDF\n",
        "\n",
        "docs = loader.load() # cargar documento\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBjYuHGPjLrV",
        "outputId": "11174158-4b28-4f25-a7fa-8bbc1ab8cbbd"
      },
      "outputs": [],
      "source": [
        "len(docs) # noten como se genera una lista de 55 elementos, en este caso cada elemento es una p√°gina"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXF7VUEhjD0h",
        "outputId": "5ad0ecc9-4617-460e-89ea-dcf2e2ab2ecd"
      },
      "outputs": [],
      "source": [
        "docs[0] # cada elemento es del tipo Document, el cual posee el contenido de cada p√°gina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j8Jo0tsgjZo"
      },
      "source": [
        "#### **Split**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujHLedrNjaJK"
      },
      "source": [
        "Despu√©s de haber ingestado el documento, el siguiente paso es dividir cada documento en chunks de texto m√°s peque√±os. Para eso usaremos `RecursiveCharacterTextSplitter` (nuevamente, pueden consultar otros tipos de splitters en el siguiente [link](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xc6aXk5AdEp",
        "outputId": "6eb35c27-3899-44c6-8918-7f31653ffc81"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) # inicializamos splitter\n",
        "splits = text_splitter.split_documents(docs) # dividir documentos en chunks\n",
        "splits[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHVhSmEekQ9J",
        "outputId": "36fdbe2b-ed13-407b-ece4-8eda46cce608"
      },
      "outputs": [],
      "source": [
        "splits[0] # cada elemento es un Document, esta vez con menos contenido que en el paso anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTbzpn5NgWnj",
        "outputId": "b3149b89-8213-4445-fbfe-97d1302bb0be"
      },
      "outputs": [],
      "source": [
        "len(splits) # noten como ahora contamos con 162 chunks (pues 1 p√°gina contiene m√°s de 1 chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCQA5oEag1KC"
      },
      "source": [
        "#### **Embed & Store**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFGNEXTWno6q"
      },
      "source": [
        "Con los documentos separados en chunks, pasamos ahora a transformarlos a *embeddings*. Para esto simplemente ocupamos embeddings de Google a trav√©s de `GoogleGenerativeAIEmbeddings` (de nuevo, pueden revisar m√°s opciones de embeddings en la siguiente [p√°gina](https://python.langchain.com/v0.1/docs/integrations/text_embedding/)).\n",
        "\n",
        "Por el lado del vector store, usaremos `FAISS` (nuevamente, pueden revisar m√°s opciones en este [link](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uhSxJ55Adsq",
        "outputId": "f486346c-7161-4e90-a555-9db5ff1b69f5"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") # inicializamos los embeddings\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding) # vectorizacion y almacenamiento\n",
        "vectorstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWzzbr-ChMu2"
      },
      "source": [
        "### **Retrieval and Generation**\n",
        "\n",
        "Con los documentos indexados, el siguiente paso es **habilitar que nuestro LLM pueda conseguir la informaci√≥n de los documentos** para responder preguntas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VED9GgAXhZMF"
      },
      "source": [
        "#### **Retrieve**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCqxMPaisbWK"
      },
      "source": [
        "Para esto, el primer paso es habilitar nuestro *vector store* para buscar los documentos m√°s relevantes. Para esto, simplemente usamos el m√©todo `.as_retriever`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3vvxfXgAwHc",
        "outputId": "4eb86dae-08ed-4d5b-d2c8-0d4b85d9d311"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", # m√©todo de b√∫squeda\n",
        "                                     search_kwargs={\"k\": 3}, # n¬∞ documentos a recuperar\n",
        "                                     )\n",
        "retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh4G9LyWs5XU"
      },
      "source": [
        "Con esto, podemos probar nuestro retriever:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1I1hiGk-APC",
        "outputId": "d8ea0f92-5dbe-41bd-abc4-6048533caf02"
      },
      "outputs": [],
      "source": [
        "question = \"como fue la participacion en las elecciones?\" # pregunta\n",
        "relevant_documents = retriever.invoke(question) # top k documentos relevantes a la pregunta\n",
        "relevant_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9zmZxyTtUOl"
      },
      "source": [
        "Noten como la informaci√≥n relevante est√° almaenada en el atributo **page_content**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "J2gmKpyIBwBu",
        "outputId": "faea2b5a-5073-4a95-d4a4-72c6ab27ba1e"
      },
      "outputs": [],
      "source": [
        "relevant_documents[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMHHJ08puIAk"
      },
      "source": [
        "Como solo nos interesa ese atributo, podemos definir la funci√≥n `format_docs` para procesar los chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "755y45N2uqjO",
        "outputId": "11e135da-1d5b-454a-e676-e2969079ebae"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "print(format_docs(relevant_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogUMJd4tu4rL"
      },
      "source": [
        "Finalmente, `LangChain` nos permite unir todos estos pasos en una **chain**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U17IJPu7FIOW",
        "outputId": "2bfd9a68-9dac-4330-c94e-c2a40775f16d"
      },
      "outputs": [],
      "source": [
        "retriever_chain = retriever | format_docs # chain\n",
        "print(retriever_chain.invoke(\"como fue la participacion en las elecciones?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV95K3OKhdiR"
      },
      "source": [
        "#### **Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP99sehr1LeA"
      },
      "source": [
        "Con el retriever ya listo, la √∫nica pieza faltante es **entregar los documentos relevantes al LLM** para que pueda responder de mejor manera.\n",
        "\n",
        "Para esto, primero definiremos un **template** para con las instrucciones a seguir:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW0vqLe_DRAM"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# noten como ahora existe el par√°metro de context!\n",
        "rag_template = '''\n",
        "Eres un asistente experto en la interpretaci√≥n de resultados electorales de la pol√≠tica chilena.\n",
        "Tu √∫nico rol es contestar preguntas del usuario a partir de informaci√≥n relevante que te sea proporcionada.\n",
        "Responde siempre de la forma m√°s completa posible y usando toda la informaci√≥n entregada.\n",
        "Responde s√≥lo lo que te pregunten a partir de la informaci√≥n relevante, NUNCA inventes una respuesta.\n",
        "\n",
        "Informaci√≥n relevante: {context}\n",
        "Pregunta: {question}\n",
        "Respuesta √∫til:\n",
        "'''\n",
        "\n",
        "rag_prompt = PromptTemplate.from_template(rag_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBEjVf8t2l3k"
      },
      "source": [
        "Finalmente, condensamos todo en una chain para recuperar informaci√≥n relevante y responder al mismo tiempo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaFffmmoC8FH",
        "outputId": "95d73224-9fd2-43f2-c439-a1176e51b46a"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever_chain, # context lo obtendremos del retriever_chain\n",
        "        \"question\": RunnablePassthrough(), # question pasar√° directo hacia el prompt\n",
        "    }\n",
        "    | rag_prompt # prompt con las variables question y context\n",
        "    | llm # llm recibe el prompt y responde\n",
        "    | StrOutputParser() # recuperamos s√≥lo la respuesta\n",
        ")\n",
        "\n",
        "question = \"que informaci√≥n tienes sobre nulos y blancos?\"\n",
        "response = rag_chain.invoke(question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0CPJZL6u3cV"
      },
      "source": [
        "## **Agentes üïµÔ∏è‚Äç‚ôÇÔ∏è**\n",
        "\n",
        "Ahora que ya conocemos c√≥mo implementar un RAG b√°sico sobre documentos PDF, buscaremos hacer lo mismo (es decir, nutrir nuestro LLM de fuentes externas) pero con dos diferencias:\n",
        "\n",
        "- Alimentaremos nuestro agente de la informaci√≥n proporcionada por **motores de b√∫squeda** (e.g: Google).\n",
        "- Remplazaremos las chains por **Agentes**.\n",
        "\n",
        "Todo genial, pero..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu71fPDqgY7o"
      },
      "source": [
        "### **¬øQu√© son los Agentes?**\n",
        "\n",
        "Similar a lo que vimos en la clase de RL, un agente se define como un modelo que tiene la capacidad para ejecutar **acciones**. De esta manera, el objetivo del agente es **elegir una <u>secuencia</u> de acciones** a realizar para cumplir con un objetivo espec√≠fico.\n",
        "\n",
        "> **Pregunta**: Ok, pero entonces cual es la diferencia con las chains?\n",
        "\n",
        "Si bien las **chains** tienen la capacidad de implementar una secuencia de acciones o pasos, estas acciones estan **programadas de manera fija** en el c√≥digo. En contraste, los **agentes** utilizan el **LLM como <u>motor de razonamiento</u>** para determinar **qu√© acciones tomar y en qu√© orden**.\n",
        "\n",
        "Ahora que ya conocemos qu√© es un Agente, conozcamos uno de los framework m√°s famosos para implementar Agentes con LLM.\n",
        "\n",
        "<center>\n",
        "<img src='https://media4.giphy.com/media/dBZsIa2eWkVBaU6lWY/giphy.gif' width=450/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6PMb0sO8srU"
      },
      "source": [
        "### **ReAct**\n",
        "\n",
        "Introducido en el paper de [Yao et al. (2022)](https://arxiv.org/abs/2210.03629), **ReAct** se presenta como un framework para que agentes puedan **razonar sobre acciones** a partir de observaciones. En particular, para cumplir un objetivo un agente basado en ReAct debe seguir la siguiente secuencia:\n",
        "\n",
        "- El agente **razona** sobre qu√© acci√≥n tomar.\n",
        "- En base al razonamiento hecho, el **agente ejecuta la acci√≥n**.\n",
        "- A partir de la acci√≥n ejecutada, el **agente observa y eval√∫a el nuevo escenario** (feedback).\n",
        "\n",
        "> **Pregunta**: ¬øQu√© parecido encuentran con lo que hemos visto hasta ahora?\n",
        "\n",
        "<center>\n",
        "<img src='https://peterroelants.github.io/images/llm/ReAct_loop.png' width=400/>\n",
        "</center>\n",
        "\n",
        "Por √∫ltimo, es importante se√±alar que Los resultados muestran como los **agentes basados en ReAct** puede **superar m√∫ltiples benchmarks en tareas de lenguaje y toma de decisiones**, adem√°s de mejorar la interpretabiliad y confiabilidad en los LLM.\n",
        "\n",
        "Pongamos en pr√°ctica lo aprendido con un ejemplo!\n",
        "\n",
        "Comencemos primero cargando un prompt predefinido del **hub** de langchain para usar ReAct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afuc0QUc-vDt",
        "outputId": "62bd9bbc-4c3c-483a-b4fa-ee17c4959849"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "\n",
        "react_prompt = hub.pull(\"hwchase17/react\") # template de ReAct\n",
        "print(react_prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIXUFXkvAEwm"
      },
      "source": [
        "> **Pregunta:** T√≥mense un momento para estudiar el prompt. ¬øQu√© variables recibe?\n",
        "\n",
        "Una de las ventajas de usar agentes es que tienen una f√°cil integraci√≥n con **tools**, es decir, **herramientas que puede usar el agente para lograr un objetivo** en particular.\n",
        "\n",
        "Para este caso particular, usaremos la tool del motor de b√∫squeda `Tavily` para permitir que nuestro agente pueda recuperar **informaci√≥n de la web** (pueden consultar m√°s tools en el siguiente [link](https://api.python.langchain.com/en/v0.1/community_api_reference.html#module-langchain_community.tools)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQdjpJY-vAIc"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "search = TavilySearchResults(max_results = 1) # inicializamos tool\n",
        "tools = [search] # guardamos las tools en una lista"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xIsr_QoEF9L"
      },
      "source": [
        "Con las tools definidas, podemos inicializar nuestro agente ReAct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQyhcqwn0VQM",
        "outputId": "8d2010b0-8e63-4f06-e349-00ea8df799fa"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "\n",
        "agent = create_react_agent(llm, tools, react_prompt) # primero inicializamos el agente ReAct\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) # lo transformamos a AgentExecutor para habilitar la ejecuci√≥n de tools\n",
        "agent_executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IQrb_6GFCnM"
      },
      "source": [
        "Finalmente, probamos nuestro agente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmnvG3ExEDa0",
        "outputId": "1d849862-9288-4686-ab9a-12b881707c12"
      },
      "outputs": [],
      "source": [
        "response = agent_executor.invoke({\"input\": \"qu√© equipo gan√≥ el mundial de LoL 2024?\"})\n",
        "print(response[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Ce0kdfuOOH"
      },
      "source": [
        "### **Implementando nuestras propias tools**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueOUdQzkGlAl"
      },
      "source": [
        "Algo interesante que podemos hacer es **programar nuestras propias tools** para que el agente interact√∫e con ellas.\n",
        "\n",
        "Revisemos un ejemplo en que programos tools con algunas **operaciones matem√°ticas**:\n",
        "\n",
        "<center>\n",
        "<img src='https://media1.tenor.com/images/dfe0c1c6eaf41b91996aacee0879ebc2/tenor.gif?itemid=3486402' width=400  />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6vdEHI8GrXG"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def multiply(x: int or float, y: int or float) -> float:\n",
        "    \"\"\"Multiply 'x' times 'y'.\"\"\"\n",
        "    return float(x * y)\n",
        "\n",
        "@tool\n",
        "def exponentiate(x: int or float, y: int or float) -> float:\n",
        "    \"\"\"Raise 'x' to the 'y'.\"\"\"\n",
        "    return float(x**y)\n",
        "\n",
        "@tool\n",
        "def add(x: int or float, y: int or float) -> float:\n",
        "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
        "    return float(x + y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnvwvsi8wd0o"
      },
      "source": [
        "Luego, simplemente agrupamos las tools en una lista:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLdQCpOEwj42"
      },
      "outputs": [],
      "source": [
        "tools = [add, multiply, exponentiate]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Boytyrf9wlwU"
      },
      "source": [
        "En paralelo, crearemos un **prompt** para nuestro agente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_2aI1WMxKHd",
        "outputId": "4895c70a-d9df-4c07-951c-12e9b07922b7"
      },
      "outputs": [],
      "source": [
        "# noten como ahora se incluye la variable agent_scratchpad\n",
        "math_template = \"\"\"\n",
        "Eres un asistente experto en matem√°ticas.\n",
        "Tu √∫nico rol es responder la pregunta del usuario usando las tools disponibles.\n",
        "\n",
        "Pregunta: {input}\n",
        "{agent_scratchpad}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(math_template)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62rbgyi5yPpA"
      },
      "source": [
        "Con el prompt creado, pasamos a **crear nuestro agente**.\n",
        "\n",
        "Noten que como nuestras tools reciben m√°s de un par√°metro de entrada (a y b), **remplazaremos ReAct por [Tool Calling](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/)** (de igual manera, pueden encontrar todos los tipos de Agentes disponibles y sus limitantes en el siguiente [link](https://python.langchain.com/v0.1/docs/modules/agents/agent_types/)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awjcOSa1tpVz",
        "outputId": "9a2cf5cf-9461-45ed-c3aa-02b162860f02"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_tool_calling_agent\n",
        "\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "math_agent = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "math_agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWJtWIw4zeKq"
      },
      "source": [
        "Finalmente, podemos probar el funcionamiento de nuestro agente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3JPN3nSzBn1",
        "outputId": "a18e409e-1e6e-4f7b-851d-d0cbad68bb80"
      },
      "outputs": [],
      "source": [
        "math_agent.invoke({\"input\": \"cuanto es 10 ** 3 + 5 * 1.4?\",})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhfkEYvU5KAU"
      },
      "source": [
        "## **Soluciones Multi Agente üë®‚Äçüë©‚Äçüë¶‚Äçüë¶**\n",
        "\n",
        "<center>\n",
        "<img src='https://media.tenor.com/FApRE_u99tgAAAAC/teamwork-team-game.gif' width=400  />\n",
        "</center>\n",
        "\n",
        "En las secciones pasadas habilitamos agentes que puedan hacer RAG sobre fuentes externas:\n",
        "\n",
        "- Una **chain** que responde preguntas de las √∫ltimas elecciones municipales en base a un informe en PDF\n",
        "- Un **agente** que responde preguntas matem√°ticas a partir de tools creadas manualmente.\n",
        "\n",
        "Con esto en consideraci√≥n, nace la pregunta natural: **¬øQu√© pasa si combinamos ambas soluciones?**\n",
        "\n",
        "El objetivo de esta secci√≥n es introducirlos al paradigma **multiagente**, es decir, **combinar 2 o m√°s funcionalidades en un mismo chat**. En particular, buscaremos implementar una arquitectura simple de enrutamiento, la cual consta de 4 agentes:\n",
        "\n",
        "- **Agente router**, el cual recibe y dirige la pregunta del usuario a alguno de los agentes.\n",
        "- **Agente de elecciones**: responde preguntas sobre las elecciones municipales 2024\n",
        "- **Agente experto en matem√°ticas**: responde preguntas matem√°ticas\n",
        "- **Agente de redireccionamiento**: en caso de que la pregunta del usuario no pertenezca a alguno de los temas anteriores, invita al usuario a reorientar su pregunta (esto es √∫til para evitar preguntas maliciosas).\n",
        "\n",
        "**Nota**: Para efectos de esta secci√≥n y por simplicidad, no se hace distinci√≥n entre Agente y Chain.\n",
        "\n",
        "<center>\n",
        "<img src='https://preview.redd.it/smart-orchestrator-router-for-multiple-specialized-llms-v0-gjgkmlbu3jlc1.png?width=627&format=png&auto=webp&s=13ef701d45f642ce36ae8e99cb172b903fe7d36b' width=600  />\n",
        "</center>\n",
        "\n",
        "**Importante: La implementaci√≥n es s√≥lo ilustrativa y <u>no cumple con el est√°ndar actual recomendado por LangChain</u>. Si desean conocer mejor c√≥mo implementar soluciones multiagente, les recomiendo estudiar [LangGraph](https://www.langchain.com/langgraph)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkVE826M7KBf"
      },
      "source": [
        "#### **Agente Router**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jro3pKvN7Nx5"
      },
      "source": [
        "Primero comenzamos creando nuestro agente router:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Fwa_3Gj90luf",
        "outputId": "ef7205b8-0b57-4243-fe13-bad8bdeb3107"
      },
      "outputs": [],
      "source": [
        "router_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Eres un asistente experto en la clasificaci√≥n de preguntas del usuario.\n",
        "    Tu √∫nico rol es clasificar preguntas del usuario en las categor√≠as 'elecciones', 'math', u 'otro' seg√∫n el siguiente criterio:\n",
        "    - 'elecciones': Cuando la pregunta sea relacionada con las elecciones municipales de chile 2024.\n",
        "    - 'math': Cuando la pregunta sea relacionada a preguntas de matem√°ticas\n",
        "    - 'otro': Todo aquella pregunta que no est√© contenida en las categor√≠as anteriores.\n",
        "\n",
        "    No respondas con m√°s de una palabra y no incluyas.\n",
        "\n",
        "    <pregunta>\n",
        "    {question}\n",
        "    </pregunta>\n",
        "\n",
        "    Categor√≠a:\"\"\"\n",
        ")\n",
        "\n",
        "router_chain = (\n",
        "    router_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "router_chain.invoke({\"question\": \"cuanto es 2+2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST8NEJ517Spw"
      },
      "source": [
        "#### **Agente Redirect**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OXBCeUI7YjB"
      },
      "source": [
        "Repetimos lo mismo para crear nuestro agente de redireccionamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bNCjgieC_JNF",
        "outputId": "8d7be134-2406-4d48-b8f0-de4360f8eb6c"
      },
      "outputs": [],
      "source": [
        "redirect_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Eres un asistente experto en el redireccionamiento de preguntas de usuarios.\n",
        "    Vas a recibir una pregunta del usuario, tu √∫nico rol es indicar que no puedes responder su pregunta y redireccionar al usuario\n",
        "    para que te pregunte sobre las elecciones municipales de Chile 2024 o c√°lculos matem√°ticos.\n",
        "\n",
        "    Recuerda ser amable y cordial en tu respuesta.\n",
        "\n",
        "    Pregunta: {question}\n",
        "    Respuesta cordial:\"\"\"\n",
        ")\n",
        "\n",
        "redirect_chain = (\n",
        "    redirect_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "redirect_chain.invoke({\"question\": \"dame la receta para hacer una pizza\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TrJi-By9az-"
      },
      "source": [
        "#### **Juntando todo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu2WZ1vw9c_o"
      },
      "source": [
        "Finalmente, podemos juntar todo lo que hemos desarrollado en una sola funci√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMCkpmgn7BOU"
      },
      "outputs": [],
      "source": [
        "def route_question(question):\n",
        "  '''\n",
        "  Recibe una pregunta de usuario.\n",
        "  Rutea la pregunta al agente respectivo y responde de manera acorde.\n",
        "  '''\n",
        "\n",
        "  topic = router_chain.invoke({\"question\": question}) # enrutamiento\n",
        "\n",
        "  if \"elecciones\" in topic: # si la pregunta es sobre las elecciones, utilizar cadena\n",
        "      return rag_chain.invoke(question)\n",
        "  elif \"math\" in topic: # si la pregunta es de matem√°ticas, utilizar agente\n",
        "      return math_agent.invoke({\"input\": question})[\"output\"]\n",
        "  else: # de lo contrario, redireccionar pregunta\n",
        "      return redirect_chain.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFGSYoq8AmPx"
      },
      "source": [
        "Para finalmente hacer pruebas de su funcionamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PXyQed6MlHe",
        "outputId": "8bb3675d-1d94-4635-d478-15661717e7eb"
      },
      "outputs": [],
      "source": [
        ":print(route_question(\"c√≥mo puedo hacerme millonario?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnniS37e9SOI",
        "outputId": "45fdafd9-27ea-4de9-8835-5f6b3f9c8689"
      },
      "outputs": [],
      "source": [
        "print(route_question(\"como fue la participaci√≥n de las elecciones?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "951mqY0cL2nu",
        "outputId": "55e794ad-c5db-4c73-abb6-eaf9aa3d00f4"
      },
      "outputs": [],
      "source": [
        "print(route_question(\"cuanto es 5+3?\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
