{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1111d3ad-458e-475b-ab30-0017e4117e8a",
   "metadata": {},
   "source": [
    "# Clase 23: Despliegue\n",
    "\n",
    "**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**\n",
    "\n",
    "- Conocer como exportar modelos de ML usando `pickle`\n",
    "- Aprender a montar ambientes virtuales con `conda`\n",
    "- Interiorizar al estudiante sobre aplicaciones web y su división en Front End y Back End\n",
    "- Levantar una aplicación web usando `FastAPI` y `Gradio`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec262af",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "Esta clase tiene como objetivo introducir a los estudiantes a algunas herramientas para desplegar modelos de ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe4da8",
   "metadata": {},
   "source": [
    "## Datos de esta clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594a4fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Para esta clase, ejemplificaremos lo aprendido utilizando el clásico **Iris Dataset**. El objetivo es simple: entrenar un modelo de ML para clasificar flores del tipo iris en sus 3 categorías: setosa, versicolor y virginica. Las características disponibles son 4: el largo y ancho del pétalo y sépalo.\n",
    "\n",
    "![Iris Dataset](../../recursos/2024-01/despliegue/images/iris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94c037",
   "metadata": {},
   "source": [
    "Comencemos primero importando los datos. Podemos hacer esto de manera simple usando la API de `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_df = load_iris(as_frame=True) # cargar dataset\n",
    "X = iris_df[\"data\"] # features para predecir\n",
    "y = iris_df[\"target\"] # variable target, 0: setosa, 1: versicolor, 2: viginica\n",
    "\n",
    "# features disponibles\n",
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable a predecir\n",
    "y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab1252",
   "metadata": {},
   "source": [
    "Con los datos ya ingestados, podemos entrenar un clasificador de `RandomForest` de manera simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "seed = 3380\n",
    "\n",
    "# separamos los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state = seed)\n",
    "\n",
    "model = RandomForestClassifier(random_state = seed) # instanciar modelo\n",
    "model.fit(X_train, y_train) # fit\n",
    "\n",
    "y_pred = model.predict(X_test) # predict sobre X_test\n",
    "accuracy_score(y_test, y_pred) # performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cce3e3",
   "metadata": {},
   "source": [
    "Genial! Entrenamos efectivamente un modelo de ML para resolver nuestro problema :)\n",
    "\n",
    "Del entrenamiento, notamos que nuestro modelo tiene una alta capacidad de predicción, acertando el 97.7% de los casos en el conjunto de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab87708",
   "metadata": {},
   "source": [
    "## Entrené mi modelo... ¿y ahora qué?\n",
    "\n",
    "<center>\n",
    "<img src='https://media1.tenor.com/m/NeOTch1WXawAAAAd/finding-nemo-bags.gif' width=450  />\n",
    "</center>\n",
    "\n",
    "Supongamos que tenemos un cercano (jefe, colega, mamá, hermano, perro, etc) al que queramos mostrarle el funcionamiento de nuestro modelo. ¿Qué es lo que tendríamos que hacer para lograr esto?\n",
    "\n",
    "Una primera idea sería mostrarles directamente nuestro código montado en nuestro equipo y mostrar como se ejecuta... aunque esto dificilmente es una solución aceptable en términos productivos. \n",
    "\n",
    "Para atacar este problema de manera efectiva, nos dedicaremos el resto de la clase a aprender diferentes técnicas de **despliegue** de nuestro modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80350a31",
   "metadata": {},
   "source": [
    "## Exportar modelo\n",
    "\n",
    "Antes de desplegar el modelo entrenado, el primer paso es **serializar** nuestro modelo para que luego sea ingestado en otro dispositivo.\n",
    "\n",
    "Podemos lograr esto usando la librería `pickle`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7303b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# crear \"model.pkl\" con nuestro modelo serializado\n",
    "with open('./model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1c514",
   "metadata": {},
   "source": [
    "Podemos validar que nuestro modelo se carga de manera efectiva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f983eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borramos modelo\n",
    "del model\n",
    "\n",
    "# noten como ya no podemos predecir sobre X pues model ya no existe\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos modelo\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# verificamos funcionamiento\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ed5cc",
   "metadata": {},
   "source": [
    "## Esencia del despliegue\n",
    "\n",
    "Pasemos ahora al despliegue de nuestro modelo. De forma general, cualquier herramienta de despliegue debería ser capaz de:\n",
    "\n",
    "0. Cargar nuestro modelo \n",
    "1. Recibir nuevos datos (e.g: una nueva *fila* de $X$)\n",
    "2. Pre procesar los datos de manera adecuada (escalar, transformar a one_hot, crear nuevas features, etc)\n",
    "3. Generar una predicción sobre los datos procesados\n",
    "4. Retornar la predicción generada\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src='../../recursos/2024-01/despliegue/images/diagrama_despliegue.png' width = 650/>\n",
    "</div>\n",
    "\n",
    "> **Pregunta:** ¿Qué herramienta aprendida en el curso nos puede ayudar a los pasos 2 y 3?\n",
    "\n",
    "Veamos como se vería esto de manera conceptual en el código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0258692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar modelo\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "labels_dict = {0: 'setosa', 1: 'versicolor', 2: 'virginica'} # diccionario de etiquetas\n",
    "def make_prediction(sepal_length: float, sepal_width: float, petal_length: float, petal_width: float):\n",
    "    '''\n",
    "    función que devuelve la predicción del modelo dado un set de atributos\n",
    "    '''\n",
    "\n",
    "    # mantener el orden!\n",
    "    features = [\n",
    "        [sepal_length, sepal_width, petal_length, petal_width] # obs a predecir, OJO con el orden!! \n",
    "    ]\n",
    "    \n",
    "    prediction = model.predict(features).item() # generar prediccion\n",
    "    label = labels_dict[prediction] # transformar a etiqueta\n",
    "\n",
    "    return label # retornar prediccion\n",
    "\n",
    "make_prediction(sepal_length=2, sepal_width=1, petal_length=1, petal_width=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a7adcc",
   "metadata": {},
   "source": [
    "Noten que si queremos cambiar los atributos de entrada, simplemente debemos cambiar los valores de los parámetros de la función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction(sepal_length=5, sepal_width=4, petal_length=5, petal_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c93ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction(sepal_length=1, sepal_width=7, petal_length=1.7, petal_width=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76654e53",
   "metadata": {},
   "source": [
    "Guardaremos una copia de esta función en `recursos/2024-02/make_prediction.py` para volver a utilizarla más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d74dcd",
   "metadata": {},
   "source": [
    "## Ambientes virtuales\n",
    "\n",
    "La idea de esta sección es **generar las condiciones necesarias para que nuestro proyecto pueda ser ejecutado otros dispositivos** de manera fácil. Una primera aproximación para lograr esto es a través de **entornos virtuales**, los cuales pueden ser interpretados como un espacio virtual aislado donde podemos instalar y levantar configuraciones *custom* de nuestro proyecto.\n",
    "\n",
    "> **Pregunta:** ¿Se les ocurre algún ejemplo donde sea útil hacer uso de estos ambientes?\n",
    "\n",
    "### ¿Cómo crear nuestro ambiente virtual?\n",
    "\n",
    "El primer paso para trabajar sobre ambientes virtuales es instalar [anaconda](https://www.anaconda.com). Con la distribución instalada, podemos levantar un ambiente virtual ejecutando en la terminal los siguientes comandos:\n",
    "\n",
    "```bash\n",
    "conda create --name nombre_ambiente python=version_python -y # crear ambiente virtual\n",
    "```\n",
    "\n",
    "donde *version_python* equivale a la versión de python deseada (3.9, 3.10, 3.11, etc). Luego de crear nuestro ambiente, es necesario entrar a este por medio de:\n",
    "\n",
    "```bash\n",
    "conda activate nombre_ambiente # activar ambiente virtual\n",
    "```\n",
    "\n",
    "```bash\n",
    "conda deactivate # desactivar ambiente virtual\n",
    "```\n",
    "\n",
    "### Levantando un proyecto en un ambiente virtual\n",
    "\n",
    "Para levantar un proyecto en un ambiente virtual, necesitaremos de los siguientes elementos:\n",
    "\n",
    "- **main.py:** Código python a ejecutar (basicamente el código python que programaron en su proyecto)\n",
    "- **requirements.txt:** Archivo con las liberias necesarias para ejecutar `main.py` (pandas==2.2.2, etc.)\n",
    "- **.env:** Archivo con las credenciales necesarias para ejecutar el proyecto (API KEYS, etc.)\n",
    "\n",
    "Noten que este ambiente de python es un ambiente *virgen* y no tiene ninguna librería instalada. Pueden verificar lo anterior por medio de:\n",
    "\n",
    "```bash\n",
    "pip freeze # printear librerias instaladas por pip\n",
    "```\n",
    "\n",
    "Para instalar las librerias necesarias, lo pueden realizar de manera simple por medio de:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Al contrario, si desean exportar las librerias utilizadas en su proyecto a un archivo `requirements.txt`:\n",
    "\n",
    "```bash\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "**Importante**: Noten que usando ambientes virtuales, somos capaces de montar nuestro modelo de manera fácil en otro equipo! Sólo necesitaríamos:\n",
    "- Modelo (`model.pkl`)\n",
    "- Librerías (`requirements.txt`)\n",
    "- Código de ejecución (`make_predictions.py` o `main.py`)\n",
    "\n",
    "> **Pregunta**: ¿Conocen otra herramienta que pueda resolver el mismo problema?\n",
    "\n",
    "#### Paréntesis: Exportando librerías de ambientes de `conda`\n",
    "\n",
    "Al trabajar con ambientes de `conda`, algunas librerias quedan antecedidas por \"@\", por ejemplo:\n",
    "\n",
    "```text\n",
    "zipp @ file:///Users/builder/cbouss/perseverance-python-buildout/croot/zipp_1707348942775/work\n",
    "```\n",
    "\n",
    "Esto es un **problema** pues levantará un error para instalar las librerías en un nuevo entorno. Pueden solucionar esto de manera fácil exportando sus librerías a través de:\n",
    "\n",
    "```bash\n",
    "pip list --format=freeze > requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69de2c",
   "metadata": {},
   "source": [
    "## Aplicación Web\n",
    "\n",
    "Como ya conocemos lo básico para montar nuestro modelo en otros dispositivos, profundicemos ahora en cómo desplegar nuestro modelo para que terceros puedan interactuar con nuestro trabajo.\n",
    "\n",
    "La forma más robusta para desplegar una solución basada en machine learning es a través de una **aplicación web**, es decir, una aplicación almacenada en un servidor remoto con la que podamos interactuar a través de un navegador web.\n",
    "\n",
    "Una aplicación web se compone de 2 elementos principales:\n",
    "- **Frontend**: Lo que se muestra al usuario.\n",
    "- **Backend**: Procesamiento \"tras bambalinas\".\n",
    "\n",
    "> **Pregunta:** ¿Es posible levantar el back sin el front, o vice versa?\n",
    "\n",
    "A lo largo de esta sección veremos en detalle cómo implementar cada uno de estos componentes. Manos a la obra!\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FcJQX2zzna7-rdEocH3jYw.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f1877",
   "metadata": {},
   "source": [
    "### Front End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a8954",
   "metadata": {},
   "source": [
    "### ¿Qué es el Front End?\n",
    "\n",
    "<center>\n",
    "<img src='https://media1.tenor.com/m/DfXYNBOTEQ8AAAAd/react-fron-end.gif' width=300  />\n",
    "</center>\n",
    "\n",
    "En el contexto de aplicaciones web, **Front End se refiere a la parte de la aplicación que interactúa con los usuarios** (conocida también como el *Cliente*). En términos sencillos, es todo lo que vemos en la pantalla cuando accedemos a un sitio web o aplicación, por ejemplo: tipos de letra, colores, formato para diferentes tamaños de pantalla, desplazamientos, efectos visuales, etc.\n",
    "\n",
    "Algunos *framework* comunes para desarrollar componentes de Front End son:\n",
    "\n",
    "- `React`\n",
    "- `Vue`\n",
    "- `Angular`\n",
    "- `Gradio`\n",
    "- `Streamlit`\n",
    "\n",
    "Si bien la mayoria de estos framework son basados en Javascript, `Gradio` y `Streamlit` utilizan Python para levantar el Cliente (aunque por debajo igual utilizan Javascript). Para esta clase nos enfocaremos en `Gradio`.\n",
    "\n",
    "Comencemos!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2c305",
   "metadata": {},
   "source": [
    "### Gradio\n",
    "\n",
    "*Esta sección está basada en el siguiente [tutorial](https://www.youtube.com/watch?v=97KxA1r184o)*.\n",
    "\n",
    "![Gradio](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQdC1xNkDaMNEbtQRyupw5v32HSGVA6w0zNjA&s)\n",
    "\n",
    "[gradio](https://www.gradio.app) es una librería de Python similar a `Streamlit` que permite generar **demos** de manera simple especificando los **componentes** de entrada y salida esperados por tu modelo de machine learning.\n",
    "\n",
    "¿A qué nos referimos con componentes de entrada y salida? Gradio viene con diferentes componentes para diferentes tipos de modelos de machine learning. Algunos ejemplos:\n",
    "\n",
    "*   Para un **clasificador de imágenes**, el input esperado es de tipo `Image` y la salida es del tipo `Label`.\n",
    "*   Para un modelo de **speech recognition**, el input esperado es del tipo `Microphone` (lo que permite al usuario grabar desde el navegador) o `Audio` (lo que permite a los usuarios subir sus propios archivos de audio), mientras que la salida es del tipo `Text`.\n",
    "* Para un modelo de **questiong answering**, se esperan dos entradas: [`Text`, `Text`], una entrada de texto para el párrafo y otro texto para la pregunta, mientras que la salida es del tipo `Text` para contener la respuesta generada.\n",
    "\n",
    "Una lista completa de los componentes habilitados se puede encontrar en la [documentación](https://gradio.app/docs/).\n",
    "\n",
    "Además de los componentes de entrada y salida, Gradio espera un tercer parámetro: **la función de predicción**. Este parámetro puede ser ***cualquier* función regular de Python** que reciba los parámetros correspondientes a los componentes de entrada y retorne una salida congruente con los componentes de salida.\n",
    "\n",
    "Veamos esto en código!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41002af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(fn = make_prediction, # noten como estamos usando la función que generamos anteriormente\n",
    "                    inputs = [\"number\", \"number\", \"number\", \"number\"], # valores de entrada\n",
    "                    outputs = [\"text\"]) # valor de salida\n",
    "\n",
    "demo.launch(share = True) # share = True: nos permite compartir el demo con quien queramos!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d402339c",
   "metadata": {},
   "source": [
    "Genial! Ahora que tenemos una primera versión, veamos como podemos mejorarla un poco en términos estéticos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01844a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Interface(\n",
    "    fn = make_prediction,\n",
    "    inputs = [ # definimos el intervalo y asignamos un nombre a cada input\n",
    "        gr.Slider(label = 'Sepal Length', minimum = 0, maximum = 10),\n",
    "        gr.Slider(label = 'Sepal Width', minimum = 0, maximum = 10), \n",
    "        gr.Slider(label = 'Petal Length', minimum = 0, maximum = 10), \n",
    "        gr.Slider(label = 'Petal Width', minimum = 0, maximum = 10)],\n",
    "    outputs = gr.Text(label = 'Predicted Label'), # se define un nombre para la salida\n",
    "    title = 'Iris ML Demo', # asignar un titulo al demo\n",
    "    examples=[[0.5, 1.5, 2.5, 3.5], [1, 3, 5, 7]], # generar ejemplos\n",
    "    )\n",
    "\n",
    "demo.launch(share = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3349c13",
   "metadata": {},
   "source": [
    "Genial! Revisemos ahora como customizar aun más el *display* de nuestra aplicación mediante [gr.Blocks](https://www.gradio.app/docs/gradio/blocks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(theme = gr.themes.Base()) as demo:\n",
    "\n",
    "    # agregamos un markdown para describir la aplicacion\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Iris ML Demo\n",
    "    Bienvenid@ a Iris ML demo! Esta herramienta esta diseñada para predecir la clase de una flor a partir de sus características usando Machine Learning.\n",
    "    ## Cómo usar este demo?\n",
    "    Usar esta herramienta es fácil! Sólo debes seguir los siguientes pasos:\n",
    "    1. Fijar los valores de **Sepal Length**, **Sepal Width**, **Petal Length** y **Petal Width**.\n",
    "    2. Observar el tipo de flor que predice el modelo.\n",
    "    \n",
    "    Eso es todo! Estás list@ para explorar y predecir diferentes tipos de flores. Que lo disfrutes!\n",
    "    \"\"\")\n",
    "\n",
    "    # definimos explicitamente la posicion de los elementos\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            sepal_length_slider = gr.Slider(label = 'Sepal Length', minimum = 0, maximum = 10, value = 3.8)\n",
    "            sepal_width_slider = gr.Slider(label = 'Sepal Width', minimum = 0, maximum = 10, value = 6.4)\n",
    "            petal_length_slider = gr.Slider(label = 'Petal Length', minimum = 0, maximum = 10, value = 7.3)\n",
    "            petal_width_slider = gr.Slider(label = 'Petal Width', minimum = 0, maximum = 10, value = 4.9)\n",
    "\n",
    "        with gr.Column():\n",
    "            label = gr.Text(label = 'Predicted Label') # se define un nombre para la salida\n",
    "    \n",
    "    with gr.Row():\n",
    "        button = gr.Button(value = 'Predict!')\n",
    "\n",
    "    # setear interactividad\n",
    "    inputs = [sepal_length_slider, sepal_width_slider, petal_length_slider, petal_width_slider]\n",
    "    outputs = [label]\n",
    "    button.click(fn = make_prediction, inputs = inputs, outputs = outputs)\n",
    "\n",
    "    examples = [\n",
    "        [0.5, 1.5, 2.5, 3.5], # example 1\n",
    "        [1, 3, 5, 7], # example 2\n",
    "    ]\n",
    "    gr.Examples(examples = examples, inputs = inputs) \n",
    "\n",
    "    demo.launch(share = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d467034",
   "metadata": {},
   "source": [
    "Pueden encontrar la totalidad del código en `recursos/2024-02/frontend.py`.\n",
    "\n",
    "**Bonus:** [Tutorial para Desplegar su aplicación de gradio en un servidor de HuggingFace](https://www.youtube.com/watch?v=97KxA1r184o)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5bc7e",
   "metadata": {},
   "source": [
    "### Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e26aa0",
   "metadata": {},
   "source": [
    "#### ¿Qué es el Back End?\n",
    "\n",
    "<center>\n",
    "<img src='https://media1.tenor.com/m/NoxXhCo1EU4AAAAC/figura-backend.gif' width=125  />\n",
    "</center>\n",
    "\n",
    "En el contexto de aplicaciones web, **Back End se refiere a la parte de la aplicación que opera detrás de escena y gestiona la lógica del servidor, los datos y la integración con otras aplicaciones o servicios**. Es el núcleo que se encarga de procesar solicitudes desde el Front End, acceder a bases de datos, realizar cálculos, y devolver respuestas para que el cliente las muestre de manera amigable.\n",
    "\n",
    "Algunos ejemplos de procesamientos a realizar en el Back End contemplan:\n",
    "\n",
    "- **Modelos de ML**: Inferencia de nuevas observaciones usando un modelo de ML.\n",
    "- **Gestión de Datos:** Operaciones CRUD (crear, leer, actualizar y eliminar datos) en bases de datos.\n",
    "- **Autenticación y Seguridad:** Verificación de usuarios, gestión de permisos y protección de datos.\n",
    "- **Integraciones:** Comunicación con servicios externos (por ejemplo, APIs de terceros).\n",
    "\n",
    "**Lenguajes y Frameworks Comunes:**\n",
    "- Python: `Django, Flask, FastAPI`\n",
    "- JavaScript/TypeScript: `Node.js`\n",
    "- Java: `Spring Boot`\n",
    "- Ruby: `Ruby on Rails`\n",
    "- PHP: `Laravel`\n",
    "\n",
    "En esta clase nos enfocaremos en usar un backend sencillo utilizando `FastAPI` en Python, que nos permite crear `API REST` de manera rápida y eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc045e3e",
   "metadata": {},
   "source": [
    "### ¿Qué es una API?\n",
    "\n",
    "Una **API (Application Programming Interface)** es un conjunto de reglas y herramientas que permiten a diferentes sistemas, aplicaciones o servicios comunicarse entre sí. Es como un \"contrato\" que define cómo se deben realizar las interacciones entre ellos, facilitando el intercambio de datos y funcionalidades. De esta manera, las APIs son el **componente clave que permiten que el Front End y el Back End se trabajen de manera conjunta.**\n",
    "\n",
    "Características Clave de las APIs:\n",
    "- **Interoperabilidad:** Permiten que diferentes tecnologías y plataformas trabajen juntas.\n",
    "- **Estandarización:** Usan protocolos y formatos bien definidos, como HTTP, JSON o XML.\n",
    "- **Reusabilidad:** Un solo API puede ser utilizado por múltiples aplicaciones o servicios.\n",
    "\n",
    "### ¿Qué es una API REST?\n",
    "\n",
    "Una **API REST (Representational State Transfer)** es un tipo específico de API que sigue los principios de arquitectura REST. Este enfoque se basa en el uso de HTTP para manejar las operaciones sobre los datos y los recursos que maneja la aplicación. REST es ampliamente utilizado debido a su simplicidad y capacidad para integrarse fácilmente con aplicaciones web.\n",
    "\n",
    "Una API REST considera los siguientes principios:\n",
    "1. **Arquitectura Cliente-Servidor:** El Front End (cliente) y el Backend (servidor) están separados y se comunican mediante peticiones HTTP.\n",
    "2. **Sin Estado:** Cada solicitud enviada desde el cliente al servidor debe contener toda la información necesaria para procesarla, sin depender de un estado previo.\n",
    "3. **Interfaz Uniforme:** Los recursos se identifican mediante URLs claras y consistentes.\n",
    "4. **Operaciones HTTP Estándar:**\n",
    "     - **GET**: Solicitud para pedir datos.\n",
    "     - **POST**: Enviar datos (en el cuerpo de la solicitud) comunmente para ser procesados y guardados.\n",
    "     - **DELETE**: Elimina un dato.\n",
    "     - **PUT**: Actualiza un dato.\n",
    "5. **Uso de Representaciones:** Los datos se transfieren usando formatos como JSON o XML.\n",
    "\n",
    "Por otro lado, es útil tener en cuenta los [códigos de respuesta](https://http.cat/) al interactuar con nuestra API:\n",
    "  - `1xx` - Respuestas informativas\n",
    "  - `2xx` - Respuestas satisfactorias \n",
    "  - `3xx` - Redirecciones \n",
    "  - `4xx` - Errores de los clientes \n",
    "  - `5xx` - Errores de los servidores \n",
    "\n",
    "De esta manera, el objetivo de esta sección es **utilizar FastAPI para generar una API REST que permita entregar las inferencias de nuestro modelo de ML al cliente.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa206d7",
   "metadata": {},
   "source": [
    "#### FastAPI\n",
    "\n",
    "Ya que conocemos lo que es una API REST, revisemos como podemos usar `FastAPI` para implementar nuestra propia API.\n",
    "\n",
    "[FastAPI](https://fastapi.tiangolo.com) es un framework moderno y de alto rendimiento para construir APIs con Python, que destaca por su simplicidad, rapidez y eficiencia. FastAPI permite desarrollar aplicaciones web rápidamente utilizando la tipificación de Python para generar documentación automática y validaciones de datos sin esfuerzo adicional. Con una curva de aprendizaje amigable y un rendimiento comparable a frameworks como Node.js y Go, FastAPI es una excelente elección para aprender y dominar el desarrollo de APIs en Python.\n",
    "\n",
    "Comencemos instalando la libreria:\n",
    "\n",
    "<img src='../../recursos/2024-01/despliegue/images/fastapi.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install \"fastapi[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a50b3f",
   "metadata": {},
   "source": [
    "Con la libreria instalada, pasemos ahora a escribir nuestra API!\n",
    "\n",
    "**Nota importante: A diferencia de la sección de `gradio`, será necesario ejecutar nuestro código usando la terminal. Encontrarán el código completo de la API en `recursos/2024-02/backend.py` (recuerden moverse hacia recursos/2024-02/ para ejecutarlo)**\n",
    "\n",
    "Para usar `fastapi`, lo primero que se debe realizar es generar una instancia del módulo FastAPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "# crear aplicación\n",
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3c321",
   "metadata": {},
   "source": [
    "Para levantar nuestra aplicación, tenemos 2 opciones:\n",
    "\n",
    "1. Escribir en la terminal el siguiente comando (opción recomendada):\n",
    "\n",
    "```python\n",
    "uvicorn nombre_script:app\n",
    "```\n",
    "\n",
    "2. Adjuntar al final del código de la aplicación el siguiente *chunk*:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run('nombre_script:app')\n",
    "```\n",
    "\n",
    "El siguiente paso es crear un *home* o vista default de nuestra aplicación. Usualmente, esta vista está hecha para introducir al usuario al funcionamiento de la aplicación.\n",
    "\n",
    "Veamos como podemos implementar esto en nuestra aplicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get('/') # ruta\n",
    "async def home(): \n",
    "    return {'Hello': 'World'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443cbd21",
   "metadata": {},
   "source": [
    "Noten como FastAPI hace uso de *decoradores* para definir las rutas de la aplicación (donde en este caso, estamos definiendo un `GET`).\n",
    "\n",
    "Luego si levantamos nuestra aplicación e ingresamos a [http://127.0.0.1:8000](http://127.0.0.1:8000), deberiamos obtener como respuesta:\n",
    "\n",
    "```{python}\n",
    "{'Hello': 'World'}\n",
    "```\n",
    "\n",
    "> **Pregunta:** ¿Qué es un json? ¿Porqué es deseable hacer uso de este tipo en nuestra API?\n",
    "\n",
    "Veamos ahora como definir una segunda ruta de acceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e540c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from make_prediction import make_prediction\n",
    "\n",
    "# init app\n",
    "app = FastAPI()\n",
    "\n",
    "# def home\n",
    "@app.get('/') # ruta\n",
    "async def home():\n",
    "    return {'Hello': 'World'}\n",
    "\n",
    "@app.get('/classroom') # ruta\n",
    "async def classroom():\n",
    "    return {'Message': 'This is my first API :)'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932f900",
   "metadata": {},
   "source": [
    "Donde si ingresan a [http://127.0.0.1:8000/classroom](http://127.0.0.1:8000/classroom) deberian observar:\n",
    "\n",
    "```python\n",
    "{'Message': 'This is my first API :)'}\n",
    "```\n",
    "\n",
    "Genial! Ahora que ya sabemos como definir algunas rutas de acceso en nuestra aplicación, veamos como implementar un método `POST` para desplegar nuestro modelo.\n",
    "\n",
    "> **Pregunta:** ¿Porqué es deseable generar un método `POST` para el despliegue de nuestra solución?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "# init app\n",
    "app = FastAPI()\n",
    "\n",
    "# def home\n",
    "@app.get('/') # ruta\n",
    "async def home():\n",
    "    return {'Hello': 'World'}\n",
    "\n",
    "@app.get('/classroom') # ruta\n",
    "def classroom():\n",
    "    return {'Message': 'This is my first API :)'}\n",
    "\n",
    "# def predict method\n",
    "@app.post(\"/predict\") # ruta\n",
    "async def predict(sepal_length: float, sepal_width: float, petal_length: float, petal_width: float): # parametros de entrada\n",
    "\n",
    "    label = make_prediction(sepal_length, sepal_width, petal_length, petal_width) # generar prediccion\n",
    "\n",
    "    return {\"label\": label} # retornar prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52829ef6",
   "metadata": {},
   "source": [
    "Como pueden ver, desplegar nuestra solución fue sumamente simple ya que pudimos reciclar gran parte de lo hecho en `make_prediction`. Probemos ahora nuestra API!\n",
    "\n",
    "> **Pregunta:** ¿Qué deberia pasar si ingresamos a [http://127.0.0.1:8000/predict](http://127.0.0.1:8000/predict)? ¿Porqué?\n",
    "\n",
    "Para probar que el método `POST` funciona, `FastAPI` nos habilita la ruta [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) por defecto en donde podemos probar lo que hemos desarrollado en la API.\n",
    "\n",
    "Probemos en vivo nuestra API REST!\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/ug1DBRF_MjIAAAAC/bill-oreilly-well-do-it-live.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9589b251",
   "metadata": {},
   "source": [
    "### Juntando todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8382be62",
   "metadata": {},
   "source": [
    "Ahora que ya conocemos como implementar tanto el Back End como el Front End, haremos que ambas partes trabajen de manera **simultánea**. Para lograr esto, seguiremos el siguiente flujo:\n",
    "\n",
    "1. Usuario solicita una predicción de nuestro modelo a través del Front\n",
    "2. Front recibe solicitud y la envia al Back\n",
    "3. Back procesa y devuelve la predicción solicitada al Front\n",
    "4. Front envia la predicción al Usuario\n",
    "\n",
    "Considerando el esquema señalado, necesitamos entonces una **vía de comunicación entre el Front y el Back** para procesar la solicitud del usuario. \n",
    "\n",
    "Si recordamos que el Back es una API REST, podemos simplemente enviar un *request* al Back y así retornar la respuesta! \n",
    "\n",
    "Esto lo podemos hacer a través de la librería `requests`, veamos un ejemplo:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/of1uLME1mx8AAAAd/put-them-together-build.gif\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9226bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# payload\n",
    "data = {\n",
    "    \"sepal_length\": 3.5,\n",
    "    \"sepal_width\": 2.0,\n",
    "    \"petal_length\": 1.6,\n",
    "    \"petal_width\": 5.4\n",
    "}\n",
    "\n",
    "url = \"http://127.0.0.1:8000/predict\" # el back debe estar ejecutándose\n",
    "response = requests.post(url, json = data) # invocamos un POST enviando la data en el payload\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee30b8",
   "metadata": {},
   "source": [
    "Luego, podemos modificar nuestro script de `Gradio` para realizar requests al back. \n",
    "\n",
    "Primero definimos una función de predicción usando `requests`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# primero definimos una función para obtener respuestas del back a través de requests\n",
    "def get_backend_prediction(\n",
    "        sepal_length: float, \n",
    "        sepal_width: float, \n",
    "        petal_length: float, \n",
    "        petal_width: float,\n",
    "        ):\n",
    "\n",
    "    # payload\n",
    "    data = {\n",
    "        \"sepal_length\": sepal_length,\n",
    "        \"sepal_width\": sepal_width,\n",
    "        \"petal_length\": petal_length,\n",
    "        \"petal_width\": petal_width,\n",
    "    }\n",
    "\n",
    "    url = \"http://127.0.0.1:8000/predict\" # url del back end\n",
    "    response = requests.post(url, json = data) # código de respuesta\n",
    "    label = response.json()[\"label\"] # obtener contenido de la respuesta\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a3ded",
   "metadata": {},
   "source": [
    "Finalmente levantamos la misma aplicación de `gradio` que habíamos escrito antes, aunque ahora usando la función `get_backend_prediction`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91be55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# la única linea que cambia es la función a invocar\n",
    "with gr.Blocks(theme = gr.themes.Base()) as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Iris ML Demo\n",
    "    Bienvenid@ a Iris ML demo! Esta herramienta esta diseñada para predecir la clase de una flor a partir de sus características usando Machine Learning.\n",
    "    ## Cómo usar este demo?\n",
    "    Usar esta herramienta es fácil! Sólo debes seguir los siguientes pasos:\n",
    "    1. Fijar los valores de **Sepal Length**, **Sepal Width**, **Petal Length** y **Petal Width**.\n",
    "    2. Observar el tipo de flor que predice el modelo.\n",
    "    \n",
    "    Eso es todo! Estás list@ para explorar y predecir diferentes tipos de flores. Que lo disfrutes!\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            sepal_length_slider = gr.Slider(label = 'Sepal Length', minimum = 0, maximum = 10, value = 3.8)\n",
    "            sepal_width_slider = gr.Slider(label = 'Sepal Width', minimum = 0, maximum = 10, value = 6.4)\n",
    "            petal_length_slider = gr.Slider(label = 'Petal Length', minimum = 0, maximum = 10, value = 7.3)\n",
    "            petal_width_slider = gr.Slider(label = 'Petal Width', minimum = 0, maximum = 10, value = 4.9)\n",
    "\n",
    "        with gr.Column():\n",
    "            label = gr.Text(label = 'Predicted Label') # se define un nombre para la salida\n",
    "    \n",
    "    with gr.Row():\n",
    "        button = gr.Button(value = 'Predict!')\n",
    "\n",
    "    # setear interactividad\n",
    "    inputs = [sepal_length_slider, sepal_width_slider, petal_length_slider, petal_width_slider]\n",
    "    outputs = [label]\n",
    "    button.click(fn = get_backend_prediction, inputs = inputs, outputs = outputs) # esta linea invoca el back end\n",
    "\n",
    "    examples = [\n",
    "        [0.5, 1.5, 2.5, 3.5], # example 1\n",
    "        [1, 3, 5, 7], # example 2\n",
    "    ]\n",
    "    gr.Examples(examples = examples, inputs = inputs) \n",
    "\n",
    "    demo.launch(share = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f63d6",
   "metadata": {},
   "source": [
    "Finalmente, deberían ser capaces de ejecutar ambos script (una terminal para el back, otra terminal para el front) y montar su aplicación fuera de jupyter notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
