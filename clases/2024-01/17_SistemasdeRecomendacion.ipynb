{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_3zKUVeiUZL"
      },
      "source": [
        "# Clase 17: Sistemas de Recomendación\n",
        "\n",
        "**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-BmzhVdKDTs"
      },
      "source": [
        "## Objetivos de la clase\n",
        "\n",
        "- Introducir al estudiante a los sistemas de recomendación\n",
        "- Diferenciar entre los principales tipos de sistemas de recomendación, sus ventajas y desventajas\n",
        "- Aprender a implementar sistemas de recomendación basados en contenido usando técnicas de NLP\n",
        "- Aprender a implementar sistemas de recomendación basados en filtros colaborativos usando `surprise`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otL4t8MY4GNI"
      },
      "source": [
        "## Dataset a ocupar\n",
        "\n",
        "Para esta clase ocuparemos datos basados en el dataset de [TMDB 5000 Movie Dataset](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv), el cual contiene calificaciones de películas realizadas por usuarios. De esta forma, la idea de esta clase es que podamos usar este dataset para entrenar diferentes sistemas de recomendación y así entender el funcionamiento de estos modelos.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://storage.googleapis.com/kaggle-datasets-images/138/287/229bfb5d3dd1a49cc5ac899c45ca2213/dataset-cover.png\" style=\"width: 50%;\">\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "__Rat3nba2Sh",
        "outputId": "83a7d18d-f657-43ae-b72e-f30c73f21c55"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet('../../recursos/2024-01/recommendations/movies.parquet')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2cha98_LNy-"
      },
      "source": [
        "## ¿Qué son los Sistemas de Recomendación?\n",
        "\n",
        "Un **sistema de recomendación** es un algoritmo de machine learning, típicamente ligado a servicios digitales, que utiliza grandes volúmenes de datos para sugerir o recomendar productos adicionales a los consumidores. Estas recomendaciones se basan en una variedad de criterios, como compras previas, historial de búsquedas, información demográfica, entre otros. Los sistemas de recomendación son extremadamente útiles pues **permiten a los usuarios descubrir productos y servicios que quizás no habrían encontrado por sí mismos**, ayudando a filtrar y priorizar las alternativas más adecuadas a sus gustos y necesidades.\n",
        "\n",
        "Los sistemas de recomendación son entrenados para discernir las preferencias, decisiones y características tanto de personas como de productos a partir de **interacciones usuario-producto** (\"user-item\" en inglés). Entre estas interacciones podemos encontrar clicks sobre el producto, *likes* y hasta la cantidad de tiempo que pasa un usuario sobre el catálogo del producto en particular.\n",
        "\n",
        "> **Pregunta**: ¿Han visto este tipo de anuncios antes? ¿Dónde?\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MLr8crqbUC9MazNIGOZOzw.png\" style=\"width: 50%;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyEX-rmLLkXy"
      },
      "source": [
        "## Objetivo del Sistema de Recomendación\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://i.giphy.com/guufsF0Az3Lpu.webp\" style=\"width: 30%;\">\n",
        "</div>\n",
        "\n",
        "\n",
        "El **objetivo principal** de un sistema de recomendación es **incrementar las ventas de productos**. Después de todo, los sistemas de recomendación son empleados por comerciantes con el fin de elevar sus beneficios económicos. Mediante la recomendación de artículos meticulosamente seleccionados a los usuarios, los sistemas de recomendación destacan productos relevantes para estos últimos. Esto, a su vez, aumenta el volumen de ventas y las ganancias para el comerciante.\n",
        "\n",
        "> **Pregunta:** Es lo mismo hablar sobre \"permitir a los usuarios descubrir nuevos productos\" e \"incrementar ventas de productos\"? Por qué es importante hablar sobre esto?\n",
        "\n",
        "Para cumplir con este objetivo, el problema de recomendación puede ser formulado de dos maneras:\n",
        "\n",
        "- **Problema de regresión:** Este enfoque se centra en **predecir el valor de calificación para una combinación específica de usuario y artículo**. Partimos de la base de que existen datos de entrenamiento que reflejan las preferencias de los usuarios por ciertos artículos. Con m usuarios y n artículos, esto se traduce en una matriz m × n incompleta, donde los valores ya conocidos se emplean para el entrenamiento. Los valores faltantes se predicen utilizando este modelo de entrenamiento. Este problema es también conocido como el problema de *matrix completion*, en el cual se especifica una matriz de manera incompleta y los valores restantes son inferidos por el algoritmo de aprendizaje.\n",
        "\n",
        "- **Problema de clasificación:** En contextos prácticos, no siempre es necesario predecir las calificaciones de los usuarios para artículos específicos a fin de hacer recomendaciones. En cambio, podría ser más útil para un comerciante **recomendar los artículos top-k para un usuario específico**, o identificar los usuarios top-k a los que dirigir un artículo particular. La identificación de artículos top-k es más habitual que la de usuarios top-k, aunque los métodos utilizados en ambos casos son esencialmente los mismos. Este problema también se denomina problema de recomendación top-k y constituye la formulación de clasificación del problema de recomendaciones.\n",
        "\n",
        "En el segundo caso, los valores absolutos de las calificaciones predichas no son relevantes. La primera formulación es más general, ya que las soluciones al segundo caso pueden obtenerse resolviendo primero la formulación para diversas combinaciones de usuario y artículo, para luego clasificar las predicciones. No obstante, en muchas situaciones resulta más sencillo y práctico diseñar métodos que aborden directamente la versión de clasificación del problema.\n",
        "\n",
        "> **Pregunta**: Cual de las dos formulaciones podría ser más atingente para las necesidades de una empresa? Porqué?\n",
        "\n",
        "## ¿Qué propiedades debe cumplir una buena recomendación?\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://i.giphy.com/ApdE3t6VqYQz8yAIOa.webp\" width=\"450\">\n",
        "</div>\n",
        "\n",
        "\n",
        "En orden para cumplir con el objetivo de incrementar el *engagement* del usuario con los productos, las recomendaciones generadas deben cumplir con las siguientes características:\n",
        "\n",
        "- **Relevancia:** El objetivo operacional más evidente de un sistema de recomendación es **sugerir artículos que resulten pertinentes para el usuario**. Los usuarios son más propensos a consumir artículos que les resultan interesantes. Recomendar productos relevantes constituye el objetivo principal de un sistema de recomendación, aunque no es suficiente por sí misma para generar un sistema exitoso.\n",
        "\n",
        "- **Novedad:** Los sistemas de recomendación son exitosos cuando el producto recomendado es algo que el usuario **no ha visto en el pasado**. Por ejemplo, películas populares del género favorito por el usuario rara vez representan una novedad. Además, la recomendación reiterada de artículos populares puede resultar en una disminución de la diversidad de ventas.\n",
        "\n",
        "- **Serendipia:** También llamado *casualidad*, se refiere a que las recomendaciones son algo **inesperadas** y, por ende, conllevan un elemento de descubrimiento fortuito, a diferencia de las recomendaciones más previsibles. La serendipia se distingue de la novedad porque implica recomendaciones que resultan verdaderamente sorprendentes para el usuario, más allá de ser meramente algo desconocido para ellos anteriormente. Es común que algunos usuarios consuman exclusivamente artículos de un tipo específico, aunque puedan tener **intereses latentes en otros tipos de artículos**, cuyo descubrimiento resultaría sorprendente para ellos. A diferencia de la novedad, los métodos serendípicos se enfocan en **descubrir estas recomendaciones inesperadas**.\n",
        "\n",
        "- **Diversidad:** Los sistemas de recomendación tipicamente sugieren una lista de top-k items. Cuando todas las recomendaciones son similares, incrementa el riesgo que el usuario pueda no elegir ninguna recomendación. Por otra mano, si la **lista de recomendaciones posee productos de diferentes tipos**, existe una **mayor probabilidad de que el usuario pueda gustarle al menos uno de esos productos**. La diversidad en las recomendaciones tiene el beneficio de asegurar que el usuario no se aburra por recomendaciones de productos similares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZrAY7VGLqB2"
      },
      "source": [
        "## Tipos de Sistemas de Recomendación\n",
        "\n",
        "Si bien existe un gran número de tipos de sistemas de recomendación, en esta clase nos centraremos en dos categorías principales: **Métodos Basados en Contenido** y **Filtros Colaborativos**. Conozcamos en detalle cada uno de estos:\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L3RyKloSo-mTAezwlarVcg.png\" width=\"500\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNpTbCsKm9FF"
      },
      "source": [
        "### Métodos Basados en Contenido\n",
        "\n",
        "En contraste con los filtros colaborativos, los métodos basados en contenido aprovechan información adicional sobre los usuarios y los artículos. Por ejemplo, en un sistema de recomendación de películas, esta información podría incluir la edad, el sexo, la profesión y otros datos personales de los usuarios, así como la categoría, los actores principales, la duración y otras características relevantes de las películas.\n",
        "\n",
        "La esencia de los métodos basados en contenido radica en **desarrollar un modelo que utilice las características disponibles** para explicar las interacciones observadas entre usuarios y artículos. Tomando en cuenta el ejemplo de las películas, se podría intentar modelar cómo, por ejemplo, las mujeres jóvenes tienden a valorar mejor ciertas películas, mientras que los hombres jóvenes prefieren otras. Si conseguimos establecer un modelo efectivo, realizar nuevas predicciones para un usuario resulta sencillo: basta con examinar el perfil del usuario (edad, sexo, etc.) y, con base en esa información, identificar las películas que podrían ser de su interés.\n",
        "\n",
        "La principal ventaja de este tipo de métodos es con respecto al problema de **Coldstart**, es decir, predecir para nuevos usuarios y/o productos. Por ejemplo, si un nuevo usuario es registrado junto a sus datos demográficos (sexo, edad, residencia, etc.) es posible generar un set de recomendaciones sólo en función de estas características. Sólo los nuevos usuarios o artículos con características completamente nuevas tendrán problemas en la predicción (ejemplo: sistema que no se entrenó con personas de la tercera edad), aunque a medida que el sistema madura, la probabilidad de que esto ocurra es cada vez menor.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ReuY4yOoqKMatHNJupcM5A@2x.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVuPpC79nJh1"
      },
      "source": [
        "### Implementación\n",
        "\n",
        "En esta sección implementaremos un sistema de recomendación basado en contenido de producto usando el resumen de la película (del tipo `string`) para generar recomendaciones.\n",
        "\n",
        "**Recuerden que un sistema de recomendación basado en contenido puede construirse usando features de usuarios y/o productos.** Si bien en esta sección nos basaremos únicamente en características de la película, perfectamente podríamos entrenar un sistema de recomendación usando características del usuario (edad, sexo, etc).\n",
        "\n",
        "Recordemos el formato de los datos disponibles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "dDIaxPVspeC8",
        "outputId": "4e91e502-b9ce-4a85-ecc4-261d486f46c5"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZa14sqKodER"
      },
      "source": [
        "Noten como los datos contienen múltiples calificaciones para cada película. Recordando que este tipo de modelo **no necesita de las interacciones usuario-producto**, es posible entonces reducir la dimensionalidad de nuestro dataset a 1 fila por películas agrupando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Q4kuBzWinRyd",
        "outputId": "35d7f139-224a-4a51-b8a6-4b1ac6bb0007"
      },
      "outputs": [],
      "source": [
        "# agrupamos datos por producto\n",
        "df_bow = df.groupby('movieId')[['title', 'overview']].first().reset_index() # solo nos quedamos con atributos de productos\n",
        "df_bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmPe2XaSnWlF"
      },
      "source": [
        "Habiendo preparado nuestro dataset, podemos ahora procesar nuestro dataset para generar recomendaciones basadas en contenido de producto. Noten que para este caso, tenemos 2 atributos del tipo string por cada película: `title` y `overview`. ¿Cómo podemos procesar este tipo de feature?\n",
        "\n",
        "#### Bag of Words\n",
        "\n",
        "`Bag of Words` es un modelo de conteo utilizado en Procesamiento de Lenguaje Natural (NLP) que tiene como objetivo **generar una representación vectorial** (vector de características en nuestro cas) para cada documento a través del conteo de las palabras que contienen.\n",
        "\n",
        "La siguiente figura muestra un ejemplo de `Bag of Words` en acción:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://user.oc-static.com/upload/2020/10/23/16034397439042_surfin%20bird%20bow.png\" width=\"500\">\n",
        "</p>\n",
        "\n",
        "> **Pregunta:** ¿Conocen otros métodos para vectorizar texto? ¿Cuáles?\n",
        "\n",
        "Podemos implementar fácilmente este método a través de `sklearn`:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit_transform(text)\n",
        "```\n",
        "\n",
        "Probemos ahora a procesar la columna `overview` usando `Bag of Words`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "3RZm_9WInh0N",
        "outputId": "b1964880-6e10-43d6-cc4e-a71c841f8f9f"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer() # instanciamos BoW\n",
        "bow = vectorizer.fit_transform(df_bow['overview']).toarray() # Vectorizamos texto\n",
        "bow = pd.DataFrame(bow, columns=vectorizer.get_feature_names_out()) # Transformamos a Dataframe\n",
        "bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKbc9-F2nmoI"
      },
      "source": [
        "Noten como a partir de una columna de texto, se generan 17435 columnas (1 por cada palabra distinta contenida en `overview`).\n",
        "\n",
        "Un problema típico de esta metodología es el incremento explosivo de la dimensionalidad de los datos. Para atacar esto, un método efectivo es **limpiar el texto de entrada** aplicando técnicas de procesamiento de texto. En específico, implementaremos:\n",
        "- Transformación de palabras a minúsculas\n",
        "- Filtrar a todas las palabras que aparezcan en 10 documentos como mínimo\n",
        "- Eliminación de *stopwords* en inglés\n",
        "\n",
        "Nuevamente, podemos implementar lo anterior de manera sencilla usando `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "nZG0mmbEnqhK",
        "outputId": "d9923cdd-4122-4bf3-e0de-0d988d10784a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase = True, min_df = 10, stop_words = 'english') # instanciamos BoW con data cleaning\n",
        "bow = vectorizer.fit_transform(df_bow['overview']).toarray() # Vectorizamos texto\n",
        "bow = pd.DataFrame(bow, columns=vectorizer.get_feature_names_out()) # Transformamos a Dataframe\n",
        "bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kay9Y0Q8nu4y"
      },
      "source": [
        "Observen como el número de features se redujo desde 17435 a 1472 (sólo 10% de lo que era originalmente!)\n",
        "\n",
        "> **Pregunta:** ¿Como sería el resultado si tengo más columnas de texto? ¿Y si además tengo columnas numéricas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCrPquLrsKs9"
      },
      "source": [
        "#### Buscando películas con mayor similitud\n",
        "\n",
        "Con los datos vectorizados, podemos ahora encontrar las películas que guardan una mayor relación entre sí a través de la **similitud coseno**. Este método calcula la similitud entre dos vectores y es calculada de la siguiente forma:\n",
        "\n",
        "$$\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}$$\n",
        "\n",
        "donde $\\cos(\\theta)$ varía entre los valores (-1, 1) y (a,b) representa el par de vectores a calcular la similitud. Una similitud coseno igual a 1 indica que ambos vectores son idénticos, mientras que una similitud igual a -1 indica que los vectores son opuestos.\n",
        "\n",
        "Si consideramos que $a$ y $b$ son los vectores generados anteriormente por BoW, podemos interpretar el resultado de esta operación como el **grado de similaridad de palabras clave entre dos descripciones de película (`overview`)**.\n",
        "\n",
        "**Pregunta:** ¿Conocen otras formas para medir la similitud entre dos vectores?\n",
        "\n",
        "Algunas medidas de similitud interesantes: [link](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise)\n",
        "\n",
        "Veamos ahora como implementar la similitud coseno usando `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncb8qU1tn2dm",
        "outputId": "3341b4a8-16b3-4de2-91b3-90e213970b4b"
      },
      "outputs": [],
      "source": [
        "# importamos cosine_similarity de sklearn\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# calculamos la similitud coseno de cada vector generado contra el resto\n",
        "cosine_sim = cosine_similarity(bow, bow)\n",
        "cosine_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOcJU1d2xKIj",
        "outputId": "ed883059-bb3b-479f-ce58-c86fb260b5c1"
      },
      "outputs": [],
      "source": [
        "cosine_sim.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWBNkD9lwRQ7"
      },
      "source": [
        "Noten como el resultado es una matriz de N x N, donde cada valor de la matriz puede ser interpretado como la similitud coseno entre dos descripciones de películas. Esta matriz comparte las mismas propiedades que la matriz de correlación: es **simétrica** y la **diagonal representa la similitud coseno de la película consigo misma** (similitud = 1).\n",
        "\n",
        "Con la similitud calculada, generemos 5 recomendaciones de película para un usuario que tiene preferencias a películas como *Rocky*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akIzSO4Cn569",
        "outputId": "971a42a7-9b11-4b24-aabf-7b8d0fb4787e"
      },
      "outputs": [],
      "source": [
        "def get_recommendations(df, movie, k = 5):\n",
        "\n",
        "  \"\"\"\n",
        "  Returns the top k similar movies using BoW\n",
        "  \"\"\"\n",
        "\n",
        "  # Assert movie is contained in dataset\n",
        "  assert movie in df['title'].values, 'movie is not contained in train dataset!'\n",
        "\n",
        "  # Get idx of movie\n",
        "  idx = df[df['title'] == movie].index[0]\n",
        "\n",
        "  # Get pairwise similarities of all movies with specified movie\n",
        "  sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "\n",
        "  # Sort the movies based on the similarity scores\n",
        "  sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  # Get the scores of the k most similar movies\n",
        "  sim_scores = sim_scores[1:k+1]\n",
        "\n",
        "  # Get the movie indices\n",
        "  movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "  # Return the top k most similar movies\n",
        "  return df['title'].iloc[movie_indices].values\n",
        "\n",
        "get_recommendations(df = df_bow, movie = 'Rocky')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia4RP51V5kfl"
      },
      "source": [
        "Veamos ahora como generar recomendaciones para un usuario en específico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aCmt0pfn8FX",
        "outputId": "b1447e7c-d468-47e9-ce73-805d4f8df15a"
      },
      "outputs": [],
      "source": [
        "# primero rescatamos la película más vista por el usuario\n",
        "userId = 33 # usuario a recomendar\n",
        "user_movies = df[df['userId'] == userId].sort_values('rating', ascending = False) # películas rateadas por el usuario\n",
        "best_movie = user_movies.iloc[0]['title'] # película con mejor rating\n",
        "print(f'most liked movie by user {userId}: {best_movie}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrV3kixIn-bo",
        "outputId": "67657de3-01d4-4ed4-ced3-9259a899b9d6"
      },
      "outputs": [],
      "source": [
        "# finalmente, printeamos las k mejores recomendaciones para el usuario\n",
        "get_recommendations(df = df_bow, movie = best_movie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy5MXP0R__yk"
      },
      "source": [
        "> **Pregunta:** Habíamos dicho que el entrenamiento de este tipo de modelos no necesitaba interacciones de usuario-producto. Si es así, ¿porqué necesitamos saber la película mas vista por el usuario? ¿cómo se vería esto reflejado en la realidad?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l0M8z33693E"
      },
      "source": [
        "## Filtros Colaborativos\n",
        "\n",
        "Los métodos colaborativos para sistemas de recomendación **se basan únicamente en las interacciones pasadas registradas entre usuarios y artículos** para producir nuevas recomendaciones. Estas interacciones se almacenan en la **matriz de interacciones usuario-producto**, la cual sigue la siguiente forma:\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*swlCZkfOdnxKJnQ1xHjIkw.png\" style=\"width: 30%;\">\n",
        "</div>\n",
        "\n",
        "La **principal ventaja** de los enfoques colaborativos es que **no requieren información sobre los usuarios o los artículos**, por lo que pueden utilizarse en muchas situaciones. Además, **cuanto más interactúan los usuarios con los artículos, más precisas se vuelven las nuevas recomendaciones**\n",
        "\n",
        "Sin embargo, dado que solo considera interacciones pasadas para hacer recomendaciones, el filtrado colaborativo sufre del problema de **Coldstart**. Este inconveniente puede abordarse de diferentes maneras:\n",
        "- Recomendando **artículos aleatorios** a nuevos usuarios o nuevos artículos a usuarios aleatorios (estrategia aleatoria)\n",
        "- Recomendando **artículos populares** a nuevos usuarios o nuevos artículos a los usuarios más activos (estrategia de máxima expectativa)\n",
        "- Recomendando un **conjunto de diversos artículos** a nuevos usuarios o un nuevo artículo a un conjunto de diversos usuarios (estrategia exploratoria)\n",
        "- Utilizando un **método no colaborativo durante la primera etapa** de vida del usuario o del artículo.\n",
        "\n",
        "Finalmente, los filtros colaborativos se pueden clasificar en dos categorías:\n",
        "- **Basados en memoria**: Trabajan directamente sobre los valores registrados en la matriz usuario-producto y se basan principalemente en la búsqueda de vecinos cercanos (por ejemplo, usando `KNN`).\n",
        "- **Basados en modelo**: Asumen que existe un proceso generativo que explica las interacciones usuario-producto el cual intentan modelar para generar predicciones.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*m_Z6Da5FZ62KN2yH-x_GOQ@2x.png\" style=\"width: 30%;\">\n",
        "</div>\n",
        "\n",
        "Conociendo lo básico de Filtros Colaborativos, veamos ahora en detalle uno de estos modelos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjFojK8tETIk"
      },
      "source": [
        "### NMF\n",
        "\n",
        "**Non-negative Matrix Factorization (NMF)** es un modelo perteneciente a la familia de filtros colaborativos basados en modelo. Al igual que modelos como Singular Value Decomposition (SVD), la idea de este modelo es generar una predicción para cada valor de la matriz usuario-producto a través de una **descomposición matricial** de la matriz objetivo. Si llamamos $V_{U,I}$ a la matriz de ratings observada, podemos entonces descomponerla en matrices $W_{U, k}$ y $H_{k, I}$ de modo que:\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*NyhKrN2TbYLFTMZgSr0rIQ.png\" style=\"width: 30%;\">\n",
        "</div>\n",
        "\n",
        "donde $U$ representa el número total de usuarios, $I$ se refiere al número de productos, y $k$ representa el número de componentes de ambos embeddings. Los valores de cada matriz pueden ser inicializados de forma aleatoria o siguiendo alguna regla en particular. Sin embargo, este modelo tiene la restricción de que estos valores deben ser **estrictamente positivos**.\n",
        "\n",
        "Habiendo definido ambas matrices, podemos entonces generar una predicción de $V$ usando:\n",
        "\n",
        "$$\\hat V = W \\cdot H$$\n",
        "$$\\hat r_{u, i} = h_i^T w_u$$\n",
        "\n",
        "Teniendo la predicción $\\hat r_{u, i}$, podemos entonces minimizar la distancia entre la predicción y el valor real:\n",
        "\n",
        "$$\\min \\sum_{(u, i) \\in R_W}\\left|r_{u, i}-\\hat{r}_{u, i}\\right|$$\n",
        "\n",
        "donde los valores de $\\hat r_{u,i}$ son estimados a través de métodos de optimización como **Stochastic Gradient Descent**. Noten además que en este caso se muestra la distancia euclideana, pero en la práctica podríamos ocupar otro tipo de distancia como la [norma frobenius](https://mathworld.wolfram.com/FrobeniusNorm.html).\n",
        "\n",
        "Por último y al igual que otros modelos de factorización de matrices, es usual agregar términos de sesgo a nuestra estimación:\n",
        "\n",
        "$$\\hat{r}_{u i}=\\mu+b_u+b_i+h_i^T w_u$$\n",
        "\n",
        "donde en este caso el término $\\mu$ representa la media de los ratings, y los coeficientes $b_u$, $b_i$ representan los coeficientes de sesgo (intercepto) para el usuario $u$ y producto $i$, respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PANiwlIZ_1Uj"
      },
      "source": [
        "### Implementación\n",
        "\n",
        "Para esta sección haremos uso de la librería `surprise`, la cual contiene implementaciones de diferentes modelos de filtros colaborativos para entrenar con unas pocas líneas de código al estilo `scikit`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlTq_Z07LojX",
        "outputId": "c0cf5986-38b6-4243-c068-bdf047bf0d98"
      },
      "outputs": [],
      "source": [
        "# instalamos la librería surprise\n",
        "!pip install surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwz2qiBFv7WU"
      },
      "outputs": [],
      "source": [
        "import surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "l-6ARZdPv9Ji",
        "outputId": "d7141ccd-489b-4f3e-dcef-32bf01100022"
      },
      "outputs": [],
      "source": [
        "# recordemos el formato de la data disponible\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-khip8qz7bQ"
      },
      "source": [
        "> **Pregunta**: ¿Necesitamos las features `title` y `overview`?\n",
        "\n",
        "Con la librería instalada, el primer paso es transformar nuestro Dataframe de `pandas` a Dataset de `surprise`. Para esto, debemos hacer uso de dos clases que nos facilita la libreria:\n",
        "- [Reader](https://surprise.readthedocs.io/en/stable/reader.html): Utilizado para la lectura de valores (ratings) de la data disponible. **En esta clase deben especificar la escala de los ratings de su dataframe.**\n",
        "- [Dataset](https://surprise.readthedocs.io/en/stable/dataset.html): Clase default utilizada por `surprise` para el posterior procesamiento de datos usando los modelos. **Es mandatorio que los datos hayan sido transformados a este formato.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7gS8bk91Img",
        "outputId": "8d22c871-3188-4044-e119-cdc4757232d4"
      },
      "outputs": [],
      "source": [
        "from surprise import Reader, Dataset\n",
        "\n",
        "reader = Reader(rating_scale = (0, 5)) # instanciamos reader, es importante definir la escala de los valores (ratings)\n",
        "\n",
        "df_collaborative = df[['userId', 'movieId', 'rating']].copy() # ordenamos dataset en user, movie, rating (paso clave para trabajar con Surprise)\n",
        "dataset = Dataset.load_from_df(df_collaborative, reader) # generamos Dataset de Surprise\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpanToPQ42jc"
      },
      "source": [
        "Para hacer Holdout, simplemente usamos el método `train_test_split` de `surprise`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLZ_D1R-29tJ",
        "outputId": "02247953-fc0f-4b87-8b24-b03bef73330c"
      },
      "outputs": [],
      "source": [
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "trainset, testset = train_test_split(dataset, test_size = 0.3, random_state = 3380)\n",
        "trainset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgw6pfzo2Ekd",
        "outputId": "5b7181b0-63a0-41ad-ba9d-26cbe3e4a540"
      },
      "outputs": [],
      "source": [
        "# noten como a diferencia del trainset, testset es solo una lista de tuplas\n",
        "testset[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtOf8LJP49p6"
      },
      "source": [
        "Alternativamente podemos usar un set predefinido:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LBWbgF148GO",
        "outputId": "ec2b6a4a-be79-418e-d591-43763cff57d6"
      },
      "outputs": [],
      "source": [
        "trainset = Dataset.load_from_df(df_collaborative.iloc[:30000], reader) # generamos Dataset con primeras 30k filas\n",
        "trainset = trainset.build_full_trainset() # instanciamos trainset\n",
        "\n",
        "testset = df_collaborative.iloc[30000:].copy() # filtramos el resto del dataframe\n",
        "testset = [testset.iloc[i].to_list() for i in range(len(testset))] # test set debe ser una lista de iterables con elementos (user_id, item_id, rating)\n",
        "\n",
        "trainset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlaGx3mq28d2"
      },
      "source": [
        "Con los datos preparados, podemos entrenar y generar predicciones sobre los datos usando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "gaefaVIa2D1Z",
        "outputId": "dd3e7323-875e-4684-9fa6-841de3e171c7"
      },
      "outputs": [],
      "source": [
        "from surprise import NMF\n",
        "\n",
        "model = NMF(random_state = 3380) # instanciamos modelo NMF (cambiar a modelo de preferencia)\n",
        "model.fit(trainset) # fit del modelo\n",
        "predictions = model.test(testset) # retorna un dataframe con el rating estimado para cada (user_id, item_id)\n",
        "pd.DataFrame(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdGPDdSE7vVs"
      },
      "source": [
        "##### Evaluación: Regresión\n",
        "\n",
        "Con el rating estimado y el observado, podemos evaluar la predicción de nuestro modelo a través de métricas de regresión. Para esto, podemos usar el módulo `surprise.accuracy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPcp4bLE5tr7",
        "outputId": "c5792cc9-66cd-43ba-de74-2f12357e883a"
      },
      "outputs": [],
      "source": [
        "from surprise.accuracy import mae\n",
        "mae(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBInJwGk8ROi"
      },
      "source": [
        "O simplemente calcular nuestra métrica de preferencia usando `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwYVaB_06vLD",
        "outputId": "c33b13f7-2668-4fab-82bc-26695397c9c6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "df_results = pd.DataFrame(predictions)\n",
        "mean_absolute_error(df_results['r_ui'], df_results['est'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlmTaPdTs8-8"
      },
      "source": [
        "Por otro lado, podemos obtener el top 3 de recomendaciones para cada usuario en el testset usando el siguiente chunk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWNs16CKtEt_",
        "outputId": "afb8c358-b6e5-4281-a837-a6417bd81887"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def get_top_n(predictions, n=10):\n",
        "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
        "\n",
        "    Args:\n",
        "        predictions(list of Prediction objects): The list of predictions, as\n",
        "            returned by the test method of an algorithm.\n",
        "        n(int): The number of recommendation to output for each user. Default\n",
        "            is 10.\n",
        "\n",
        "    Returns:\n",
        "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
        "        [(raw item id, rating estimation), ...] of size n.\n",
        "    \"\"\"\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_n[uid].append((iid, est))\n",
        "\n",
        "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = user_ratings[:n]\n",
        "\n",
        "    return top_n\n",
        "\n",
        "get_top_n(predictions, n = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-xrYVQ8-tIl"
      },
      "source": [
        "Finalmente, si queremos generar predicciones para películas que el **usuario no ha puntuado**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5OTi-E382P7",
        "outputId": "d324b160-da81-4bb0-e8b8-f1d5e7a0cb3a"
      },
      "outputs": [],
      "source": [
        "user_id = 1 # id de usuario a predecir\n",
        "movies_rated_by_user = df_collaborative[df_collaborative['userId'] == user_id]['movieId'] # peliculas rateadas por usuario\n",
        "total_movies = df_collaborative['movieId'].unique() # lista total de peliculas\n",
        "out_movies = [movie for movie in total_movies if movie not in movies_rated_by_user] # lista de peliculas que usuario no generó rating\n",
        "out_movie = out_movies[0] # elegimos el primer elemento de la lista\n",
        "\n",
        "model.test([[user_id,out_movie,None]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxicQnnsmvhJ"
      },
      "source": [
        "#### Evaluación: Clasificación\n",
        "\n",
        "Una alternativa en la evaluación de nuestro modelo es responder **cuántos productos relevantes es capaz de recomendar nuestro sistema**. Un **producto relevante** se define como un producto que el usuario otorga un **rating mayor a un threshold** definido. Como en la práctica es posible recomendar toda la cartera de productos al usuario, es natural **filtrar al top $k$ de recomendaciones relevantes**. De esta manera, podemos definir las siguientes métricas:\n",
        "\n",
        "$$\\text{Precision@k} = \\frac{\\text{Recommended items that are relevant}}{\\text{Recommended items}}$$\n",
        "\n",
        "$$\\text{Recall@k} = \\frac{\\text{Recommended items that are relevant}}{\\text{Relevant items}}$$\n",
        "\n",
        "Pueden encontrar más métricas de clasificación en el siguiente [enlace.](https://neptune.ai/blog/recommender-systems-metrics)\n",
        "\n",
        "Vamos ahora a la implementación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEBepKM0pBqe",
        "outputId": "da5e0f5c-c832-48b1-bbf2-f6f8f4783435"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
        "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    user_est_true = defaultdict(list)\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Number of relevant items\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "        # Number of recommended items in top k\n",
        "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "        # Number of relevant and recommended items in top k\n",
        "        n_rel_and_rec_k = sum(\n",
        "            ((true_r >= threshold) and (est >= threshold))\n",
        "            for (est, true_r) in user_ratings[:k]\n",
        "        )\n",
        "\n",
        "        # Precision@K: Proportion of recommended items that are relevant\n",
        "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
        "\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
        "\n",
        "        # Recall@K: Proportion of relevant items that are recommended\n",
        "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
        "\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
        "\n",
        "    return precisions, recalls\n",
        "\n",
        "# obtenemos precision y recall para cada usuario usando top 3 recomendaciones relevantes\n",
        "precisions, recalls = precision_recall_at_k(predictions, k = 3)\n",
        "\n",
        "# computamos la media entre todos los usuarios para cada métrica\n",
        "precision = np.mean([precision for precision in precisions.values()])\n",
        "recall = np.mean([recall for recall in recalls.values()])\n",
        "\n",
        "print(f'precision @ 3: {precision:.2f}')\n",
        "print(f'recall @ 3: {recall:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvXcVckrwl1F"
      },
      "source": [
        "> **Pregunta:** ¿Qué debería ocurrir si aumentamos $k$? ¿Porqué?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUmLt8JLr-xv",
        "outputId": "57eb5768-1c08-43b3-d01e-cae546f35c7e"
      },
      "outputs": [],
      "source": [
        "# obtenemos precision y recall para cada usuario usando top 10 recomendaciones relevantes\n",
        "precisions, recalls = precision_recall_at_k(predictions, k = 10)\n",
        "\n",
        "# computamos la media entre todos los usuarios para cada métrica\n",
        "precision = np.mean([precision for precision in precisions.values()])\n",
        "recall = np.mean([recall for recall in recalls.values()])\n",
        "\n",
        "print(f'precision @ 3: {precision:.2f}')\n",
        "print(f'recall @ 3: {recall:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnM2tXTpw7Lg"
      },
      "source": [
        "> **Pregunta:** ¿Y si disminuimos el valor del threshold?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA0oHIyasNCi",
        "outputId": "8491e5a5-46c7-45a5-a51a-efb5e45581bd"
      },
      "outputs": [],
      "source": [
        "# obtenemos precision y recall para cada usuario usando un threshold de 3.0\n",
        "precisions, recalls = precision_recall_at_k(predictions, threshold = 3.0)\n",
        "\n",
        "# computamos la media entre todos los usuarios para cada métrica\n",
        "precision = np.mean([precision for precision in precisions.values()])\n",
        "recall = np.mean([recall for recall in recalls.values()])\n",
        "\n",
        "print(f'precision @ 3: {precision:.2f}')\n",
        "print(f'recall @ 3: {recall:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lgeqhnp7qLB"
      },
      "source": [
        "## Bibliografía\n",
        "\n",
        "- Aggarwal, C. C. (2016). *Recommender systems* (Vol. 1). Cham: Springer International Publishing.\n",
        "- [Surprise](https://surpriselib.com/)\n",
        "- [Introduction to Recommender Systems](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada)\n",
        "- [Recommender Systems — A Complete Guide to Machine Learning Models](https://towardsdatascience.com/recommender-systems-a-complete-guide-to-machine-learning-models-96d3f94ea748)\n",
        "- [Getting started with a movie recommendation system](https://www.kaggle.com/code/ibtesama/getting-started-with-a-movie-recommendation-system#Content-Based-Filtering)\n",
        "- [Non-negative matrix factorization for recommendation systems](https://medium.com/logicai/non-negative-matrix-factorization-for-recommendation-systems-985ca8d5c16c)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
