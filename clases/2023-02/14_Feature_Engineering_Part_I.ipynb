{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrAEIjr4m_0"
   },
   "source": [
    "# Clase 14 - Feature Engineering üìé\n",
    "\n",
    "- MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos\n",
    "- Profesor: Ignacio Meza De la jara"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gNbak7104TXh"
   },
   "source": [
    "## Recapitulemos ü§†\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8ytmZUX-F4s"
   },
   "source": [
    "\n",
    "    üìù Recordemos por que usamos este flujo! \n",
    "\n",
    "MLOps, o Operaciones de Aprendizaje Autom√°tico, es un conjunto de pr√°cticas y herramientas que automatizan y gestionan el ciclo de vida del aprendizaje autom√°tico desde el desarrollo hasta la implementaci√≥n. Mejora la eficiencia, consistencia, escalabilidad, reproducibilidad y colaboraci√≥n entre los cient√≠ficos de datos, los ingenieros de aprendizaje autom√°tico y otras partes interesadas. MLOps ayuda a las organizaciones a crear, implementar y administrar modelos de aprendizaje autom√°tico de manera confiable, escalable y eficiente, lo que en √∫ltima instancia mejora la calidad de los modelos y facilita su implementaci√≥n en producci√≥n.\n",
    "\n",
    "![ML ops](https://camo.githubusercontent.com/4724bc1636a3fee34f87a1fac991fc4ccd270c0a3a510db04394f90b05d065eb/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f4d4453373230322f4d4453373230322f6d61696e2f7265637572736f732f323032332d30312f31332d4544412f6d65746f646f6c6f6769612e706e67)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "W_SoNsDt-JEY"
   },
   "source": [
    "## Objetivos de la Clase üéØ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AFXUMGNe-IMq"
   },
   "source": [
    "\n",
    "\n",
    "- Comprender qu√© son los m√©todos de aprendizajes autom√°tico.\n",
    "- Comprender qu√© son las features en el aprendizaje autom√°tico y por qu√© son importantes.\n",
    "- Aprender t√©cnicas de selecci√≥n de caracter√≠sticas, como la eliminaci√≥n de caracter√≠sticas redundantes o irrelevantes.\n",
    "- Aprender t√©cnicas de transformaci√≥n de caracter√≠sticas, como la normalizaci√≥n o la creaci√≥n de caracter√≠sticas sint√©ticas.\n",
    "- Comprender que las caracter√≠sticas afectan el rendimiento del modelo y c√≥mo evaluar las caracter√≠sticas seleccionadas.\n",
    "- Aprender a aplicar t√©cnicas de feature engineering utilizando Scikit-Learn.\n",
    "\n",
    "se√±alar que en la segunda se van a hacer nuevas caracteristicas\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QKZR-CzZ-n3w"
   },
   "source": [
    "## ¬øQue es Machine Learning? ü§î"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning es un subcampo de la inteligencia artificial (IA) que se enfoca en el desarrollo de algoritmos y modelos inform√°ticos que permiten a las computadoras aprender y mejorar autom√°ticamente a trav√©s de la experiencia, sin ser programadas expl√≠citamente para realizar tareas espec√≠ficas.\n",
    "\n",
    "En lugar de ser programadas con reglas predefinidas, las m√°quinas de aprendizaje autom√°tico pueden aprender y adaptarse a partir de datos de entrada, identificando patrones, haciendo predicciones y tomando decisiones basadas en la informaci√≥n disponible. Estos modelos son entrenados utilizando datos hist√≥ricos y retroalimentaci√≥n, y pueden ajustarse y mejorar con el tiempo a medida que se les expone a m√°s datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/14-Feature-Engineering/machine_learning.png?raw=true' width=700 />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LS7p1p9Z-tMg"
   },
   "source": [
    "### Esquema General de Machine Learning\n",
    "\n",
    "\n",
    "#### Unsupervised Learning / Aprendizaje No Supervisado\n",
    "\n",
    "T√©cnicas que no requieren datos etiquetados para entrenar modelos predictivos. Dentro de estos algoritmos encontramos los siguientes grupos: \n",
    "\n",
    "\n",
    "- **Clustering:** T√©cnicas para agrupar observaciones por su similitud a trav√©s de m√©tricas de distancia (como la distancia euclideana por ejemplo). Permite encontrar grupos que no son claros ante nuestro criterio. Por ejemplo, agrupar clientes seg√∫n sus caracter√≠sticas.\n",
    "\n",
    "\n",
    "- **Reducci√≥n de Dimensionalidad:** Conjuntos de t√©cnicas que permiten representar datos en menos dimensiones que las originales. Su utilidad radica tanto en mejorar el rendimiento de los clasificadores como tambi√©n en permitir visualizar datos. Ejemplo: Usar T-SNE para proyectar los datos del Better Life Index en dos dimensiones. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "         \n",
    "<img src='https://miro.medium.com/v2/resize:fit:809/0*tamvSiqDneDfw2Vr' width=400 />            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "### Supervised Learning / Aprendizaje Supervisado\n",
    "\n",
    "T√©cnicas que requieren datos etiquetados para entrenar modelos predictivos. Dentro de estos algoritmos encontramos los siguientes grupos:\n",
    "\n",
    "\n",
    "- **Clasificaci√≥n:** Tarea que consiste en predecir una clase/categor√≠a. Ejemplo: Predecir si una persona tiene caries o no a partir de una imagen.\n",
    "\n",
    "\n",
    "- **Regresi√≥n:** Tarea que consiste en predecir un n√∫mero real. Ejemplo: A partir del registro metereol√≥gico, elaborar un clasificador que permita predecir la temperatura de ma√±ana."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://static.javatpoint.com/tutorial/machine-learning/images/supervised-machine-learning.png' width=400 />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta ‚ùì:** ¬øQu√© necesito para desarrollar un modelo predictivo?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xYa6XjsD9fDH"
   },
   "source": [
    "## ¬øQu√© es Feature Engineering? üßÆ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8xEZkmnQ89it"
   },
   "source": [
    "\n",
    "\n",
    "Feature engineering es el proceso de seleccionar, transformar y crear caracter√≠sticas relevantes de entrada para construir un modelo de aprendizaje autom√°tico preciso y eficiente.\n",
    "\n",
    "> **Pregunta ‚ùì:** ¬øSe necesita alg√∫n conocimiento previo para realizar Feature Engineering?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema a visitar: House Pricing\n",
    "\n",
    "![House Pricing](https://storage.googleapis.com/kaggle-competitions/kaggle/5407/media/housesbanner.png)\n",
    "\n",
    "Fuente: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "Al igual que en la clase anterior, el dataset **`house pricing`** consiste en 80 variables (79 variables explicativas m√°s una variable objetivo) que describen aspectos fundamentales de hogares residenciales en la ciudad de *Ames, Iowa*. \n",
    "\n",
    "La variable objetivo es el precio final de cada hogar (regresi√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librer√≠as a utilizar en la clase\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El conjunto a trabajar es el de entrenamiento\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/MDS7202/MDS7202/main/recursos/2023-01/13-EDA//train.csv\",\n",
    "    index_col=\"Id\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Actividad üìé**: Imaginen que nos pasan estos datos y nos se√±alan que el conjunto de datos esta compuesto por:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Terreno üèîÔ∏è\n",
    "        \"LotArea\",  # Area del terreno\n",
    "        \"LandSlope\",  # Pendiente del terreno\n",
    "        \"Neighborhood\",  # Barrio\n",
    "- Metadatos de la Vivienda üìÜ\n",
    "        \"BldgType\",  # Tipo de vivienda\n",
    "        \"YearBuilt\",  # A√±o de construcci√≥n\n",
    "        \"YearRemodAdd\",  # A√±o de remodelaci√≥n\n",
    "        \"Utilities\",  # Agua, luz, etc...\n",
    "- Materiales üß±\n",
    "        \"Foundation\",  # Fundaci√≥n de la vivienda\n",
    "        \"RoofMatl\",  # Material del techo\n",
    "        \"RoofStyle\",  # Estilo del techo\n",
    "        \"Exterior1st\",  # Material del Exterior\n",
    "        \"ExterCond\",  # Condici√≥n del material exterior\n",
    "- Interior de la casa üè°\n",
    "        \"GrLivArea\",  # Area habitable sobre el nivel del suelo.\n",
    "        \"1stFlrSF\",  # Area primer piso\n",
    "        \"2ndFlrSF\",  # Area segundo piso\n",
    "        \"FullBath\",  # Ba√±os completos\n",
    "        \"HalfBath\",  # Ba√±os de visita?\n",
    "        \"BedroomAbvGr\",  # Piezas\n",
    "        \"KitchenAbvGr\",  # Cocinas\n",
    "        \"KitchenQual\",  # Calidad de la cocina\n",
    "- S√≥tano ü™®\n",
    "        \"TotalBsmtSF\",  # Total s√≥tano\n",
    "        \"BsmtCond\",  # Condici√≥n del s√≥tano\n",
    "- Garaje üöó\n",
    "        \"GarageType\",  # Tipo de garaje\n",
    "        \"GarageCars\",  # Cantidad de autos por garaje\n",
    "- Piscina ü§Ω‚Äç‚ôÇÔ∏è\n",
    "        \"PoolArea\",  # Area de la piscina\n",
    "        \"PoolQC\",  # Calidad de la piscina\n",
    "- Calefacci√≥n y Aire üå¶Ô∏è\n",
    "        \"Heating\",  # Calefacci√≥n\n",
    "        \"HeatingQC\",  # Calidad de la Calefacci√≥n\n",
    "        \"CentralAir\",  # Aire Acondicionado Central\n",
    "- Calidad y Condici√≥n üåü\n",
    "        \"OverallQual\",  # Calidad general\n",
    "        \"OverallCond\",  # Condici√≥n general actual\n",
    "- Datos de la venta  üíµ\n",
    "        \"SaleType\",  # Tipo de venta\n",
    "        \"SaleCondition\",  # Condici√≥n de la vivienda en la venta\n",
    "        \"SalePrice\",  # Precio de la venta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¬øPor que es importante? ü§®"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchos de los algoritmos de aprendizaje automatico **poseen un mejor desempe√±o cuando los valores de las features** (aka columnas) **se transforman a un valor facil de interpretar por los modelos**. Por otro lado, datos sucios pueden entorpecer las predicciones generadas por nuestros modelos, al igual que las escalas en que se presentan los datos. Por esto, **es relevante que las features que utilizemos se encuentren en escalas similares y con distribuciones relativamente similares a la distribuci√≥n normal**.\n",
    "\n",
    "> Importante ‚ùó: Gran parte de los modelos que se generan en la industria son centrados en datos, a diferencia de la academia el trabajo no se centra en el desarrollo de modelos ultra complejos, sino en modelos completamente estandarizados y por ello se necesita un mayor tiempo en la extracci√≥n de features desde los datos. Fuente interesante: [Data-Centric Approach vs Model-Centric Approach in Machine Learning](https://neptune.ai/blog/data-centric-vs-model-centric-machine-learning).\n",
    "\n",
    "Consideren los siguientes ejemplos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo 1**\n",
    "\n",
    "- El gr√°fico de la derecha se grafica una variable con respecto a otra sin escalar \n",
    "- El gr√°fico de la izquierda muestra ambas variables estandarizadas:\n",
    "\n",
    "> **Pregunta ‚ùì:** ¬øEs entendible para un humano el gr√°fico de la derecha?¬øQu√© se puede interpretar de este?¬øQu√© efectos tendr√° usar las variables no escaladas (izquierda) vs las escaladas (derecha) usando alg√∫n modelo predictivo basado en distancias?\n",
    "\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/14-Feature-Engineering/escalamiento_2.png?raw=true' width=1000 />\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html'>Compare the effect of different scalers on data with outliers en el User Guide de Scikit-learn</a>.\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "    Los Datos son del dataset <a href='https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset'>California Housing dataset</a>. \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo 2**\n",
    "\n",
    "- El gr√°fico de la derecha se grafica una variable un conjunto de variables en 2 dimensiones diferenciadas por la clase. \n",
    "- El gr√°fico de la izquierda se grafica el mismo grafico pero con el conjunto de variables transformado:\n",
    "\n",
    "> **Pregunta ‚ùì:** ¬øCual de los dos conjuntos de datos es mas facil de separar?¬øQue efectos tendr√≠a utilizar las variables en el estado original contra el estado final para alg√∫n modelo lineal?.\n",
    "\n",
    "<img src='https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/14-Feature-Engineering/feature-engineering1.jpg?raw=true' width=1000 />\n",
    "\n",
    "<div align='center'>\n",
    "    Fuente: <a href='https://www.kdnuggets.com/2018/12/feature-engineering-explained.html'>Feature Engineering for Machine Learning: 10 Examples</a>.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "f77XXaFh9r-D",
    "tags": []
   },
   "source": [
    "### ¬øPero que hay del Deep Learning, no que solucionaba este problema? üò£"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pUafO1u69q6R"
   },
   "source": [
    "Si bien uno de los aspectos m√°s relevantes del Deep Learning es la extracci√≥n autom√°tica de las features desde los datos, sin embargo:\n",
    "\n",
    "- El desarrollo actual de deep learning no nos permite abstraer todas las features desde los datos.\n",
    "- Muchas empresas no utilizan deep learning para generar modelos, ya sea por capacidad o simplemente porque no lo necesitan. **Recalcar, el deep learning no es la soluci√≥n para todo!** [Ejemplo](https://arxiv.org/pdf/2106.03253.pdf)\n",
    "- Deep learning suele tener buenos resultados en datos no estructurados, sin embargo, en datos tabulares no."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "U7gJTJFA9oGe"
   },
   "source": [
    "### ¬øComo podemos realizar este proceso? üíª"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "W2v44Cpv9lQ9"
   },
   "source": [
    "\n",
    "\n",
    "Actualmente podemos realizar feature engineering de muchas formas, especificamente no necesitamos mas que `numpy` o `Pandas` para generar features, pero hay un sin fin de herramientas que nos pueden facilitar la vida.\n",
    "\n",
    "En aspectos generales, la extracci√≥n de caracteristicas la podemos realizar de las siguientes formas:\n",
    "\n",
    "- **Python**: Es uno de los lenguajes de programaci√≥n m√°s populares para el aprendizaje autom√°tico y cuenta con varias bibliotecas √∫tiles para el feature engineering, como Pandas, NumPy y Scikit-learn.\n",
    "\n",
    "- **R**: Es otro lenguaje de programaci√≥n popular para el aprendizaje autom√°tico y cuenta con bibliotecas como dplyr y tidyr para el manejo y transformaci√≥n de datos.\n",
    "\n",
    "- **SQL**: Puede ser utilizado para realizar consultas y transformaciones en bases de datos para la selecci√≥n y extracci√≥n de caracter√≠sticas.\n",
    "\n",
    "- **Big Data**: Tecnolog√≠as como Hadoop y Spark permiten el procesamiento y an√°lisis de grandes cantidades de datos y pueden ser √∫tiles para el feature engineering en grandes conjuntos de datos.\n",
    "\n",
    "- **AutoML**: Herramientas como H2O.ai o DataRobot pueden automatizar parte del proceso de feature engineering, utilizando t√©cnicas de aprendizaje autom√°tico para seleccionar y transformar autom√°ticamente las caracter√≠sticas.\n",
    "\n",
    ">Importante: Para efectos de este curso nos enfocaremos en el desarrollo de features utilizando `Python`, sin embargo en la industria se van a encontrar con la utilizaci√≥n de `SQL`, `PySpark`, `DataFlow`, entre otras para la extracci√≥n.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://www.tecton.ai/wp-content/uploads/2020/10/whatisfeaturestore3.svg\" width=400/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Vdg2OynTEp7F"
   },
   "source": [
    "## Veamos nuestra librer√≠a core: Scikit-Lern ‚öíÔ∏è"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt5zcy-yFvrE"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/MDS7202/MDS7202/a60ef458182c2f26af3aaf4f8e0446a8512d4f75/clases/2022-01/15_Preprocesamiento_Intro_a_Scikit-Learn/resources/scikit-learn.png\" width=400/>\n",
    "\n",
    "[`scikit-learn`](https://scikit-learn.org/stable/) es probablemente una de las librer√≠as de Aprendizaje Autom√°tico m√°s populares para Python. *Open-source* y construida sobre `numpy`, `scipy` y `matplotlib`, ofrece interfaces y flujos de trabajos (*frameworks*) simples y eficientes para construir aplicaciones enfocadas an√°lisis de datos y predicci√≥n.\n",
    "Sus *APIs* permite generar c√≥digo limpio y est√° provista de una extensa documentaci√≥n.\n",
    "\n",
    "> Parentesis: API: **Application Programming Interface / Interfaz de programaci√≥n de aplicaciones** - Son las interfaces comunes (funciones, objetos, m√©todos, etc..) que un software o librer√≠a ofrece para comunicarse con el resto. Esta define entre otras cosas: el tipo de llamadas o funciones que pueden ser hechas, como hacerlas, el tipo de datos de entrada y salida, las conveciones, etc...\n",
    "\n",
    "Una gran ventaja de Scikit-learn consiste en su estructura transversal de clases y herencia. La mayor√≠a de clase pertenece a alguna de estas dos categor√≠as:\n",
    "\n",
    "* *`transformers`*: Permite transformar datos input antes de utilizar algoritmos de aprendizaje sobre ellos. Con las clases *`transformers`*, se pueden realizar imputaciones de valores faltantes, estandrizaci√≥n de variables, escalamientos y seleccion de caracter√≠siticas por medio de algoritmos especializados. Esto comunmente se logra a trav√©s de las interfaces\n",
    "    - `fit` que permite aprender los par√°metros de la transformaci√≥n, por ejemplo la media y varianza en la normalizaci√≥n.\n",
    "    - `transform` que aplica la transformaci√≥n a los datos.\n",
    "    - `fit_transform` permite ambas operaciones al mismo tiempo.\n",
    "\n",
    "* *`estimators`*: Proveen los algoritmos de aprendizaje autom√°tico a trav√©s de los m√©todos `fit` y `predict`.\n",
    "\n",
    "El m√©todo usual de importaci√≥n se basa en seleccionar un subm√≥dulo de la librer√≠a indicando (de manera opcional) el objeto que se utilizar√°. Por ejemplo, si se desea utilizar el escalador de datos Min-M√°x del subm√≥dulo `preprocessing`, se har√≠a de la manera usual, por medio de:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "> **Nota**: No se recomienda importar la librer√≠a completa `import sklearn as sk` pues su estructura de subm√≥dulos es suficientemente grande, como para considerar cada uno como una librer√≠a. \n",
    "\n",
    "\n",
    "A lo largo del curso se estudiar√°n distintos componentes de esta librer√≠a. Durante esta clase nos centraremos en los m√≥dulos `preprocessing`, `compose` y `pipeline`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "62-sDVIg39ya"
   },
   "source": [
    "## Operaciones Comunes de Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a la importancia que posee de la etapa de Feature Engineering en los proyectos de ML, se han desarrollado muchas t√©cnicas para agilizar el proceso. \n",
    "\n",
    "Este proceso incluye:\n",
    "\n",
    "- Creaci√≥n de nuevas Features a partir de operaciones usando los datos disponibles.\n",
    "- Transformaciones como las vistas en la clase de preprocesamiento (escalamiento, normalizaci√≥n, one hot encoding para variables categ√≥ricas etc...).\n",
    "- Reducci√≥n de Caracter√≠sticas en la que se combinan/reducen caracter√≠sticas redundantes (usando por ejemplo, PCA).\n",
    "- Selecci√≥n de Caracter√≠sticas en la que a partir de diversos criterios se seleccionan las caracter√≠sticas que m√°s aportan al modelo.\n",
    "\n",
    "El proceso de generar y preprocesar las features requiere mucha creatividad y al mismo conocimiento del dominio del problema.\n",
    "\n",
    "Para efectos de esta clase revisaremos las siguientes: \n",
    "\n",
    "- **Missing Values**\n",
    "- **Escalamiento de variables**\n",
    "- **Discretizaci√≥n** \n",
    "- **Codificaci√≥n de caracter√≠sticas categ√≥ricas**\n",
    "\n",
    "Sin embargo, a pesar de que no forma parte de un proceso de feature Engineering tradicional, comentaremos que es el **data-drift** y porque es importante que lo midas al momento de trabajar con tus variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Escalamiento\n",
    "\n",
    "Un paso importante antes de introducir features en los modelos, es el escalamiento de las variables. El objetivo de esto es que el dominio de las variables sea similar y de esta forma obtener mejores resultados. Este proceso es una de las cosas m√°s sencillas que se pueden hacer y que (por lo general) se traduce en un aumento del rendimiento del modelo. Ojo que no hacer esto puede hacer que su modelo no tenga sentido como es el caso de algoritmos cl√°sicos.\n",
    "\n",
    "Tomar en consideraci√≥n que muchos de los algoritmos modernos como **XGBoost** se√±alan que no necesitan escalamiento para el entrenamiento, sin embargo, escalar las variables de entrada puede impactar positivamente en el poder predictivo del modelo.\n",
    "\n",
    "Para realizar el escalamiento utilizaremos el m√≥dulo `sklearn.preprocessing`, este entrega diversas t√©cnicas de escalamiento, normalizaci√≥n y estandarizaci√≥n de datos a trav√©s de clases `Transformers` (no confundir con los transformers de Deep Learning).\n",
    "\n",
    "### Estandarizaci√≥n\n",
    "\n",
    "La est√°ndarizaci√≥n es una de las transformaciones mas relevantes a tener presente durante el modelamiento. esto debido a que un gran cantidad de algoritmos de aprendizaje autom√°tico / estad√≠stico, asumen que los datos a operar se encuentran **distribuidos de manera normal**. **Si los datos no se distribuyen normalmente y contienen valores at√≠picos**, **es posible que la media y la desviaci√≥n t√≠pica no reflejen con exactitud la tendencia central y la variabilidad de los datos**.\n",
    "\n",
    "En la pr√°ctica, se ignora la forma de la distribuci√≥n a trabajar y simplemente e transforma removiendo la media y escalando por la desviaci√≥n est√°ndar.\n",
    "\n",
    "\n",
    "El objeto `StandarScaler` permite estandarizar datos.\n",
    "\n",
    "**Ejemplo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.histogram(df, x='OverallQual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x='OverallCond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x='GarageCars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x='GarageArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x='GrLivArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_estandarize = [\n",
    "    'OverallQual',\n",
    "    'OverallCond',\n",
    "    'GarageCars',\n",
    "    'GarageArea',\n",
    "    'GrLivArea'\n",
    "]\n",
    "fig1 = px.histogram(df[cols_to_estandarize].melt(),\n",
    "             x='value',\n",
    "             color='variable',\n",
    "             barmode='group')\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformaci√≥n a aplicar es mover todos los datos a una distribuci√≥n normal con media 0 y varianza 1.\n",
    "\n",
    "Para esto, por cada dato: \n",
    "    \n",
    "$$ z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "En donde $\\mu$ es la media de la columna y $\\sigma$ es la desviaci√≥n est√°ndar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esto, importamos escalador, lo inicializamos y luego ejecutamos `fit_transform` sobre los datos.\n",
    "Notese que esto retorna un arreglo numpy con los datos escalados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "estandarized_df = standard_scaler.fit_transform(df.loc[:, cols_to_estandarize])\n",
    "\n",
    "estandarized_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por comodidad, convertimos los datos escalados a un Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estandarized_df = pd.DataFrame(estandarized_df, columns=cols_to_estandarize)\n",
    "estandarized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estandarized_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora como se muestran las distribuciones de los datos estandarizados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Al aplicar `.fit_transform()` se obtienen los par√°metros de media `.mean_` y desviaci√≥n est√°ndar `.scale_` para cada columna del dataframe operado. Observe que tales atributos del objeto tipo `StandardScaler` son p√∫blicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(estandarized_df[cols_to_estandarize].melt(),\n",
    "             x='value',\n",
    "             color='variable',\n",
    "             barmode='group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estandarized_df.plot.kde(figsize=(16,9), bw_method=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta ‚ùì**: ¬øQu√© sucede con las variables que se alejan mucho de ser normales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x='BsmtUnfSF')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalamiento m√≠nimo-m√°ximo\n",
    "\n",
    "Una buena alternativa al m√©todo anterior, es el escalamiento por rango, este tiene la forma:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_{i} - \\min(x)}{\\max (x)-\\min (x)}\n",
    "\\end{equation}\n",
    "\n",
    "para $x$ columna a tratar, $x_i$ elemento a transformar. Esta transformaci√≥n permite hacer que los datos se muevan entre 0 y 1 y puede ser utilizado y la distribuci√≥n de los datos no normales. \n",
    "\n",
    "> **Nota:** este transformador se ve afectado por la presencia de outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, 'Fireplaces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['BsmtUnfSF', 'Fireplaces']\n",
    "\n",
    "# BsmtUnfSF = Unfinished square feet of basement area\n",
    "df.loc[:, 'BsmtUnfSF'].plot.hist(backend='plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "scaled_data = minmax_scaler.fit_transform(df.loc[:, cols_to_scale])\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=cols_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.plot.hist(backend='plotly', barmode='overlay')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos minimos y m√°ximos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea escalar por rango, la mejor pr√°ctica es comprender los m√≠nimos y m√°ximos *absolutos* para cada columna. Esto se refiere, a las cotas superiores e inferiores que posee la columna **por definici√≥n**, a modo de ejemplo, considere un dataframe con las notas de una asginatura donde se enze√±a an√°lisis de datos, se sabe que la nota m√°xima en cierto √≠tem se codifica en una columna y su m√°ximo es en efecto es 7.0, sin embargo el m√≠nimo en dicha columna es 1.5, que es distinto al m√≠nimo natural para dicho item que es 1.0. Esto puede acarrear problemas con datos nuevos, sobretodo si aparece una nota inferior a 1.5. \n",
    "\n",
    "> **Ejercicios üìù**\n",
    "\n",
    "1. Investigue los par√°metros que se deben usar para proporcionar escalamiento por rango con valores m√°ximos y m√≠nimos proporcionados expl√≠citamente. \n",
    "\n",
    "2. Estudie el transformador `MaxAbsScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Un outlier es un at√≠picos que est√°n fuera del rango comun del resto de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x='LotArea', marginal='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "scaled_data = minmax_scaler.fit_transform(df.loc[:, ['LotArea']])\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=['LotArea'])\n",
    "scaled_data.plot.hist(backend='plotly', barmode='overlay')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta:** ¬øQu√© pasa cuando queremos estandarizar pero tenemos outliers?\n",
    "\n",
    "Respuesta: La gran mayor√≠a de los datos tienen a quedar en un rango muy acotado. En el caso anterior, hacia la izquierda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# media con los todos los datos de LotArea\n",
    "original = round(df['LotArea'].describe(), 3)\n",
    "original.name = 'LotArea Original'\n",
    "\n",
    "# datos presentes solo en el rango intercuant√≠lico\n",
    "q1 = df['LotArea'].quantile(.25)\n",
    "q3 = df['LotArea'].quantile(.75)\n",
    "mask = df['LotArea'].between(q1, q3, inclusive='both')\n",
    "iqr = df.loc[mask, 'LotArea']\n",
    "iqr.name ='LotArea Filtro IQR'\n",
    "iqr = round(iqr.describe(), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observen las diferencias entre las medias y las desviaciones est√°ndar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = pd.concat([original, iqr], axis=1)\n",
    "desc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si normalizamos usando estandarizaci√≥n original, cada dato va a ser dividido por 9981, 9 veces m√°s grande que la usando la versi√≥n sin outliers usando IQR!\n",
    "\n",
    "Esto en t√©rminos pr√°cticos har√° que los datos tiendan a concentrarse mucho m√°s cercanos a la media."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaci√≥n Logaritmica"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformaci√≥n logar√≠tmica es un m√©todo de transformaci√≥n de datos en el que **se sustituye cada variable x por un log(x + c)**, utilizando un base logaritmica y constante **c** escogida por el analista. Con **este tipo de transformaci√≥n se reduce o elimina la asimetr√≠a de nuestros datos originales**, PERO, para poder utilizar esta transformaci√≥n es importante que los datos originales deben tener una distribuci√≥n logar√≠tmica normal. De lo contrario, la transformaci√≥n logar√≠tmica no funcionar√°.\n",
    "\n",
    "$$log(X+c), \\, c \\in R$$\n",
    "\n",
    "Si bien esta transformaci√≥n suele ser una de las m√°s √∫tiles, transformar las variables genera una perdide da interpretabilidad directa en los datos, por esta raz√≥n se deben realizar transformaciones para visualizar el impacto de x en su dominio original. Para m√°s informaci√≥n revisar el siguiente [art√≠culo](https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(x):\n",
    "    return np.log(x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df['BsmtUnfSF'], marginal='box', barmode='overlay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformer = log_transform(df['LotArea'])\n",
    "\n",
    "px.histogram(log_transformer, marginal='box', barmode='overlay')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaci√≥n Robusta\n",
    "\n",
    "Cuando se trabaja con columnas que poseen valores fuera de rango (outliers) las transformaciones anteriores pueden fallar. En este caso, se recomienda utilizar una transformaci√≥n similar a la estandarizaci√≥n, pero que trabaje sobre la mediana como media y el rango intercuant√≠lico como desviaci√≥n est√°ndar:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_i - Q_2(x)}{Q_3(x) - Q_1(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Donde $IQR = Q_3(x) - Q_1(x)$ es el rango intercuart√≠lico de la columna $x$. \n",
    "\n",
    "En otras palabras, por cada ejemplo se sustrae la mediana y se divide por el rango intercuartil 75%- 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo para obtener IQR\n",
    "data = np.array([1, 14, 19, 20, 22, 24, 26, 47])\n",
    "\n",
    "# Calculamos rango intercurtil\n",
    "q3, q1 = np.percentile(data, [75 ,25])\n",
    "q3 - q1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el objeto `RobustScaler` y se aplica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df['LotArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "lot_area_standarized = standard_scaler.fit_transform(df[['LotArea']])\n",
    "robust_scaled_lot_area = robust_scaler.fit_transform(df[['LotArea']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparacion = np.concatenate([lot_area_standarized, robust_scaled_lot_area], axis=1)\n",
    "comparacion = pd.DataFrame(comparacion, columns=['Estandarizacion Com√∫n', 'Estandarizaci√≥n Robusta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(comparacion, marginal='box', barmode='overlay')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como los datos en la estandarizaci√≥n robusta se distribuyen de forma mas amplia que en caso de la estandarizaci√≥n normal.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapeo a distribuciones gaussianas \n",
    "\n",
    "Como se mencion√≥ anteriormente, no siempre se cumple la hip√≥tesis de normalidad en las columnas de un dataset, en tal caso, no es una buena idea estandarizar los datos pues puede llevar a problemas al momento de operar con algoritmos que requieren normalidad en su formulaci√≥n. Existe una familia de transformaciones param√©trica que busca aproximar una distribuci√≥n arbitraria a una gaussiana, se accede a este tipo de transformaciones por medio de la clase `PowerTransformer`, en esta clase se encuentran 2 transformaciones:\n",
    "\n",
    "* Box-Cox: Solo puede ser utilizada en datos extrictamente positivos. Viene dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "x_{i}^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    "\\frac{x_{i}^{\\lambda}-1}{\\lambda} & \\text { si } \\lambda \\neq 0 \\\\\n",
    "\\ln \\left(x_{i}\\right) & \\text { si } \\lambda=0\n",
    "\\end{cases}.\n",
    "\\end{equation}\n",
    "\n",
    "* Yeo-Johnson dada por: \n",
    "\n",
    "\\begin{equation}\n",
    "x_{i}^{(\\lambda)}=\n",
    "\\begin{cases}\n",
    "\\left[\\left(x_{i} + 1\\right)^{\\lambda}-1 \\right] / \\lambda & \\text { si } \\lambda \\neq 0, x_{i} \\geq 0 \\\\\n",
    "\\ln \\left(x_{i}+1\\right) & \\text { si } \\lambda=0, x_{i} \\geq 0 \\\\\n",
    "-\\left[\\left(-x_{i}+1\\right)^{2-\\lambda}-1\\right] /(2-\\lambda) & \\text { si } \\lambda \\neq 2, x_{i}<0 \\\\\n",
    "-\\ln \\left(-x_{i}+1\\right) & \\text { si } \\lambda=2, x_{i}<0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "En ambos casos, el par√°metro $\\lambda$ es estimado por m√°xima verosimilitud.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BsmtUnfSF** = Metros cuadrados no construidos en el subterraneo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsmtUnfSF_solo_num = df.loc[df['BsmtUnfSF'] > 0, ['BsmtUnfSF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(bsmtUnfSF_solo_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box - Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformaci√≥n box-cox\n",
    "\n",
    "transformer_bc = PowerTransformer(method='box-cox')\n",
    "\n",
    "df_bc = transformer_bc.fit_transform(bsmtUnfSF_solo_num)\n",
    "df_bc = pd.DataFrame(df_bc, columns=['BsmtUnfSF'])\n",
    "\n",
    "\n",
    "px.histogram(df_bc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yeo-Johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformaci√≥n yeo-johnson\n",
    "transformer_yj = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "df_yj = transformer_yj.fit_transform(bsmtUnfSF_solo_num)\n",
    "df_yj = pd.DataFrame(df_yj, columns=['BsmtUnfSF'])\n",
    "\n",
    "px.histogram(df_yj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ejercicio üìù**\n",
    "\n",
    "1. Obtenga los valores de lambda para cada uno de los de m√©todos revisados.\n",
    "\n",
    "2. El preprocesamiento por transformaci√≥n de cuantiles es un m√©todo robusto que permite transformar una distribuci√≥n de datos en una variable uniforme o normal. Permite el reducir el impacto de outliers. Investigue su formulaci√≥n, ventajas y desventajas, aplique el transformer `QuantileTransformer` en los datos recientemente generados para observar su comportamiento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizaci√≥n\n",
    "\n",
    "Otro m√©todo de transformaci√≥n de datos es la normalizaci√≥n de estos. Esto conisite en un mapeo a la bola cerrada seg√∫n una norma a elecci√≥n. Consiste simplemente en dividir los datos por su norma euclidiana l2. En el caso de 3 dimensiones se ve de esta manera:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{x_{i}}{\\sqrt{x_{i}^{2}+y_{i}^{2}+z_{i}^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "El escalado de las entradas a normas unitarias es una operaci√≥n habitual en la clasificaci√≥n de textos o la agrupaci√≥n por cl√∫steres.\n",
    "\n",
    "**Ejemplo** \n",
    "\n",
    "Se estudia como opera este transformador de datos en una visualizaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x='LotArea',\n",
    "    y='GrLivArea',\n",
    "    z='GarageArea',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estudia el impacto de la transformaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "cols_to_normalize = ['LotArea','GrLivArea','GarageArea']\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalized_data = normalizer.fit_transform(df.loc[:, cols_to_normalize])\n",
    "normalized_data = pd.DataFrame(normalized_data, columns=cols_to_normalize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se visualiza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    normalized_data,\n",
    "    x='LotArea',\n",
    "    y='GrLivArea',\n",
    "    z='GarageArea',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¬øQu√© aspectos deber√≠amos considerar al momento de realizar escalamientos? üòÖ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al momento de realizar escalamiento de cualquier tipo deben considerar los siguientes puntos:\n",
    "\n",
    "- **El escalamiento es una fuente de data leakeage**. ¬øQu√© es data leakage?, data leakage se produce cuando se utiliza informaci√≥n ajena al conjunto de datos de entrenamiento para crear el modelo.\n",
    "- **Necesidad de entrenamiento ante variaciones de los datos de entrada**. Muchos de los escalamientos necesitan el c√°lculo de un estad√≠stico para realizar la transformaci√≥n, esto implica que si los datos vistos durante el entrenamiento cambian en producci√≥n las transformaciones no ser√°n las mejores y necesitar√°n un reentrenamiento de los datos.\n",
    "\n",
    "üëÄ **Spoiler**: Mas tarde en este curso aprenderemos una forma simple para evitar el data-leakage utilizando pipelines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Codificaci√≥n de Variables Ordinales\n",
    "\n",
    "Para el manejo de adecuado de variables ordinales,  se recomienda expresar sus valores en funci√≥n de c√≥digos n√∫mericos. El transformer `OrdinalEncoder` permite transformar caracter√≠sticas categ√≥ricas en c√≥digos enteros (n√∫meros enteros)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena el codificador"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BsmtQual: Evaluates the height of the basement\n",
    "\n",
    "       Ex\tExcellent (100+ inches)\t\n",
    "       Gd\tGood (90-99 inches)\n",
    "       TA\tTypical (80-89 inches)\n",
    "       Fa\tFair (70-79 inches)\n",
    "       Po\tPoor (<70 inches\n",
    "       NA\tNo Basement\n",
    "\t\t\n",
    "BsmtCond: Evaluates the general condition of the basement\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical - slight dampness allowed\n",
    "       Fa\tFair - dampness or some cracking or settling\n",
    "       Po\tPoor - Severe cracking, settling, or wetness\n",
    "       NA\tNo Basement\n",
    "\t\n",
    "       \n",
    "HeatingQC: Heating quality and condition\n",
    "\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BsmtQual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "\n",
    "variables_ordinal = df[['BsmtCond', 'BsmtQual','HeatingQC']]\n",
    "\n",
    "# Se entrena el codificador\n",
    "ordinales = enc.fit_transform(variables_ordinal.dropna())\n",
    "pd.DataFrame(ordinales, columns = ['BsmtCond', 'BsmtQual','HeatingQC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_ordinal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen las categor√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['BsmtCond']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta ‚ùì**: ¬øExiste alg√∫n problema en estas codificaciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_order = [\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_2 = OrdinalEncoder(categories=categories_order)\n",
    "\n",
    "ordinales = enc_2.fit_transform(variables_ordinal.dropna())\n",
    "\n",
    "pd.DataFrame(ordinales, columns = ['BsmtCond', 'BsmtQual','HeatingQC'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta ‚ùì**: ¬øCodificamos estas variables con `variables_ordinal.dropna()`. Por qu√© tuvimos que botar los valores faltantes y que se puede hacer en este caso ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta ‚ùì**: ¬øC√≥mo codificamos las variables que no tienen orden?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificaci√≥n de Variables Categoricas\n",
    "\n",
    "Un problema com√∫n con la c√≥dificaci√≥n ordinal es que las variables pasan a ser consideradas continuas por algoritmos de machine learning (en especial por la API *estimators* de scikit-learn). Para evitar esto es posible convertir cada categor√≠a en una columna por si sola y asignar un 1 cuando est√© presente. \n",
    "\n",
    "![One Hot Encoding](https://github.com/MDS7202/MDS7202/blob/main/recursos/2023-01/14-Feature-Engineering/ohe.png)\n",
    "<center>Fuente: https://morioh.com/p/811a5d22bbca </center>\n",
    "\n",
    "Esto se puede llevar a cabo por medio del transformador ` OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Foundation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "cod = ohe.fit_transform(df.loc[:, ['Foundation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cod.toarray(), columns=ohe.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'Neighborhood'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, con los barrios\n",
    "ohe.fit_transform(df.loc[:, ['Neighborhood']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Matrix\n",
    "\n",
    "Son matrices que contienen muy pocos valores distintos de 0 y estos se encuentran muy dispersos en la matriz.\n",
    "\n",
    "![](https://www.kdnuggets.com/wp-content/uploads/sparse-matrix.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "X = np.random.uniform(size=(10000, 10000))  # 100.000.000\n",
    "X[X < 0.99] = 0\n",
    "X_csr = sparse.csr_matrix(X)\n",
    "\n",
    "print(f\"Size in bytes of original matrix: {X.nbytes}\")\n",
    "print(\n",
    "    f\"Size in bytes of compressed sparse row matrix: {X_csr.data.nbytes + X_csr.indptr.nbytes + X_csr.indices.nbytes}\"\n",
    ")\n",
    "print(\n",
    "    f'Relaci√≥n: { (X_csr.data.nbytes + X_csr.indptr.nbytes + X_csr.indices.nbytes)/ X.nbytes }'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ejercicio üìù**\n",
    "\n",
    "1. Utilice este transformador en los datos categ√≥ricos anteriores. Los resultados ser√°n entregados en formato *sparse* por  lo que tendr√° que hacer uso del m√©todo `.toarray()` de los arreglos de NumPy.\n",
    "\n",
    "2. Compare con la funci√≥n `get_dummies` de pandas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5GJAkrw6-WtH"
   },
   "source": [
    "## Ya pero quiero aprender mas... ¬øalgo para leer? ü§î"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7CU5cYrs-BAF"
   },
   "source": [
    "- Para comenzar pueden visualizar con mayor profundidad la documentaci√≥n de Scikit-Learn enfocada en la extracci√≥n de features: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "- Complementar la clase leyendo el capitulo 5 del libro [Designing Machine Learning Systems](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969)\n",
    "\n",
    "- Leer capitulo 1 del libro [Machine Learning Design Patterns: Solutions to Common Challenges in Data Preparation, Model Building, and MLOps](https://www.amazon.com/-/es/Valliappa-Lakshmanan/dp/1098115783/ref=pd_bxgy_img_sccl_1/142-9514380-0202369?pd_rd_w=JQSaa&content-id=amzn1.sym.26a5c67f-1a30-486b-bb90-b523ad38d5a0&pf_rd_p=26a5c67f-1a30-486b-bb90-b523ad38d5a0&pf_rd_r=BJE9WMJ9X1QQMYF4C3EJ&pd_rd_wg=y78Jr&pd_rd_r=6a1994c2-2734-428a-a270-ded23f892987&pd_rd_i=1098115783&psc=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
